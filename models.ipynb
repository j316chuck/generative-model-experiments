{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pomegranate import State, DiscreteDistribution, HiddenMarkovModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "from Bio.Alphabet import IUPAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeHMM(): \n",
    "    \n",
    "    def __init__(self, args, x_train=None, weights=None, verbose=True): \n",
    "        \"\"\"\n",
    "        Initializes the HMM to perform generative tasks\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the HMM\n",
    "        args.name : string \n",
    "            defines the name of the HMM\n",
    "        args.hidden_size : int \n",
    "            defines the hidden size\n",
    "        args.max_iterations: int\n",
    "            sets the max iterations\n",
    "        args.n_jobs: int\n",
    "            sets the number of cores to use\n",
    "        args.batch_size : int\n",
    "            sets the batch size\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        args.char_to_int : dict\n",
    "            a map from characters to index (integer) in the sequences\n",
    "        args.build_from_samples : boolean\n",
    "            build model from samples\n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.max_iterations = args[\"max_iterations\"]\n",
    "        self.n_jobs = args[\"n_jobs\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.epoch = args[\"epoch\"]\n",
    "        self.char_to_int = args[\"char_to_int\"]\n",
    "        self.vocabulary = [pair[0] for pair in sorted(self.char_to_int.items(), key = lambda x : x[1])]\n",
    "        self.indexes = [pair[1] for pair in sorted(self.char_to_int.items(), key = lambda x : x[1])]\n",
    "        self.emission_size = len(self.indexes)\n",
    "        if args[\"build_from_samples\"] and x_train is not None: \n",
    "            self.model = HiddenMarkovModel.from_samples(DiscreteDistribution, \n",
    "                                                    n_components = self.hidden_size, \n",
    "                                                    X = x_train, \n",
    "                                                    algorithm = 'baum-welch', \n",
    "                                                    return_history = True,\n",
    "                                                    verbose = verbose,\n",
    "                                                    max_iterations = self.max_iterations,\n",
    "                                                    n_jobs = self.n_jobs, \n",
    "                                                    weights = weights,\n",
    "                                                    #batch_size = self.batch_size,\n",
    "                                                    #batches_per_epoch = self.epochs\n",
    "                                               )[0]\n",
    "            \n",
    "        else: \n",
    "            self.build_model()\n",
    "        self.model.bake()\n",
    "\n",
    "    \n",
    "    def build_model(self): \n",
    "        distributions = []\n",
    "        for _ in range(self.hidden_size): \n",
    "            emission_probs = np.random.random(self.emission_size)\n",
    "            emission_probs = emission_probs / emission_probs.sum()\n",
    "            distributions.append(DiscreteDistribution(dict(zip(self.vocabulary, emission_probs))))\n",
    "        trans_mat = np.random.random((self.hidden_size, self.hidden_size))\n",
    "        trans_mat = trans_mat / trans_mat.sum(axis = 1, keepdims = 1)\n",
    "        starts = np.random.random((self.hidden_size))\n",
    "        starts = starts / starts.sum()\n",
    "        # testing initializations\n",
    "        np.testing.assert_almost_equal(starts.sum(), 1)\n",
    "        np.testing.assert_array_almost_equal(np.ones(self.hidden_size), trans_mat.sum(axis = 1))\n",
    "        self.model = HiddenMarkovModel.from_matrix(trans_mat, distributions, starts)\n",
    "\n",
    "    def fit(self, x_train, weights=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Fits the model on an HMM with self.hidden_size\n",
    "        \"\"\"    \n",
    "        return self.model.fit(x_train, \n",
    "                        algorithm = 'baum-welch', \n",
    "                        return_history = True, \n",
    "                        verbose = verbose,\n",
    "                        max_iterations = self.max_iterations,\n",
    "                        n_jobs = self.n_jobs, \n",
    "                        weights = weights,\n",
    "                        #batch_size = self.batch_size,\n",
    "                        #batches_per_epoch = self.epochs\n",
    "                   )\n",
    "    \n",
    "    def sample(self, n, length):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        n is number of samples\n",
    "        length is how long you want each sample to be\n",
    "        \"\"\"\n",
    "        return np.array([\"\".join(seq) for seq in self.model.sample(n = n, length = length)])\n",
    "            \n",
    "        \n",
    "    def predict(self, x_test): \n",
    "        \"\"\"\n",
    "        predict the log probability of obtaining the sequences in x_test\n",
    "        log(P(X1, X2, ..., X_test)) = sum(log(P(Xi)))\n",
    "        Input: x_test a list of sequences. should be 2 or 3 dimensional\n",
    "        \"\"\"\n",
    "        assert(len(np.array(x_test).shape) == 2 or len(np.array(x_test).shape) == 3)\n",
    "        return sum([self.model.log_probability(seq) for seq in np.array(x_test)])\n",
    "                \n",
    "    def show_model(self): \n",
    "        self.model.plot()\n",
    "        \n",
    "    def save_model(self, path): \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.model.to_json(), f)\n",
    "    \n",
    "    def load_model(self, path): \n",
    "        with open(path, 'r') as f:\n",
    "            json_model = json.load(f)\n",
    "        self.model = HiddenMarkovModel.from_json(json_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_base_args(): \n",
    "    return {\n",
    "        \"name\" : \"base HMM\",\n",
    "        \"hidden_size\" : 5,\n",
    "        \"max_iterations\" : 10,\n",
    "        \"n_jobs\" : 1,\n",
    "        \"batch_size\" : 5,\n",
    "        \"epoch\" : 2,\n",
    "        \"char_to_int\" : {\"A\" : 0, \"C\" : 1, \"T\" : 2, \"G\" : 3},\n",
    "        \"build_from_samples\" : False\n",
    "    }\n",
    "\n",
    "def hmm_build_from_samples_args(): \n",
    "    args = hmm_base_args()\n",
    "    args[\"build_from_samples\"] = True\n",
    "    return args\n",
    "\n",
    "def hmm_amino_acid_args(): \n",
    "    args = hmm_base_args()\n",
    "    amino_acids = get_all_amino_acids()\n",
    "    indexes = list(range(len(amino_acids)))\n",
    "    assert(len(amino_acids) == 21)\n",
    "    assert(amino_acids == \"*\" + IUPAC.protein.letters) #*ACDEFGHIKLMNPQRSTVWY\n",
    "    args[\"char_to_int\"] = dict(zip(amino_acids, indexes))\n",
    "    return args\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit_dna_hmm(X_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    assert(hmm.char_to_int == {\"A\" : 0, \"C\" : 1, \"T\" : 2, \"G\" : 3})\n",
    "    assert(hmm.indexes == sorted(list(range(4))))\n",
    "    assert(hmm.vocabulary == list(\"ACTG\"))\n",
    "    \n",
    "def test_fit_amino_acid_hmm(X_train_sequences):\n",
    "    args = hmm_amino_acid_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    amino_acid_alphabet = get_all_amino_acids()\n",
    "    assert(hmm.char_to_int == dict(zip(amino_acid_alphabet, list(range(len(amino_acid_alphabet))))))\n",
    "    assert(hmm.indexes == sorted(list(range(len(amino_acid_alphabet)))))\n",
    "    assert(hmm.vocabulary == list(amino_acid_alphabet))\n",
    "    \n",
    "def test_sample_and_predict_dna_hmm(x_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(x_train_sequences, verbose=False)\n",
    "    seq1, seq2 = tuple(hmm.sample(2, 714))\n",
    "    np.testing.assert_almost_equal(hmm.model.probability(seq1), np.e ** hmm.predict([list(seq1)]))\n",
    "    total = 0\n",
    "    for i in \"ACTG\": \n",
    "        for j in \"ACTG\": \n",
    "            for k in \"ACTG\":\n",
    "                codon = i + j + k\n",
    "                np.testing.assert_almost_equal(hmm.model.probability(codon), np.e ** hmm.predict([list(codon)]))\n",
    "                total += np.e ** hmm.predict([list(codon)])\n",
    "    np.testing.assert_almost_equal(1, total)\n",
    "    \n",
    "def test_sample_and_predict_amino_acid_hmm(x_train_sequences): \n",
    "    args = hmm_amino_acid_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(x_train_sequences, verbose=False)\n",
    "    wild_type_amino_acid = get_wild_type_amino_acid_sequence()\n",
    "    seq1, seq2 = tuple(hmm.sample(2, len(wild_type_amino_acid)))\n",
    "    print(\"{0} amino acids away from wild type!\".format(count_substring_mismatch(seq1, \n",
    "                                                            wild_type_amino_acid)))\n",
    "    np.testing.assert_almost_equal(hmm.model.probability(seq1), np.e ** hmm.predict([list(seq1)]))\n",
    "    total = 0\n",
    "    amino_acid_alphabet = get_all_amino_acids()\n",
    "    for i in amino_acid_alphabet: \n",
    "        for j in amino_acid_alphabet: \n",
    "            amino_acid = i + j\n",
    "            np.testing.assert_almost_equal(hmm.model.probability(amino_acid), np.e ** hmm.predict([list(amino_acid)]))\n",
    "            total += np.e ** hmm.predict([list(amino_acid)])\n",
    "    np.testing.assert_almost_equal(total, 1)   \n",
    "    \n",
    "def test_fit_dna_hmm_from_samples(X_train_sequences): \n",
    "    args = hmm_build_from_samples_args()\n",
    "    hmm = GenerativeHMM(args, X_train_sequences)\n",
    "    #test max iterations and n_jobs \n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args, X_train_sequences)\n",
    "    assert(hmm.char_to_int == {\"A\" : 0, \"C\" : 1, \"T\" : 2, \"G\" : 3})\n",
    "    assert(hmm.indexes == sorted(list(range(4))))\n",
    "    assert(hmm.vocabulary == list(\"ACTG\"))\n",
    "\n",
    "def test_fit_dna_hmm_weights(X_train_sequences):\n",
    "    args = hmm_base_args()\n",
    "    weights = np.identity(4)\n",
    "    weights = np.vstack([weights, [0.25, 0.25, 0.25, 0.25]])\n",
    "    for weight in weights:\n",
    "        counts = {\"A\" : 0, \"C\" : 0, \"T\" : 0, \"G\" : 0}\n",
    "        hmm = GenerativeHMM(args)\n",
    "        hmm.fit(X_train_sequences, weight, verbose=False)\n",
    "        json_model = json.loads(hmm.model.to_json())\n",
    "        for state in json_model[\"states\"]: \n",
    "            if state is not None and state[\"distribution\"] is not None:\n",
    "                mp = state[\"distribution\"][\"parameters\"][0]\n",
    "                for k, v in mp.items(): \n",
    "                    counts[k] = counts[k] + v\n",
    "        print(\"Weights:\", weight, \"\\nCounts:\", counts)    \n",
    "        \n",
    "\n",
    "def test_fit_dna_hmm_from_samples_weights(X_train_sequences):\n",
    "    args = hmm_build_from_samples_args()\n",
    "    weights = np.identity(4)\n",
    "    weights = np.vstack([weights, [0.25, 0.25, 0.25, 0.25]])\n",
    "    for weight in weights: \n",
    "        counts = {\"A\" : 0, \"C\" : 0, \"T\" : 0, \"G\" : 0}\n",
    "        hmm = GenerativeHMM(args, X_train_sequences, weight, verbose=False)\n",
    "        json_model = json.loads(hmm.model.to_json())\n",
    "        for state in json_model[\"states\"]: \n",
    "            if state is not None and state[\"distribution\"] is not None:\n",
    "                mp = state[\"distribution\"][\"parameters\"][0]\n",
    "                for k, v in mp.items(): \n",
    "                    counts[k] = counts[k] + v\n",
    "        print(\"Weights:\", weight, \"\\nCounts:\", counts) \n",
    "        \n",
    "def test_save_and_load_hmm(x_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences, verbose=False)\n",
    "    hmm.save_model(\"./models/test.json\")\n",
    "    cached_hmm = GenerativeHMM(args)\n",
    "    cached_hmm.load_model(\"./models/test.json\")\n",
    "    for i in \"ACTG\": \n",
    "        for j in \"ACTG\": \n",
    "            for k in \"ACTG\":\n",
    "                codon = i + j + k\n",
    "                np.testing.assert_almost_equal(hmm.predict([list(codon)]), cached_hmm.predict([list(codon)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 4605.2749657058885\tTime (s): 0.06758\n",
      "[2] Improvement: 118.43908114318037\tTime (s): 0.0772\n",
      "[3] Improvement: 136.9073133369384\tTime (s): 0.07086\n",
      "[4] Improvement: 159.94660009599465\tTime (s): 0.07716\n",
      "[5] Improvement: 187.18908311484847\tTime (s): 0.08191\n",
      "[6] Improvement: 218.7204234949313\tTime (s): 0.0832\n",
      "[7] Improvement: 253.39038411345973\tTime (s): 0.08333\n",
      "[8] Improvement: 287.02934348963026\tTime (s): 0.08507\n",
      "[9] Improvement: 309.2169626667019\tTime (s): 0.0758\n",
      "[10] Improvement: 308.27388485256233\tTime (s): 0.08028\n",
      "Total Training Improvement: 6584.388042014136\n",
      "Total Training Time (s): 0.8800\n",
      "[1] Improvement: 3703.906352701277\tTime (s): 0.02808\n",
      "[2] Improvement: 151.3515699529671\tTime (s): 0.02937\n",
      "[3] Improvement: 179.43646891265234\tTime (s): 0.03036\n",
      "[4] Improvement: 229.4424297317746\tTime (s): 0.02946\n",
      "[5] Improvement: 283.28310957495705\tTime (s): 0.02871\n",
      "[6] Improvement: 316.66049157975067\tTime (s): 0.03642\n",
      "[7] Improvement: 312.35783577164693\tTime (s): 0.03027\n",
      "[8] Improvement: 282.5510133622156\tTime (s): 0.02951\n",
      "[9] Improvement: 253.43246066629945\tTime (s): 0.04144\n",
      "[10] Improvement: 231.38039219363418\tTime (s): 0.03372\n",
      "[11] Improvement: 218.4005701564456\tTime (s): 0.04554\n",
      "[12] Improvement: 215.8536122388614\tTime (s): 0.05462\n",
      "[13] Improvement: 221.22132346016588\tTime (s): 0.04065\n",
      "[14] Improvement: 238.52326250237093\tTime (s): 0.03819\n",
      "[15] Improvement: 278.7623425214406\tTime (s): 0.03782\n",
      "[16] Improvement: 343.1584320369511\tTime (s): 0.04267\n",
      "[17] Improvement: 399.47648899068736\tTime (s): 0.03541\n",
      "[18] Improvement: 405.8827699921021\tTime (s): 0.02816\n",
      "[19] Improvement: 360.4990958257695\tTime (s): 0.03036\n",
      "[20] Improvement: 287.6166621184093\tTime (s): 0.02922\n",
      "Total Training Improvement: 8913.196684290378\n",
      "Total Training Time (s): 0.8452\n",
      "230 amino acids away from wild type!\n"
     ]
    }
   ],
   "source": [
    "test_fit_amino_acid_hmm(test_amino_acid_sequences)\n",
    "test_sample_and_predict_amino_acid_hmm(test_amino_acid_sequences)\n",
    "test_save_and_load_hmm(test_amino_acid_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 9988.078221918113\tTime (s): 0.207\n",
      "[2] Improvement: 51.13657606347988\tTime (s): 0.2085\n",
      "[3] Improvement: 54.381578968284884\tTime (s): 0.2301\n",
      "[4] Improvement: 60.11117372616718\tTime (s): 0.2576\n",
      "[5] Improvement: 69.96178500499809\tTime (s): 0.2486\n",
      "[6] Improvement: 85.59459196463285\tTime (s): 0.2114\n",
      "[7] Improvement: 109.30187992156425\tTime (s): 0.2281\n",
      "[8] Improvement: 144.82575322852063\tTime (s): 0.2598\n",
      "[9] Improvement: 198.2806136766594\tTime (s): 0.2435\n",
      "[10] Improvement: 279.0093306618801\tTime (s): 0.2667\n",
      "Total Training Improvement: 11040.6815051343\n",
      "Total Training Time (s): 2.6503\n",
      "[1] Improvement: 558.6716769016202\tTime (s): 0.08606\n",
      "[2] Improvement: 41.951675838645315\tTime (s): 0.1004\n",
      "[3] Improvement: 27.33550045908487\tTime (s): 0.08635\n",
      "[4] Improvement: 21.169997687611612\tTime (s): 0.0963\n",
      "[5] Improvement: 19.052319261347293\tTime (s): 0.1006\n",
      "[6] Improvement: 18.764983236440457\tTime (s): 0.1059\n",
      "[7] Improvement: 19.435050926054828\tTime (s): 0.1012\n",
      "[8] Improvement: 20.74634113207867\tTime (s): 0.1206\n",
      "[9] Improvement: 22.603162427112693\tTime (s): 0.08016\n",
      "[10] Improvement: 25.00705866458884\tTime (s): 0.07609\n",
      "[11] Improvement: 28.0120917037857\tTime (s): 0.1115\n",
      "[12] Improvement: 31.709161068487447\tTime (s): 0.1381\n",
      "[13] Improvement: 36.222005268456996\tTime (s): 0.1033\n",
      "[14] Improvement: 41.70838600449497\tTime (s): 0.123\n",
      "[15] Improvement: 48.363793395488756\tTime (s): 0.1149\n",
      "[16] Improvement: 56.426632143207826\tTime (s): 0.07428\n",
      "[17] Improvement: 66.18501194677083\tTime (s): 0.07165\n",
      "[18] Improvement: 77.9869346167543\tTime (s): 0.07334\n",
      "[19] Improvement: 92.2586008032813\tTime (s): 0.08977\n",
      "[20] Improvement: 109.53994584332395\tTime (s): 0.12\n",
      "Total Training Improvement: 1363.1503293286369\n",
      "Total Training Time (s): 2.2183\n",
      "[1] Improvement: 1463.9776933506946\tTime (s): 0.2266\n",
      "[2] Improvement: 19.28789435888757\tTime (s): 0.2255\n",
      "[3] Improvement: 9.843472822525655\tTime (s): 0.2333\n",
      "[4] Improvement: 6.990004557344946\tTime (s): 0.2252\n",
      "[5] Improvement: 6.323800558413495\tTime (s): 0.2397\n",
      "[6] Improvement: 6.333374843408819\tTime (s): 0.2266\n",
      "[7] Improvement: 6.580532394567854\tTime (s): 0.2181\n",
      "[8] Improvement: 6.93779398042534\tTime (s): 0.2282\n",
      "[9] Improvement: 7.367348459723871\tTime (s): 0.2068\n",
      "[10] Improvement: 7.858901731466176\tTime (s): 0.2205\n",
      "Total Training Improvement: 1541.5008170574583\n",
      "Total Training Time (s): 2.5191\n",
      "[1] Improvement: 7867.280920299847\tTime (s): 0.08924\n",
      "[2] Improvement: 43.26720080836094\tTime (s): 0.08559\n",
      "[3] Improvement: 34.427399243053515\tTime (s): 0.07605\n",
      "[4] Improvement: 30.24341148212261\tTime (s): 0.08556\n",
      "[5] Improvement: 27.91914788057329\tTime (s): 0.07082\n",
      "[6] Improvement: 26.554715976701118\tTime (s): 0.07613\n",
      "[7] Improvement: 25.771822216338478\tTime (s): 0.07878\n",
      "[8] Improvement: 25.40256800869247\tTime (s): 0.07134\n",
      "[9] Improvement: 25.37749746690679\tTime (s): 0.0725\n",
      "[10] Improvement: 25.673151094058994\tTime (s): 0.09865\n",
      "[11] Improvement: 26.285601229203166\tTime (s): 0.1042\n",
      "[12] Improvement: 27.21656501686084\tTime (s): 0.148\n",
      "[13] Improvement: 28.46579958431539\tTime (s): 0.09111\n",
      "[14] Improvement: 30.02670540002873\tTime (s): 0.07035\n",
      "[15] Improvement: 31.88366256408335\tTime (s): 0.08339\n",
      "[16] Improvement: 34.01056954314117\tTime (s): 0.06912\n",
      "[17] Improvement: 36.37078816833673\tTime (s): 0.07831\n",
      "[18] Improvement: 38.919398443817045\tTime (s): 0.1085\n",
      "[19] Improvement: 41.60932987871638\tTime (s): 0.09696\n",
      "[20] Improvement: 44.403414734231774\tTime (s): 0.07792\n",
      "Total Training Improvement: 8471.10966903939\n",
      "Total Training Time (s): 1.9206\n",
      "Weights: [1. 0. 0. 0.] \n",
      "Counts: {'A': 3.5027998712109536, 'C': 0.0, 'T': 1.4972001287890468, 'G': 0.0}\n",
      "Weights: [0. 1. 0. 0.] \n",
      "Counts: {'A': 0.0, 'C': 3.7374108904737104, 'T': 0.0, 'G': 1.2625891095262896}\n",
      "Weights: [0. 0. 1. 0.] \n",
      "Counts: {'A': 1.7682450629257969, 'C': 0.0, 'T': 3.231754937074203, 'G': 0.0}\n",
      "Weights: [0. 0. 0. 1.] \n",
      "Counts: {'A': 0.0, 'C': 1.282116320428015, 'T': 0.0, 'G': 3.717883679571985}\n",
      "Weights: [0.25 0.25 0.25 0.25] \n",
      "Counts: {'A': 1.4399603148107092, 'C': 1.6649600358581778, 'T': 0.5600392050205705, 'G': 1.3350404443105428}\n",
      "Weights: [1. 0. 0. 0.] \n",
      "Counts: {'A': 3.3972457294782656, 'C': 0.0, 'T': 1.6027542705217344, 'G': 0.0}\n",
      "Weights: [0. 1. 0. 0.] \n",
      "Counts: {'A': 0.0, 'C': 3.605290716814805, 'T': 0.0, 'G': 1.3947092831851953}\n",
      "Weights: [0. 0. 1. 0.] \n",
      "Counts: {'A': 1.2118581675598135, 'C': 0.0, 'T': 3.7881418324401865, 'G': 0.0}\n",
      "Weights: [0. 0. 0. 1.] \n",
      "Counts: {'A': 0.0, 'C': 1.2741123149532667, 'T': 0.0, 'G': 3.7258876850467337}\n",
      "Weights: [0.25 0.25 0.25 0.25] \n",
      "Counts: {'A': 1.5489775490075814, 'C': 1.029669537492502, 'T': 1.4510127412545806, 'G': 0.9703401722453359}\n"
     ]
    }
   ],
   "source": [
    "test_fit_dna_hmm(test_dna_sequences)\n",
    "test_sample_and_predict_dna_hmm(test_dna_sequences)\n",
    "test_fit_dna_hmm_from_samples(test_dna_sequences)\n",
    "test_save_and_load_hmm(test_dna_sequences)\n",
    "\n",
    "synthetic_data = np.array([[\"A\", \"A\", \"A\", \"T\"], [\"C\", \"C\", \"C\", \"G\"], [\"T\", \"T\", \"T\", \"A\"], [\"G\", \"G\", \"G\", \"C\"]])\n",
    "test_fit_dna_hmm_weights(synthetic_data)\n",
    "test_fit_dna_hmm_from_samples_weights(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished loading data in 4.51 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_dna_\") #./data/gfp_dna_\n",
    "mutated_df = load_saved_mutated_gfp_data()\n",
    "assert(X_train[0] == get_wild_type_dna_sequence())\n",
    "assert(count_substring_mismatch(X_train[999], get_wild_type_dna_sequence()) == 3)\n",
    "print(\"Finished loading data in {0:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "base_args = hmm_base_args()\n",
    "build_from_samples_args = hmm_build_from_samples_args()\n",
    "test_dna_sequences = np.array([list(seq) for seq in X_train[0:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished loading data in 1.53 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_amino_acid_\") #./data/gfp_amino_acid_\n",
    "assert(X_train[0] == get_wild_type_amino_acid_sequence())\n",
    "assert(count_substring_mismatch(X_train[999], get_wild_type_amino_acid_sequence()) == 3)\n",
    "print(\"Finished loading data in {0:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "base_amino_acid_args = hmm_amino_acid_args()\n",
    "test_amino_acid_sequences = np.array([list(seq) for seq in X_train[0:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
