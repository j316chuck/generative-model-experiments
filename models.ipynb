{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "\n",
    "from pomegranate import State, DiscreteDistribution, HiddenMarkovModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import load_saved_gfp_data, load_saved_mutated_gfp_data, one_hot_decode, one_hot_encode, normalize, count_substring_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeHMM(): \n",
    "    \n",
    "    def __init__(self, args, x_train=None, weights=None, verbose=True): \n",
    "        \"\"\"\n",
    "        Initializes the HMM to perform generative tasks\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the HMM\n",
    "        args.name : string \n",
    "            defines the name of the HMM\n",
    "        args.hidden_size : int \n",
    "            defines the hidden size\n",
    "        args.input : tuple\n",
    "            defines the shape of the inputs (should be 2 or 3 dimensional) \n",
    "        args.max_iterations: int\n",
    "            sets the max iterations\n",
    "        args.n_jobs: int\n",
    "            sets the number of cores to use\n",
    "        args.batch_size : int\n",
    "            sets the batch size\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        args.build_from_samples : boolean\n",
    "            build model from samples\n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.input = args[\"input\"]\n",
    "        self.max_iterations = args[\"max_iterations\"]\n",
    "        self.n_jobs = args[\"n_jobs\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.epoch = args[\"epoch\"]\n",
    "        if args[\"build_from_samples\"] and x_train is not None: \n",
    "            self.model = HiddenMarkovModel.from_samples(DiscreteDistribution, \n",
    "                                                    n_components = self.hidden_size, \n",
    "                                                    X = x_train, \n",
    "                                                    algorithm = 'baum-welch', \n",
    "                                                    return_history = True,\n",
    "                                                    verbose = verbose,\n",
    "                                                    max_iterations = self.max_iterations,\n",
    "                                                    n_jobs = self.n_jobs, \n",
    "                                                    weights = weights,\n",
    "                                                    #batch_size = self.batch_size,\n",
    "                                                    #batches_per_epoch = self.epochs\n",
    "                                               )[0]\n",
    "            \n",
    "        else: \n",
    "            self.build_model()\n",
    "        self.model.bake()\n",
    "\n",
    "    \n",
    "    def build_model(self): \n",
    "        base_pair_lst = [\"A\", \"C\", \"T\", \"G\"]\n",
    "        dists = []\n",
    "        for _ in range(self.hidden_size): \n",
    "            emission_probs = np.random.random(4)\n",
    "            emission_probs = emission_probs / emission_probs.sum()\n",
    "            dists.append(DiscreteDistribution(dict(zip(base_pair_lst, emission_probs))))\n",
    "        trans_mat = np.random.random((self.hidden_size, self.hidden_size))\n",
    "        trans_mat = trans_mat / trans_mat.sum(axis = 1, keepdims = 1)\n",
    "        starts = np.random.random((self.hidden_size))\n",
    "        starts = starts / starts.sum()\n",
    "        # testing initializations\n",
    "        np.testing.assert_almost_equal(starts.sum(), 1)\n",
    "        np.testing.assert_array_almost_equal(np.ones(self.hidden_size), trans_mat.sum(axis = 1))\n",
    "        self.model = HiddenMarkovModel.from_matrix(trans_mat, dists, starts)\n",
    "\n",
    "    def fit(self, x_train, weights=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Assumes x_train is the same shape as the self.input shape. Fits the model on an HMM with self.hidden_size\n",
    "        \"\"\"    \n",
    "        return self.model.fit(x_train, \n",
    "                        algorithm = 'baum-welch', \n",
    "                        return_history = True, \n",
    "                        verbose = verbose,\n",
    "                        max_iterations = self.max_iterations,\n",
    "                        n_jobs = self.n_jobs, \n",
    "                        weights = weights,\n",
    "                        #batch_size = self.batch_size,\n",
    "                        #batches_per_epoch = self.epochs\n",
    "                   )\n",
    "    \n",
    "    def sample(self, n, length):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        n is number of samples\n",
    "        length is how long you want each sample to be\n",
    "        \"\"\"\n",
    "        return np.array([\"\".join(seq) for seq in self.model.sample(n = n, length = length)])\n",
    "            \n",
    "        \n",
    "    def evaluate(self, x_test): \n",
    "        \"\"\"\n",
    "        evaluates the log probability of obtaining the sequences in x_test\n",
    "        log(P(X1, X2, ..., X_test)) = sum(log(P(Xi)))\n",
    "        Input: x_test a list of sequences. should be 2 or 3 dimensional\n",
    "        \"\"\"\n",
    "        assert(len(np.array(x_test).shape) == 2 or len(np.array(x_test).shape) == 3)\n",
    "        return sum([self.model.log_probability(seq) for seq in np.array(x_test)])\n",
    "                \n",
    "    def show_model(self): \n",
    "        self.model.plot()\n",
    "        \n",
    "    def save_model(self, path): \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.model.to_json(), f)\n",
    "    \n",
    "    def load_model(self, path): \n",
    "        with open(path, 'r') as f:\n",
    "            json_model = json.load(f)\n",
    "        self.model = HiddenMarkovModel.from_json(json_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(100, 714)\n",
      "Finished loading data in 4.99 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = load_saved_gfp_data()\n",
    "mutated_df = load_saved_mutated_gfp_data()\n",
    "X_train_sequences = np.array([list(seq) for seq in one_hot_decode(X_train[0:100])])\n",
    "print(X_train_sequences.shape)\n",
    "print(\"Finished loading data in {0:.2f} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_base_args(): \n",
    "    return {\n",
    "        \"name\" : \"base HMM\",\n",
    "        \"hidden_size\" : 5,\n",
    "        \"input\" : (100, 714),\n",
    "        \"max_iterations\" : 10,\n",
    "        \"n_jobs\" : 1,\n",
    "        \"batch_size\" : 5,\n",
    "        \"epoch\" : 2,\n",
    "        \"build_from_samples\" : False\n",
    "    }\n",
    "\n",
    "def hmm_build_from_samples_args(): \n",
    "    args = hmm_base_args()\n",
    "    args[\"build_from_samples\"] = True\n",
    "    return args\n",
    "\n",
    "base_args = hmm_base_args()\n",
    "build_from_samples_args = hmm_build_from_samples_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit_hmm(X_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    \n",
    "def test_fit_hmm_from_samples(X_train_sequences): \n",
    "    args = hmm_build_from_samples_args()\n",
    "    hmm = GenerativeHMM(args, X_train_sequences)\n",
    "    #test max iterations and n_jobs \n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args, X_train_sequences)\n",
    "\n",
    "def test_fit_hmm_weights(X_train_sequences):\n",
    "    args = hmm_base_args()\n",
    "    weights = np.identity(4)\n",
    "    weights = np.vstack([weights, [0.25, 0.25, 0.25, 0.25]])\n",
    "    for weight in weights:\n",
    "        counts = {\"A\" : 0, \"C\" : 0, \"T\" : 0, \"G\" : 0}\n",
    "        hmm = GenerativeHMM(args)\n",
    "        hmm.fit(X_train_sequences, weight, verbose=False)\n",
    "        json_model = json.loads(hmm.model.to_json())\n",
    "        for state in json_model[\"states\"]: \n",
    "            if state is not None and state[\"distribution\"] is not None:\n",
    "                mp = state[\"distribution\"][\"parameters\"][0]\n",
    "                for k, v in mp.items(): \n",
    "                    counts[k] = counts[k] + v\n",
    "        print(\"Weights:\", weight, \"\\nCounts:\", counts)    \n",
    "        \n",
    "\n",
    "def test_fit_hmm_from_samples_weights(X_train_sequences):\n",
    "    args = hmm_build_from_samples_args()\n",
    "    weights = np.identity(4)\n",
    "    weights = np.vstack([weights, [0.25, 0.25, 0.25, 0.25]])\n",
    "    for weight in weights: \n",
    "        counts = {\"A\" : 0, \"C\" : 0, \"T\" : 0, \"G\" : 0}\n",
    "        hmm = GenerativeHMM(args, X_train_sequences, weight, verbose=False)\n",
    "        json_model = json.loads(hmm.model.to_json())\n",
    "        for state in json_model[\"states\"]: \n",
    "            if state is not None and state[\"distribution\"] is not None:\n",
    "                mp = state[\"distribution\"][\"parameters\"][0]\n",
    "                for k, v in mp.items(): \n",
    "                    counts[k] = counts[k] + v\n",
    "        print(\"Weights:\", weight, \"\\nCounts:\", counts) \n",
    "        \n",
    "def test_save_and_load_hmm(x_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences, verbose=False)\n",
    "    hmm.save_model(\"./models/test.json\")\n",
    "    cached_hmm = GenerativeHMM(args)\n",
    "    cached_hmm.load_model(\"./models/test.json\")\n",
    "    for i in \"ACTG\": \n",
    "        for j in \"ACTG\": \n",
    "            for k in \"ACTG\":\n",
    "                codon = i + j + k\n",
    "                np.testing.assert_almost_equal(hmm.evaluate([list(codon)]), cached_hmm.evaluate([list(codon)]))\n",
    "    \n",
    "    \n",
    "def test_sample_and_evaluate_hmm(x_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(x_train_sequences, verbose=False)\n",
    "    seq1, seq2 = tuple(hmm.sample(2, 714))\n",
    "    np.testing.assert_almost_equal(hmm.model.probability(seq1), np.e ** hmm.evaluate([list(seq1)]))\n",
    "    total = 0\n",
    "    for i in \"ACTG\": \n",
    "        for j in \"ACTG\": \n",
    "            for k in \"ACTG\":\n",
    "                codon = i + j + k\n",
    "                np.testing.assert_almost_equal(hmm.model.probability(codon), np.e ** hmm.evaluate([list(codon)]))\n",
    "                total += np.e ** hmm.evaluate([list(codon)])\n",
    "    np.testing.assert_almost_equal(total, 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_and_load_hmm(X_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_and_evaluate_hmm(X_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fit hmm\n",
      "Weights: [1. 0. 0. 0.] \n",
      "Counts: {'A': 3.1852188574167757, 'C': 0.0, 'T': 1.8147811425832239, 'G': 0.0}\n",
      "Weights: [0. 1. 0. 0.] \n",
      "Counts: {'A': 0.0, 'C': 3.2129442322758064, 'T': 0.0, 'G': 1.7870557677241936}\n",
      "Weights: [0. 0. 1. 0.] \n",
      "Counts: {'A': 1.3301250046205513, 'C': 0.0, 'T': 3.6698749953794487, 'G': 0.0}\n",
      "Weights: [0. 0. 0. 1.] \n",
      "Counts: {'A': 0.0, 'C': 1.4471914475195748, 'T': 0.0, 'G': 3.5528085524804256}\n",
      "Weights: [0.25 0.25 0.25 0.25] \n",
      "Counts: {'A': 0.8700817924709296, 'C': 1.5166766089955952, 'T': 1.1299182075290701, 'G': 1.4833233910044048}\n",
      "\n",
      "Test fit hmm from samples weights:\n",
      "Weights: [1. 0. 0. 0.] \n",
      "Counts: {'A': 3.569611728727529, 'C': 0.0, 'T': 1.4303882712724705, 'G': 0.0}\n",
      "Weights: [0. 1. 0. 0.] \n",
      "Counts: {'A': 0.0, 'C': 3.1242679015408688, 'T': 0.0, 'G': 1.875732098459131}\n",
      "Weights: [0. 0. 1. 0.] \n",
      "Counts: {'A': 2.067254058953376, 'C': 0.0, 'T': 2.932745941046624, 'G': 0.0}\n",
      "Weights: [0. 0. 0. 1.] \n",
      "Counts: {'A': 0.0, 'C': 2.0858660777565254, 'T': 0.0, 'G': 2.914133922243474}\n",
      "Weights: [0.25 0.25 0.25 0.25] \n",
      "Counts: {'A': 0.5, 'C': 2.5163178179957364, 'T': 0.5, 'G': 1.4836821820042638}\n"
     ]
    }
   ],
   "source": [
    "print(\"Test fit hmm\")\n",
    "synthetic_data = np.array([[\"A\", \"A\", \"A\", \"T\"], [\"C\", \"C\", \"C\", \"G\"], [\"T\", \"T\", \"T\", \"A\"], [\"G\", \"G\", \"G\", \"C\"]])\n",
    "test_fit_hmm_weights(synthetic_data)\n",
    "print(\"\\nTest fit hmm from samples weights:\")\n",
    "test_fit_hmm_from_samples_weights(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 3299.945057351637\tTime (s): 0.2205\n",
      "[2] Improvement: 17.351066011178773\tTime (s): 0.2179\n",
      "[3] Improvement: 6.922504063040833\tTime (s): 0.2257\n",
      "[4] Improvement: 4.748723868251545\tTime (s): 0.2332\n",
      "[5] Improvement: 4.185291902336758\tTime (s): 0.2087\n",
      "[6] Improvement: 3.9491171033587307\tTime (s): 0.2258\n",
      "[7] Improvement: 3.814449769197381\tTime (s): 0.2134\n",
      "[8] Improvement: 3.73763668758329\tTime (s): 0.2272\n",
      "[9] Improvement: 3.707303385453997\tTime (s): 0.2248\n",
      "[10] Improvement: 3.7189038851793157\tTime (s): 0.2088\n",
      "Total Training Improvement: 3352.0800540272176\n",
      "Total Training Time (s): 2.4884\n",
      "[1] Improvement: 12772.972851785787\tTime (s): 0.08697\n",
      "[2] Improvement: 69.98605718364706\tTime (s): 0.09651\n",
      "[3] Improvement: 59.39522110926919\tTime (s): 0.08275\n",
      "[4] Improvement: 54.511270776056335\tTime (s): 0.1029\n",
      "[5] Improvement: 54.175986628761166\tTime (s): 0.07213\n",
      "[6] Improvement: 56.829158752501826\tTime (s): 0.08672\n",
      "[7] Improvement: 61.81163960597769\tTime (s): 0.1315\n",
      "[8] Improvement: 69.12948869272077\tTime (s): 0.1285\n",
      "[9] Improvement: 79.17780846092501\tTime (s): 0.1303\n",
      "[10] Improvement: 92.64466615155106\tTime (s): 0.1063\n",
      "[11] Improvement: 110.54202187947521\tTime (s): 0.07597\n",
      "[12] Improvement: 134.3389759924612\tTime (s): 0.0991\n",
      "[13] Improvement: 166.2313350024342\tTime (s): 0.142\n",
      "[14] Improvement: 209.63018150079006\tTime (s): 0.1323\n",
      "[15] Improvement: 269.93145891038876\tTime (s): 0.1049\n",
      "[16] Improvement: 355.2581942340621\tTime (s): 0.0889\n",
      "[17] Improvement: 475.2751000956778\tTime (s): 0.08756\n",
      "[18] Improvement: 632.1761923863087\tTime (s): 0.07871\n",
      "[19] Improvement: 796.7447480946576\tTime (s): 0.07626\n",
      "[20] Improvement: 890.8581313731411\tTime (s): 0.08583\n",
      "Total Training Improvement: 17411.620488616594\n",
      "Total Training Time (s): 2.2167\n",
      "[1] Improvement: 3420.1127007446776\tTime (s): 0.2357\n",
      "[2] Improvement: 5.91789303792757\tTime (s): 0.2217\n",
      "[3] Improvement: 4.588074308005162\tTime (s): 0.2195\n",
      "[4] Improvement: 3.9409587036061566\tTime (s): 0.2371\n",
      "[5] Improvement: 3.4536565486632753\tTime (s): 0.2316\n",
      "[6] Improvement: 3.0881537689710967\tTime (s): 0.2108\n",
      "[7] Improvement: 2.818068940396188\tTime (s): 0.2037\n",
      "[8] Improvement: 2.6210477317217737\tTime (s): 0.2065\n",
      "[9] Improvement: 2.4796946961869253\tTime (s): 0.2426\n",
      "[10] Improvement: 2.381252070161281\tTime (s): 0.243\n",
      "Total Training Improvement: 3451.401500550317\n",
      "Total Training Time (s): 2.5757\n",
      "[1] Improvement: 6399.301047290006\tTime (s): 0.0857\n",
      "[2] Improvement: 13.275885748094879\tTime (s): 0.07422\n",
      "[3] Improvement: 9.91231723225792\tTime (s): 0.09729\n",
      "[4] Improvement: 8.334665011672769\tTime (s): 0.1553\n",
      "[5] Improvement: 7.437921725213528\tTime (s): 0.1023\n",
      "[6] Improvement: 6.843134394366643\tTime (s): 0.08281\n",
      "[7] Improvement: 6.4256821297021816\tTime (s): 0.09111\n",
      "[8] Improvement: 6.135920135246124\tTime (s): 0.08009\n",
      "[9] Improvement: 5.944983230801881\tTime (s): 0.08056\n",
      "[10] Improvement: 5.830979237522115\tTime (s): 0.08318\n",
      "[11] Improvement: 5.776143942872295\tTime (s): 0.09332\n",
      "[12] Improvement: 5.766367982898373\tTime (s): 0.09504\n",
      "[13] Improvement: 5.791005343577126\tTime (s): 0.0992\n",
      "[14] Improvement: 5.842602322372841\tTime (s): 0.139\n",
      "[15] Improvement: 5.916541841375874\tTime (s): 0.144\n",
      "[16] Improvement: 6.010631882585585\tTime (s): 0.1088\n",
      "[17] Improvement: 6.124659310997231\tTime (s): 0.07601\n",
      "[18] Improvement: 6.2599334064580034\tTime (s): 0.1045\n",
      "[19] Improvement: 6.418852563816472\tTime (s): 0.1238\n",
      "[20] Improvement: 6.604531492717797\tTime (s): 0.1072\n",
      "Total Training Improvement: 6529.953806224556\n",
      "Total Training Time (s): 2.2437\n"
     ]
    }
   ],
   "source": [
    "test_fit_hmm(X_train_sequences)\n",
    "test_fit_hmm_from_samples(X_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
