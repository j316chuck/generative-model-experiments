Training rnn_default_small 
Args:
args -- {'model_type': 'rnn', 'base_log': 'logs/synthetic_unimodal_data_length_100_skewed_gaussian/rnn', 'name': 'rnn_default_small', 'input': 2000, 'hidden_size': 100, 'latent_dim': -1, 'seq_length': 100, 'pseudo_count': 1, 'n_jobs': 1, 'device': device(type='cpu'), 'learning_rate': 0.001, 'epochs': 10, 'batch_size': 10, 'layers': 1, 'dataset': 'synthetic_unimodal_data_length_100_skewed_gaussian', 'num_data': 100, 'vocabulary': 'ACDEFGHIKLMNPQRSTVWY', 'wild_type': 'MQKPCKENEGKPKCSVPKREEKRPYGEFERQQTEGNFRQRLLQSLEEFKEDIDYRHFKDEEMTREGDEMERCLEEIRGLRKKFRALHSNHRHSRDRPYPI'}
save_epochs -- 50
model_type -- rnn
base_log -- logs/synthetic_unimodal_data_length_100_skewed_gaussian/rnn
name -- rnn_default_small
input -- 2000
hidden_size -- 100
latent_dim -- -1
seq_length -- 100
pseudo_count -- 1
n_jobs -- 1
device -- cpu
learning_rate -- 0.001
epochs -- 10
batch_size -- 10
layers -- 1
dataset -- synthetic_unimodal_data_length_100_skewed_gaussian
num_data -- 100
all_characters -- ACDEFGHIKLMNPQRSTVWY
num_characters -- 20
character_to_int -- {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}
int_to_character -- {0: 'A', 1: 'C', 2: 'D', 3: 'E', 4: 'F', 5: 'G', 6: 'H', 7: 'I', 8: 'K', 9: 'L', 10: 'M', 11: 'N', 12: 'P', 13: 'Q', 14: 'R', 15: 'S', 16: 'T', 17: 'V', 18: 'W', 19: 'Y'}
indexes -- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
train_loss_history -- []
valid_loss_history -- []
initial_probs_tensor -- []
initial_probs -- {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 1.0}
model -- RNN(
  (encoder): Embedding(20, 100)
  (rnn): LSTM(100, 100)
  (decoder): Linear(in_features=100, out_features=20, bias=True)
)
optimizer -- Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
criterion -- CrossEntropyLoss()
**************************************************
training model on train and validation datasets...
epoch 1, train neg log prob: 282.7114, test neg log probability 266.4847, time: 5.57 sec
epoch 2, train neg log prob: 252.4418, test neg log probability 238.6228, time: 10.85 sec
epoch 3, train neg log prob: 224.2298, test neg log probability 209.4618, time: 15.96 sec
epoch 4, train neg log prob: 193.5383, test neg log probability 178.1128, time: 21.01 sec
epoch 5, train neg log prob: 161.6254, test neg log probability 147.5586, time: 26.23 sec
epoch 6, train neg log prob: 132.1115, test neg log probability 121.3066, time: 31.62 sec
epoch 7, train neg log prob: 107.6798, test neg log probability 100.8202, time: 36.64 sec
epoch 8, train neg log prob: 88.9859, test neg log probability 85.7262, time: 42.01 sec
epoch 9, train neg log prob: 75.4987, test neg log probability 75.5596, time: 47.26 sec
epoch 10, train neg log prob: 66.3580, test neg log probability 68.4829, time: 52.24 sec
**************************************************
evaluating model on test dataset:
total loss: 68.1271
