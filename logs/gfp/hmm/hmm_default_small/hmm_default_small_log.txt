training hmm model.
number of parameters -- 1530.0.
model args:
**************************************************
save_epochs -- 50
model_type -- hmm
base_log -- logs/gfp/hmm/
name -- hmm_default_small
input -- 4998
hidden_size -- 30
latent_dim -- -1
seq_length -- 238
pseudo_count -- 1
n_jobs -- 1
device -- cpu
learning_rate -- 0.001
epochs -- 10
batch_size -- 10
layers -- 1
dataset -- gfp
num_data -- 100
all_characters -- *ACDEFGHIKLMNPQRSTVWY
early_stopping -- <early_stopping.EarlyStopping object at 0x1130fac18>
patience -- 10
num_characters -- 21
indexes -- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
train_loss_history -- []
valid_loss_history -- []
train_loss -- []
**************************************************
training model on train and validation datasets...
epoch 1, train neg log prob: 684.1085, test neg log probability 684.3016, time: 7.55 sec
epoch 2, train neg log prob: 683.6139, test neg log probability 683.8111, time: 15.45 sec
epoch 3, train neg log prob: 683.0482, test neg log probability 683.2499, time: 23.06 sec
epoch 4, train neg log prob: 682.2607, test neg log probability 682.4685, time: 30.37 sec
epoch 5, train neg log prob: 681.1764, test neg log probability 681.3942, time: 39.18 sec
epoch 6, train neg log prob: 679.8114, test neg log probability 680.0450, time: 46.64 sec
epoch 7, train neg log prob: 677.6780, test neg log probability 677.9387, time: 54.19 sec
epoch 8, train neg log prob: 673.3230, test neg log probability 673.6406, time: 62.40 sec
epoch 9, train neg log prob: 663.0383, test neg log probability 663.4997, time: 70.36 sec
epoch 10, train neg log prob: 644.1817, test neg log probability 644.9919, time: 77.74 sec
**************************************************
evaluating model on test dataset:
Average neg log prob: 645.1025
**************************************************
average mismatches per sample: 222.7070
