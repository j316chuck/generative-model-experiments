Training rnn_default_medium 
Args:
args -- {'model_type': 'rnn', 'base_log': 'logs/gfp/rnn/', 'name': 'rnn_default_medium', 'input': 4998, 'hidden_size': 200, 'latent_dim': -1, 'seq_length': 238, 'pseudo_count': 1, 'n_jobs': 1, 'device': device(type='cpu'), 'learning_rate': 0.001, 'epochs': 100, 'batch_size': 10, 'layers': 1, 'dataset': 'gfp', 'num_data': 1000, 'early_stopping': <early_stopping.EarlyStopping object at 0x115439588>, 'patience': 10, 'vocabulary': '*ACDEFGHIKLMNPQRSTVWY', 'wild_type': 'SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*'}
save_epochs -- 50
model_type -- rnn
base_log -- logs/gfp/rnn/
name -- rnn_default_medium
input -- 4998
hidden_size -- 200
latent_dim -- -1
seq_length -- 238
pseudo_count -- 1
n_jobs -- 1
device -- cpu
learning_rate -- 0.001
epochs -- 100
batch_size -- 10
layers -- 1
dataset -- gfp
num_data -- 1000
all_characters -- *ACDEFGHIKLMNPQRSTVWY
early_stopping -- <early_stopping.EarlyStopping object at 0x115439588>
patience -- 10
num_characters -- 21
character_to_int -- {'*': 0, 'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}
int_to_character -- {0: '*', 1: 'A', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'K', 10: 'L', 11: 'M', 12: 'N', 13: 'P', 14: 'Q', 15: 'R', 16: 'S', 17: 'T', 18: 'V', 19: 'W', 20: 'Y'}
indexes -- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
train_loss_history -- []
valid_loss_history -- []
initial_probs_tensor -- []
initial_probs -- {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 1.0, 20: 1.0}
model -- RNN(
  (encoder): Embedding(21, 200)
  (rnn): LSTM(200, 200)
  (decoder): Linear(in_features=200, out_features=21, bias=True)
)
optimizer -- Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
criterion -- CrossEntropyLoss()
**************************************************
training model on train and validation datasets...
epoch 1, train neg log prob: 249.7980, test neg log probability 48.5980, time: 2364.68 sec
epoch 2, train neg log prob: 40.3469, test neg log probability 36.0280, time: 2520.25 sec
epoch 3, train neg log prob: 33.0998, test neg log probability 31.6350, time: 2629.99 sec
epoch 4, train neg log prob: 29.9106, test neg log probability 29.4999, time: 2737.05 sec
epoch 5, train neg log prob: 28.1198, test neg log probability 28.2203, time: 2855.20 sec
epoch 6, train neg log prob: 26.9193, test neg log probability 27.3901, time: 2970.30 sec
epoch 7, train neg log prob: 26.1510, test neg log probability 26.8160, time: 3075.44 sec
epoch 8, train neg log prob: 25.5531, test neg log probability 26.2958, time: 3180.79 sec
epoch 9, train neg log prob: 25.1193, test neg log probability 26.0225, time: 3285.02 sec
epoch 10, train neg log prob: 24.8455, test neg log probability 25.8970, time: 3392.67 sec
epoch 11, train neg log prob: 24.5342, test neg log probability 25.5991, time: 3509.95 sec
epoch 12, train neg log prob: 24.3315, test neg log probability 25.4731, time: 3618.53 sec
epoch 13, train neg log prob: 24.1844, test neg log probability 25.3683, time: 3726.27 sec
epoch 14, train neg log prob: 23.9704, test neg log probability 25.3380, time: 3834.02 sec
epoch 15, train neg log prob: 23.8864, test neg log probability 25.1318, time: 3940.74 sec
epoch 16, train neg log prob: 23.6871, test neg log probability 25.2116, time: 4051.77 sec
epoch 17, train neg log prob: 23.6257, test neg log probability 25.0451, time: 4160.81 sec
epoch 18, train neg log prob: 23.5357, test neg log probability 25.0534, time: 4268.92 sec
epoch 19, train neg log prob: 23.4499, test neg log probability 24.9344, time: 4377.44 sec
epoch 20, train neg log prob: 23.3523, test neg log probability 24.9148, time: 4483.83 sec
epoch 21, train neg log prob: 23.2563, test neg log probability 24.9041, time: 4609.90 sec
epoch 22, train neg log prob: 23.1921, test neg log probability 24.9482, time: 4721.44 sec
epoch 23, train neg log prob: 23.1359, test neg log probability 25.0152, time: 4825.55 sec
epoch 24, train neg log prob: 23.1039, test neg log probability 24.9606, time: 4928.38 sec
epoch 25, train neg log prob: 22.9608, test neg log probability 24.8423, time: 5039.46 sec
epoch 26, train neg log prob: 22.9098, test neg log probability 24.9198, time: 5142.67 sec
epoch 27, train neg log prob: 22.8984, test neg log probability 24.9746, time: 5250.47 sec
epoch 28, train neg log prob: 22.8573, test neg log probability 24.9510, time: 5353.79 sec
epoch 29, train neg log prob: 22.7894, test neg log probability 24.9222, time: 5458.42 sec
epoch 30, train neg log prob: 22.7273, test neg log probability 24.8693, time: 5563.07 sec
epoch 31, train neg log prob: 22.6758, test neg log probability 24.9123, time: 5670.49 sec
epoch 32, train neg log prob: 22.6081, test neg log probability 24.9159, time: 5773.41 sec
epoch 33, train neg log prob: 22.5831, test neg log probability 24.9557, time: 5882.39 sec
epoch 34, train neg log prob: 22.5131, test neg log probability 24.9523, time: 5989.53 sec
epoch 35, train neg log prob: 22.5083, test neg log probability 24.8659, time: 6090.85 sec
--------------------------------------------------
early stopped at epoch 35
loading model from epoch 25
--------------------------------------------------
**************************************************
evaluating model on test dataset:
total loss: 25.3033
