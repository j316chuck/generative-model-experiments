{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from seqtools import SequenceTools as ST\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y, count_substring_mismatch\n",
    "from util import get_gfp_X_y_aa, load_gfp_data, one_hot_encode_aa, one_hot_encode, get_all_amino_acids, get_wild_type_amino_acid_sequence\n",
    "from losses import neg_log_likelihood\n",
    "import json\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def evaluate_ground_truth(X_aa, ground_truth, save_file=None):\n",
    "    y_gt = ground_truth.predict(X_aa, print_every=100000)[:, 0]\n",
    "    if save_file is not None:\n",
    "        np.save(save_file, y_gt)\n",
    "        \n",
    "def train_and_save_oracles(X_train, y_train, n=10, suffix='', batch_size=100):\n",
    "    for i in range(n):\n",
    "        model = build_model(X_train.shape[1])\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=neg_log_likelihood,\n",
    "                      )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=0, \n",
    "                                   patience=5, \n",
    "                                   verbose=1)\n",
    "\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs=100, \n",
    "                  batch_size=batch_size, \n",
    "                  validation_split=0.1, \n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=2)\n",
    "        model.save(\"models/oracle_%i%s.h5\" % (i, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ml_opt(X_train, oracles, ground_truth, vae_0, weights_type='dbas',\n",
    "        LD=20, iters=20, samples=500, homoscedastic=False, homo_y_var=0.1,\n",
    "        quantile=0.95, verbose=False, alpha=1, train_gt_evals=None,\n",
    "        cutoff=1e-6, it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['cbas', 'dbas','rwr', 'cem-pi', 'fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = - np.inf\n",
    "    for t in range(iters):\n",
    "        ### Take Samples ###\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_p = vae.decoder_.predict(zt)\n",
    "            Xt = get_samples(Xt_p)\n",
    "        else:\n",
    "            Xt = X_train\n",
    "        \n",
    "        ### Evaluate ground truth and oracle ###\n",
    "        yt, yt_var = get_balaji_predictions(oracles, Xt)\n",
    "        if homoscedastic:\n",
    "            yt_var = np.ones_like(yt) * homo_y_var\n",
    "        Xt_aa = np.argmax(Xt, axis=-1)\n",
    "        if t == 0 and train_gt_evals is not None:\n",
    "            yt_gt = train_gt_evals\n",
    "        else:\n",
    "            yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate weights for different schemes ###\n",
    "        if t > 0:\n",
    "            if weights_type == 'cbas': \n",
    "                log_pxt = np.sum(np.log(Xt_p) * Xt, axis=(1, 2))\n",
    "                X0_p = vae_0.decoder_.predict(zt)\n",
    "                log_px0 = np.sum(np.log(X0_p)*Xt, axis=(1, 2))\n",
    "                w1 = np.exp(log_px0-log_pxt)\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                w2= scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "                weights = w1*w2 \n",
    "            elif weights_type == 'cem-pi':\n",
    "                pi = scipy.stats.norm.sf(max_train_gt, loc=yt, scale=np.sqrt(yt_var))\n",
    "                pi_thresh = np.percentile(pi, quantile*100)\n",
    "                weights = (pi > pi_thresh).astype(int)\n",
    "            elif weights_type == 'dbas':\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                weights = scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "            elif weights_type == 'rwr':\n",
    "                weights = np.exp(alpha*yt)\n",
    "                weights /= np.sum(weights)\n",
    "#             elif weights_type == ''\n",
    "        else:\n",
    "            weights = np.ones(yt.shape[0])\n",
    "            max_train_gt = np.max(yt_gt)\n",
    "            \n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "        if t == 0:\n",
    "            rand_idx = np.random.randint(0, len(yt), samples)\n",
    "            oracle_samples[t, :] = yt[rand_idx]\n",
    "            gt_samples[t, :] = yt_gt[rand_idx]\n",
    "        if t > 0:\n",
    "            oracle_samples[t, :] = yt\n",
    "            gt_samples[t, :] = yt_gt\n",
    "        \n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        traj[t, 6] = np.mean(yt_var)\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6])\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "            cutoff_idx = np.where(weights < cutoff)\n",
    "            Xt = np.delete(Xt, cutoff_idx, axis=0)\n",
    "            yt = np.delete(yt, cutoff_idx, axis=0)\n",
    "            weights = np.delete(weights, cutoff_idx, axis=0)\n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=it_epochs,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  sample_weight=[weights, weights],\n",
    "                  verbose=0)\n",
    "            \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_opt(X_train, oracles, ground_truth, vae_0, weights_type='fbvae',\n",
    "        LD=20, iters=20, samples=500, \n",
    "        quantile=0.8, verbose=False, train_gt_evals=None,\n",
    "        it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = - np.inf\n",
    "    for t in range(iters):\n",
    "        ### Take Samples and evaluate ground truth and oracle ##\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_sample_p = vae.decoder_.predict(zt)\n",
    "            Xt_sample = get_samples(Xt_sample_p)\n",
    "            yt_sample, _ = get_balaji_predictions(oracles, Xt_sample)\n",
    "            Xt_aa_sample = np.argmax(Xt_sample, axis=-1)\n",
    "            yt_gt_sample = ground_truth.predict(Xt_aa_sample, print_every=1000000)[:, 0]\n",
    "        else:\n",
    "            Xt = X_train\n",
    "            yt, _ = get_balaji_predictions(oracles, Xt)\n",
    "            Xt_aa = np.argmax(Xt, axis=-1)\n",
    "            fb_thresh = np.percentile(yt, quantile*100)\n",
    "            if train_gt_evals is not None:\n",
    "                yt_gt = train_gt_evals\n",
    "            else:\n",
    "                yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate threshold ###\n",
    "        if t > 0:\n",
    "            threshold_idx = np.where(yt_sample >= fb_thresh)[0]\n",
    "            n_top = len(threshold_idx)\n",
    "            sample_arrs = [Xt_sample, yt_sample, yt_gt_sample, Xt_aa_sample]\n",
    "            full_arrs = [Xt, yt, yt_gt, Xt_aa]\n",
    "            \n",
    "            for l in range(len(full_arrs)):\n",
    "                sample_arr = sample_arrs[l]\n",
    "                full_arr = full_arrs[l]\n",
    "                sample_top = sample_arr[threshold_idx]\n",
    "                full_arr = np.concatenate([sample_top, full_arr])\n",
    "                full_arr = np.delete(full_arr, range(full_arr.shape[0]-n_top, full_arr.shape[0]), axis=0)\n",
    "                full_arrs[l] = full_arr\n",
    "            Xt, yt, yt_gt, Xt_aa = full_arrs\n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "\n",
    "        rand_idx = np.random.randint(0, len(yt), samples)\n",
    "        oracle_samples[t, :] = yt[rand_idx]\n",
    "        gt_samples[t, :] = yt_gt[rand_idx]\n",
    "\n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        if t > 0:\n",
    "            traj[t, 6] = n_top\n",
    "        else:\n",
    "            traj[t, 6] = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6])\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "        \n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=1,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  verbose=0)\n",
    "            \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_oracles():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    i = 1\n",
    "    num_models = [1, 5, 20]\n",
    "    for i in range(len(num_models)):\n",
    "        RANDOM_STATE = i+1\n",
    "        nm = num_models[i]\n",
    "        X_train, y_train, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        suffix = '_%s_%i_%i' % (train_size_str, nm, RANDOM_STATE)\n",
    "        train_and_save_oracles(X_train, y_train, batch_size=10, n=nm, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_vaes():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    suffix = '_%s' % train_size_str\n",
    "    for i in [0, 2]:\n",
    "        RANDOM_STATE = i + 1\n",
    "        X_train, _, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "        vae_0.fit([X_train], [X_train, np.zeros(X_train.shape[0])],\n",
    "                  epochs=100,\n",
    "                  batch_size=10,\n",
    "                  verbose=2)\n",
    "        vae_0.encoder_.save_weights(\"models/vae_0_encoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.decoder_.save_weights(\"models/vae_0_decoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.vae_.save_weights(\"models/vae_0_vae_weights%s_%i.h5\"% (suffix, RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 1100\n",
    "RANDOM_STATE = 1\n",
    "X_train, _, _, _  = load_gfp_data(\"../../data/gfp_amino_acid_shuffle_\")\n",
    "X_train = one_hot_encode(X_train[0:TRAIN_SIZE], get_all_amino_acids())\n",
    "X_train = X_train.reshape(X_train.shape[0], -1, len(get_all_amino_acids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 990 samples, validate on 110 samples\n",
      "Epoch 1/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 145.5428 - decoder_loss: 112.5334 - kl_reshape_loss: 33.0094 - val_loss: 45.3359 - val_decoder_loss: 32.4940 - val_kl_reshape_loss: 12.8419\n",
      "Epoch 2/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 41.6371 - decoder_loss: 31.6036 - kl_reshape_loss: 10.0334 - val_loss: 37.7382 - val_decoder_loss: 28.5228 - val_kl_reshape_loss: 9.2154\n",
      "Epoch 3/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 35.9248 - decoder_loss: 29.0358 - kl_reshape_loss: 6.8890 - val_loss: 34.3560 - val_decoder_loss: 27.5166 - val_kl_reshape_loss: 6.8394\n",
      "Epoch 4/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 34.0442 - decoder_loss: 27.5965 - kl_reshape_loss: 6.4477 - val_loss: 32.3605 - val_decoder_loss: 27.1479 - val_kl_reshape_loss: 5.2126\n",
      "Epoch 5/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 33.5253 - decoder_loss: 26.9701 - kl_reshape_loss: 6.5552 - val_loss: 31.4411 - val_decoder_loss: 26.8830 - val_kl_reshape_loss: 4.5581\n",
      "Epoch 6/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 31.2755 - decoder_loss: 26.1472 - kl_reshape_loss: 5.1283 - val_loss: 30.2299 - val_decoder_loss: 25.5581 - val_kl_reshape_loss: 4.6718\n",
      "Epoch 7/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 30.3608 - decoder_loss: 25.6267 - kl_reshape_loss: 4.7341 - val_loss: 29.9877 - val_decoder_loss: 25.0424 - val_kl_reshape_loss: 4.9454\n",
      "Epoch 8/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 29.3847 - decoder_loss: 25.2811 - kl_reshape_loss: 4.1036 - val_loss: 28.9865 - val_decoder_loss: 24.7468 - val_kl_reshape_loss: 4.2397\n",
      "Epoch 9/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 29.2093 - decoder_loss: 25.1023 - kl_reshape_loss: 4.1070 - val_loss: 28.7559 - val_decoder_loss: 24.9801 - val_kl_reshape_loss: 3.7758\n",
      "Epoch 10/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 28.2861 - decoder_loss: 24.7932 - kl_reshape_loss: 3.4930 - val_loss: 30.8399 - val_decoder_loss: 27.8890 - val_kl_reshape_loss: 2.9509\n",
      "Epoch 11/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 28.0642 - decoder_loss: 24.7396 - kl_reshape_loss: 3.3246 - val_loss: 27.7105 - val_decoder_loss: 24.7750 - val_kl_reshape_loss: 2.9355\n",
      "Epoch 12/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 27.8033 - decoder_loss: 24.7625 - kl_reshape_loss: 3.0407 - val_loss: 30.5045 - val_decoder_loss: 25.0647 - val_kl_reshape_loss: 5.4398\n",
      "Epoch 13/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 27.6037 - decoder_loss: 24.2665 - kl_reshape_loss: 3.3371 - val_loss: 26.8613 - val_decoder_loss: 24.2424 - val_kl_reshape_loss: 2.6189\n",
      "Epoch 14/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 26.7496 - decoder_loss: 24.2133 - kl_reshape_loss: 2.5363 - val_loss: 26.6876 - val_decoder_loss: 24.4289 - val_kl_reshape_loss: 2.2587\n",
      "Epoch 15/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 26.4569 - decoder_loss: 24.0541 - kl_reshape_loss: 2.4028 - val_loss: 26.6274 - val_decoder_loss: 24.4877 - val_kl_reshape_loss: 2.1397\n",
      "Epoch 16/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 26.3873 - decoder_loss: 24.1080 - kl_reshape_loss: 2.2793 - val_loss: 27.0403 - val_decoder_loss: 24.1842 - val_kl_reshape_loss: 2.8561\n",
      "Epoch 17/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 26.2533 - decoder_loss: 23.7975 - kl_reshape_loss: 2.4558 - val_loss: 26.0286 - val_decoder_loss: 24.2008 - val_kl_reshape_loss: 1.8277\n",
      "Epoch 18/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 25.2684 - decoder_loss: 23.6666 - kl_reshape_loss: 1.6018 - val_loss: 25.8525 - val_decoder_loss: 24.2227 - val_kl_reshape_loss: 1.6298\n",
      "Epoch 19/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 25.1839 - decoder_loss: 23.7128 - kl_reshape_loss: 1.4712 - val_loss: 25.7380 - val_decoder_loss: 24.2757 - val_kl_reshape_loss: 1.4622\n",
      "Epoch 20/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 25.1635 - decoder_loss: 23.6572 - kl_reshape_loss: 1.5063 - val_loss: 25.5994 - val_decoder_loss: 24.3718 - val_kl_reshape_loss: 1.2276\n",
      "Epoch 21/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.9465 - decoder_loss: 23.4569 - kl_reshape_loss: 1.4896 - val_loss: 25.4886 - val_decoder_loss: 24.2099 - val_kl_reshape_loss: 1.2787\n",
      "Epoch 22/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.8153 - decoder_loss: 23.5360 - kl_reshape_loss: 1.2793 - val_loss: 25.3588 - val_decoder_loss: 24.1667 - val_kl_reshape_loss: 1.1921\n",
      "Epoch 23/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.3403 - decoder_loss: 23.3231 - kl_reshape_loss: 1.0173 - val_loss: 24.9323 - val_decoder_loss: 24.1238 - val_kl_reshape_loss: 0.8085\n",
      "Epoch 24/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.3281 - decoder_loss: 23.2993 - kl_reshape_loss: 1.0288 - val_loss: 24.5746 - val_decoder_loss: 23.6954 - val_kl_reshape_loss: 0.8792\n",
      "Epoch 25/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.1439 - decoder_loss: 23.2432 - kl_reshape_loss: 0.9008 - val_loss: 24.9394 - val_decoder_loss: 24.1273 - val_kl_reshape_loss: 0.8121\n",
      "Epoch 26/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.0059 - decoder_loss: 23.1606 - kl_reshape_loss: 0.8453 - val_loss: 24.7838 - val_decoder_loss: 24.1118 - val_kl_reshape_loss: 0.6719\n",
      "Epoch 27/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 24.4443 - decoder_loss: 23.2682 - kl_reshape_loss: 1.1761 - val_loss: 24.6829 - val_decoder_loss: 24.0577 - val_kl_reshape_loss: 0.6252\n",
      "Epoch 28/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.8480 - decoder_loss: 23.1737 - kl_reshape_loss: 0.6743 - val_loss: 24.4815 - val_decoder_loss: 23.8185 - val_kl_reshape_loss: 0.6629\n",
      "Epoch 29/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.6723 - decoder_loss: 23.0760 - kl_reshape_loss: 0.5963 - val_loss: 24.3675 - val_decoder_loss: 23.8988 - val_kl_reshape_loss: 0.4687\n",
      "Epoch 30/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.5474 - decoder_loss: 23.0028 - kl_reshape_loss: 0.5446 - val_loss: 24.2940 - val_decoder_loss: 23.7991 - val_kl_reshape_loss: 0.4948\n",
      "Epoch 31/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.5240 - decoder_loss: 23.0194 - kl_reshape_loss: 0.5046 - val_loss: 24.2795 - val_decoder_loss: 23.9250 - val_kl_reshape_loss: 0.3545\n",
      "Epoch 32/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.3515 - decoder_loss: 22.9293 - kl_reshape_loss: 0.4222 - val_loss: 23.9710 - val_decoder_loss: 23.6590 - val_kl_reshape_loss: 0.3120\n",
      "Epoch 33/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.2403 - decoder_loss: 22.8969 - kl_reshape_loss: 0.3434 - val_loss: 23.8727 - val_decoder_loss: 23.6812 - val_kl_reshape_loss: 0.1915\n",
      "Epoch 34/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.2086 - decoder_loss: 22.8806 - kl_reshape_loss: 0.3280 - val_loss: 23.8813 - val_decoder_loss: 23.7114 - val_kl_reshape_loss: 0.1699\n",
      "Epoch 35/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.0554 - decoder_loss: 22.8059 - kl_reshape_loss: 0.2495 - val_loss: 23.8148 - val_decoder_loss: 23.6626 - val_kl_reshape_loss: 0.1522\n",
      "Epoch 36/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.9942 - decoder_loss: 22.7578 - kl_reshape_loss: 0.2364 - val_loss: 23.8718 - val_decoder_loss: 23.7443 - val_kl_reshape_loss: 0.1275\n",
      "Epoch 37/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 23.0391 - decoder_loss: 22.7911 - kl_reshape_loss: 0.2480 - val_loss: 23.8050 - val_decoder_loss: 23.6605 - val_kl_reshape_loss: 0.1445\n",
      "Epoch 38/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.9403 - decoder_loss: 22.6966 - kl_reshape_loss: 0.2437 - val_loss: 23.6769 - val_decoder_loss: 23.5470 - val_kl_reshape_loss: 0.1299\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 2s 2ms/step - loss: 22.9111 - decoder_loss: 22.6474 - kl_reshape_loss: 0.2638 - val_loss: 23.8746 - val_decoder_loss: 23.7447 - val_kl_reshape_loss: 0.1299\n",
      "Epoch 40/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.9709 - decoder_loss: 22.6851 - kl_reshape_loss: 0.2858 - val_loss: 23.8749 - val_decoder_loss: 23.7412 - val_kl_reshape_loss: 0.1336\n",
      "Epoch 41/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.8926 - decoder_loss: 22.5923 - kl_reshape_loss: 0.3002 - val_loss: 23.8530 - val_decoder_loss: 23.6950 - val_kl_reshape_loss: 0.1581\n",
      "Epoch 42/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.7853 - decoder_loss: 22.4732 - kl_reshape_loss: 0.3121 - val_loss: 23.8464 - val_decoder_loss: 23.7068 - val_kl_reshape_loss: 0.1397\n",
      "Epoch 43/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.8302 - decoder_loss: 22.4693 - kl_reshape_loss: 0.3609 - val_loss: 23.8234 - val_decoder_loss: 23.6447 - val_kl_reshape_loss: 0.1787\n",
      "Epoch 44/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.7729 - decoder_loss: 22.4056 - kl_reshape_loss: 0.3673 - val_loss: 23.8758 - val_decoder_loss: 23.7078 - val_kl_reshape_loss: 0.1680\n",
      "Epoch 45/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.8066 - decoder_loss: 22.4016 - kl_reshape_loss: 0.4049 - val_loss: 23.9253 - val_decoder_loss: 23.6938 - val_kl_reshape_loss: 0.2315\n",
      "Epoch 46/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.7869 - decoder_loss: 22.3874 - kl_reshape_loss: 0.3995 - val_loss: 23.8787 - val_decoder_loss: 23.6913 - val_kl_reshape_loss: 0.1874\n",
      "Epoch 47/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.7306 - decoder_loss: 22.2704 - kl_reshape_loss: 0.4602 - val_loss: 24.0405 - val_decoder_loss: 23.8328 - val_kl_reshape_loss: 0.2076\n",
      "Epoch 48/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.7419 - decoder_loss: 22.2887 - kl_reshape_loss: 0.4532 - val_loss: 24.0721 - val_decoder_loss: 23.8667 - val_kl_reshape_loss: 0.2054\n",
      "Epoch 49/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.7143 - decoder_loss: 22.2036 - kl_reshape_loss: 0.5108 - val_loss: 23.8558 - val_decoder_loss: 23.6596 - val_kl_reshape_loss: 0.1961\n",
      "Epoch 50/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.6525 - decoder_loss: 22.1076 - kl_reshape_loss: 0.5449 - val_loss: 23.8246 - val_decoder_loss: 23.5830 - val_kl_reshape_loss: 0.2416\n",
      "Epoch 51/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.6265 - decoder_loss: 22.0531 - kl_reshape_loss: 0.5733 - val_loss: 23.9630 - val_decoder_loss: 23.7197 - val_kl_reshape_loss: 0.2433\n",
      "Epoch 52/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.6033 - decoder_loss: 21.9731 - kl_reshape_loss: 0.6301 - val_loss: 24.0735 - val_decoder_loss: 23.7711 - val_kl_reshape_loss: 0.3024\n",
      "Epoch 53/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.5292 - decoder_loss: 21.8465 - kl_reshape_loss: 0.6827 - val_loss: 23.9961 - val_decoder_loss: 23.6948 - val_kl_reshape_loss: 0.3014\n",
      "Epoch 54/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.4769 - decoder_loss: 21.7389 - kl_reshape_loss: 0.7380 - val_loss: 24.3063 - val_decoder_loss: 23.9012 - val_kl_reshape_loss: 0.4051\n",
      "Epoch 55/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.4390 - decoder_loss: 21.6433 - kl_reshape_loss: 0.7957 - val_loss: 24.1917 - val_decoder_loss: 23.7755 - val_kl_reshape_loss: 0.4162\n",
      "Epoch 56/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.5727 - decoder_loss: 21.6875 - kl_reshape_loss: 0.8851 - val_loss: 24.3996 - val_decoder_loss: 23.9971 - val_kl_reshape_loss: 0.4025\n",
      "Epoch 57/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.4614 - decoder_loss: 21.5517 - kl_reshape_loss: 0.9097 - val_loss: 24.3253 - val_decoder_loss: 23.8910 - val_kl_reshape_loss: 0.4343\n",
      "Epoch 58/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.3847 - decoder_loss: 21.4123 - kl_reshape_loss: 0.9724 - val_loss: 24.0142 - val_decoder_loss: 23.5121 - val_kl_reshape_loss: 0.5021\n",
      "Epoch 59/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.3532 - decoder_loss: 21.2997 - kl_reshape_loss: 1.0536 - val_loss: 24.1202 - val_decoder_loss: 23.6016 - val_kl_reshape_loss: 0.5186\n",
      "Epoch 60/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.2532 - decoder_loss: 21.1330 - kl_reshape_loss: 1.1202 - val_loss: 24.1526 - val_decoder_loss: 23.6343 - val_kl_reshape_loss: 0.5183\n",
      "Epoch 61/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.2391 - decoder_loss: 21.0426 - kl_reshape_loss: 1.1965 - val_loss: 24.5373 - val_decoder_loss: 23.9338 - val_kl_reshape_loss: 0.6035\n",
      "Epoch 62/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.1227 - decoder_loss: 20.8597 - kl_reshape_loss: 1.2630 - val_loss: 24.3025 - val_decoder_loss: 23.7073 - val_kl_reshape_loss: 0.5952\n",
      "Epoch 63/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.1357 - decoder_loss: 20.7909 - kl_reshape_loss: 1.3448 - val_loss: 24.4725 - val_decoder_loss: 23.8737 - val_kl_reshape_loss: 0.5989\n",
      "Epoch 64/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.1050 - decoder_loss: 20.6477 - kl_reshape_loss: 1.4572 - val_loss: 24.9880 - val_decoder_loss: 24.3634 - val_kl_reshape_loss: 0.6246\n",
      "Epoch 65/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.0045 - decoder_loss: 20.5075 - kl_reshape_loss: 1.4970 - val_loss: 24.4423 - val_decoder_loss: 23.7411 - val_kl_reshape_loss: 0.7012\n",
      "Epoch 66/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.0907 - decoder_loss: 20.5184 - kl_reshape_loss: 1.5722 - val_loss: 24.2796 - val_decoder_loss: 23.5351 - val_kl_reshape_loss: 0.7445\n",
      "Epoch 67/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.9427 - decoder_loss: 20.3406 - kl_reshape_loss: 1.6021 - val_loss: 24.7172 - val_decoder_loss: 23.9286 - val_kl_reshape_loss: 0.7885\n",
      "Epoch 68/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 22.0009 - decoder_loss: 20.2819 - kl_reshape_loss: 1.7190 - val_loss: 24.7426 - val_decoder_loss: 23.8870 - val_kl_reshape_loss: 0.8556\n",
      "Epoch 69/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.8225 - decoder_loss: 20.0821 - kl_reshape_loss: 1.7404 - val_loss: 24.5855 - val_decoder_loss: 23.7497 - val_kl_reshape_loss: 0.8358\n",
      "Epoch 70/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.7906 - decoder_loss: 19.9977 - kl_reshape_loss: 1.7929 - val_loss: 25.0009 - val_decoder_loss: 24.0980 - val_kl_reshape_loss: 0.9029\n",
      "Epoch 71/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.7430 - decoder_loss: 19.8493 - kl_reshape_loss: 1.8937 - val_loss: 24.8460 - val_decoder_loss: 23.9731 - val_kl_reshape_loss: 0.8730\n",
      "Epoch 72/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.6592 - decoder_loss: 19.7491 - kl_reshape_loss: 1.9100 - val_loss: 25.2179 - val_decoder_loss: 24.2545 - val_kl_reshape_loss: 0.9633\n",
      "Epoch 73/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.6932 - decoder_loss: 19.6926 - kl_reshape_loss: 2.0006 - val_loss: 25.2269 - val_decoder_loss: 24.2731 - val_kl_reshape_loss: 0.9538\n",
      "Epoch 74/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.7107 - decoder_loss: 19.6292 - kl_reshape_loss: 2.0815 - val_loss: 25.1018 - val_decoder_loss: 24.2001 - val_kl_reshape_loss: 0.9018\n",
      "Epoch 75/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.7146 - decoder_loss: 19.5663 - kl_reshape_loss: 2.1483 - val_loss: 25.0281 - val_decoder_loss: 24.0188 - val_kl_reshape_loss: 1.0093\n",
      "Epoch 76/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.5048 - decoder_loss: 19.3316 - kl_reshape_loss: 2.1732 - val_loss: 25.1124 - val_decoder_loss: 24.0928 - val_kl_reshape_loss: 1.0196\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 2s 2ms/step - loss: 21.5753 - decoder_loss: 19.2994 - kl_reshape_loss: 2.2759 - val_loss: 25.0617 - val_decoder_loss: 24.0210 - val_kl_reshape_loss: 1.0406\n",
      "Epoch 78/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.3775 - decoder_loss: 19.1346 - kl_reshape_loss: 2.2429 - val_loss: 25.0865 - val_decoder_loss: 23.7348 - val_kl_reshape_loss: 1.3517\n",
      "Epoch 79/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.4053 - decoder_loss: 19.0249 - kl_reshape_loss: 2.3804 - val_loss: 25.0180 - val_decoder_loss: 23.7940 - val_kl_reshape_loss: 1.2241\n",
      "Epoch 80/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.2319 - decoder_loss: 18.8626 - kl_reshape_loss: 2.3693 - val_loss: 25.2722 - val_decoder_loss: 24.1776 - val_kl_reshape_loss: 1.0946\n",
      "Epoch 81/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.3725 - decoder_loss: 18.8553 - kl_reshape_loss: 2.5172 - val_loss: 25.2916 - val_decoder_loss: 23.9456 - val_kl_reshape_loss: 1.3460\n",
      "Epoch 82/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.0910 - decoder_loss: 18.5490 - kl_reshape_loss: 2.5419 - val_loss: 25.4951 - val_decoder_loss: 24.3619 - val_kl_reshape_loss: 1.1331\n",
      "Epoch 83/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.3785 - decoder_loss: 18.7276 - kl_reshape_loss: 2.6509 - val_loss: 25.6334 - val_decoder_loss: 24.1144 - val_kl_reshape_loss: 1.5190\n",
      "Epoch 84/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.1955 - decoder_loss: 18.4882 - kl_reshape_loss: 2.7073 - val_loss: 25.5422 - val_decoder_loss: 24.0836 - val_kl_reshape_loss: 1.4585\n",
      "Epoch 85/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.0941 - decoder_loss: 18.3236 - kl_reshape_loss: 2.7706 - val_loss: 25.6847 - val_decoder_loss: 24.2878 - val_kl_reshape_loss: 1.3969\n",
      "Epoch 86/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 21.0044 - decoder_loss: 18.1400 - kl_reshape_loss: 2.8644 - val_loss: 25.5014 - val_decoder_loss: 23.9899 - val_kl_reshape_loss: 1.5114\n",
      "Epoch 87/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.8271 - decoder_loss: 17.9601 - kl_reshape_loss: 2.8670 - val_loss: 25.7075 - val_decoder_loss: 24.2888 - val_kl_reshape_loss: 1.4187\n",
      "Epoch 88/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.7974 - decoder_loss: 17.9145 - kl_reshape_loss: 2.8829 - val_loss: 25.8755 - val_decoder_loss: 24.2049 - val_kl_reshape_loss: 1.6706\n",
      "Epoch 89/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.6923 - decoder_loss: 17.7009 - kl_reshape_loss: 2.9914 - val_loss: 25.6249 - val_decoder_loss: 24.1412 - val_kl_reshape_loss: 1.4838\n",
      "Epoch 90/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.7410 - decoder_loss: 17.6861 - kl_reshape_loss: 3.0549 - val_loss: 25.9848 - val_decoder_loss: 24.3838 - val_kl_reshape_loss: 1.6010\n",
      "Epoch 91/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.6345 - decoder_loss: 17.5407 - kl_reshape_loss: 3.0938 - val_loss: 25.7462 - val_decoder_loss: 24.2031 - val_kl_reshape_loss: 1.5430\n",
      "Epoch 92/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.6837 - decoder_loss: 17.4466 - kl_reshape_loss: 3.2371 - val_loss: 25.6627 - val_decoder_loss: 23.8476 - val_kl_reshape_loss: 1.8151\n",
      "Epoch 93/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.4758 - decoder_loss: 17.2815 - kl_reshape_loss: 3.1942 - val_loss: 26.0331 - val_decoder_loss: 24.3508 - val_kl_reshape_loss: 1.6823\n",
      "Epoch 94/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.6764 - decoder_loss: 17.3088 - kl_reshape_loss: 3.3675 - val_loss: 25.7623 - val_decoder_loss: 23.9733 - val_kl_reshape_loss: 1.7890\n",
      "Epoch 95/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.5493 - decoder_loss: 17.0961 - kl_reshape_loss: 3.4532 - val_loss: 26.2905 - val_decoder_loss: 24.2062 - val_kl_reshape_loss: 2.0844\n",
      "Epoch 96/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.7032 - decoder_loss: 17.1357 - kl_reshape_loss: 3.5675 - val_loss: 26.0052 - val_decoder_loss: 24.0381 - val_kl_reshape_loss: 1.9671\n",
      "Epoch 97/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.4916 - decoder_loss: 16.9730 - kl_reshape_loss: 3.5186 - val_loss: 26.3046 - val_decoder_loss: 24.6238 - val_kl_reshape_loss: 1.6808\n",
      "Epoch 98/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.2942 - decoder_loss: 16.7632 - kl_reshape_loss: 3.5310 - val_loss: 26.1724 - val_decoder_loss: 23.9365 - val_kl_reshape_loss: 2.2359\n",
      "Epoch 99/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.4111 - decoder_loss: 16.7463 - kl_reshape_loss: 3.6648 - val_loss: 26.5880 - val_decoder_loss: 24.8060 - val_kl_reshape_loss: 1.7820\n",
      "Epoch 100/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 20.2678 - decoder_loss: 16.6375 - kl_reshape_loss: 3.6302 - val_loss: 26.5218 - val_decoder_loss: 24.6818 - val_kl_reshape_loss: 1.8400\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import History \n",
    "history = History()\n",
    "epochs = 100\n",
    "vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=21, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "vae_0.fit([X_train], [X_train, np.zeros(X_train.shape[0])],\n",
    "          epochs=epochs,\n",
    "          batch_size=10,\n",
    "          verbose=True, callbacks=[history], \n",
    "         validation_split = 0.1)\n",
    "vae_0.encoder_.save_weights(\"models/vae_0_encoder_weights_{0}.h5\".format(epochs))\n",
    "vae_0.decoder_.save_weights(\"models/vae_0_decoder_weights_{0}.h5\".format(epochs))\n",
    "vae_0.vae_.save_weights(\"models/vae_0_vae_weights_{0}.h5\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "for loss in history.history.keys():\n",
    "    plt.plot(history.history[loss], label=loss)\n",
    "plt.legend()\n",
    "plt.title(\"training loss history\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(\"./logs/base_vae_{0}_epochs_training_history.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_1 = build_vae(latent_dim=20,\n",
    "                  n_tokens=21, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "vae_1.encoder_.load_weights(\"models/vae_0_encoder_weights_{0}.h5\".format(epochs))\n",
    "vae_1.decoder_.load_weights(\"models/vae_0_decoder_weights_{0}.h5\".format(epochs))\n",
    "vae_1.vae_.load_weights(\"models/vae_0_vae_weights_{0}.h5\".format(epochs))\n",
    "\n",
    "output_mean_0, output_var_0 = vae_0.encoder_.predict([X_train])\n",
    "output_mean_1, output_var_1 = vae_1.encoder_.predict([X_train])\n",
    "np.testing.assert_array_equal(output_mean_0, output_mean_1)\n",
    "np.testing.assert_array_equal(output_var_0, output_var_1)\n",
    "z = np.random.sample((1000, 20))\n",
    "recon_0 = vae_0.decoder_.predict(z)\n",
    "recon_1 = vae_1.decoder_.predict(z)\n",
    "np.testing.assert_array_equal(recon_0, recon_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/gfp_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ec71c3fe18d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_characters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex_to_character\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_characters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwild_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wild_type_amino_acid_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/generative-model-experiments/benchmarks/cbas/util.py\u001b[0m in \u001b[0;36mget_wild_type_amino_acid_sequence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wild_type_amino_acid_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdna_to_amino_acid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_wild_type_dna_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_substring_mismatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/generative-model-experiments/benchmarks/cbas/util.py\u001b[0m in \u001b[0;36mget_wild_type_dna_sequence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wild_type_dna_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/gfp_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nucSequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdna_to_amino_acid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdna_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data/gfp_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "vocabulary = get_all_amino_acids()\n",
    "num_characters = len(vocabulary)\n",
    "index_to_character = dict(zip(range(num_characters), vocabulary))\n",
    "wild_type = get_wild_type_amino_acid_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "z = np.random.sample((num_samples, 20))\n",
    "outputs = vae_1.decoder_.predict(z)\n",
    "mismatches, all_strings = [], []\n",
    "for i in range(outputs.shape[0]):\n",
    "    string = []\n",
    "    for j in range(outputs.shape[1]):\n",
    "        k = np.random.choice(list(range(num_characters)), p = outputs[i, j])\n",
    "        string.append(index_to_character[k])\n",
    "    all_strings.append(\"\".join(string))\n",
    "    mismatches.append(count_substring_mismatch(wild_type, all_strings[-1]))\n",
    "    if i % 100 == 0:\n",
    "        print(\"finished {0}/{1} samples\".format(i, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "plt.title(\"number of mismatches\")\n",
    "plt.hist(np.array(mismatches), bins=15)\n",
    "plt.xlabel(\"mismatches\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.savefig(\"./logs/base_vae_mismatch_from_wild_type_epoch_{0}\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mismatches = []\n",
    "for amino_acid_seq in load_gfp_data(\"./data/gfp_amino_acid_shuffle_\")[0]:\n",
    "    data_mismatches.append(count_substring_mismatch(amino_acid_seq, wild_type))\n",
    "plt.title(\"number of mismatches\")\n",
    "plt.hist(data_mismatches, bins = 15)\n",
    "plt.xlabel(\"mismatches\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.savefig(\"./logs/X_train_wild_type_mismatches.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experimental_weighted_ml(it, repeats=3):\n",
    "    \n",
    "    assert it in [0, 1, 2]\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    \n",
    "    test_kwargs = [\n",
    "#                    {'weights_type':'cbas', 'quantile': 1},\n",
    "#                    {'weights_type':'rwr', 'alpha': 20},\n",
    "#                    {'weights_type':'dbas', 'quantile': 0.95},\n",
    "#                    {'weights_type':'cem-pi', 'quantile': 0.8},\n",
    "        {'weights_type': 'fbvae', 'quantile': 0.8}\n",
    "    ]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters':1000\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for k in range(repeats):\n",
    "        for j in range(len(test_kwargs)):\n",
    "            test_name = test_kwargs[j]['weights_type']\n",
    "            suffix = \"_%s_%i_%i\" % (train_size_str, RANDOM_STATE, k)\n",
    "            if test_name == 'fbvae':\n",
    "#                 suffix = suffix + \"_%.2f\" % test_kwargs[j]['quantile']\n",
    "                if base_kwargs['iters'] == 1000:\n",
    "                    suffix += '_long'\n",
    "            \n",
    "                print(suffix)\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                [kwargs.pop(k) for k in ['homoscedastic', 'homo_y_var', 'cutoff', 'it_epochs']]\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_max = fb_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "            else:\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "                \n",
    "            np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "            np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "            np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "\n",
    "            with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "                json.dump(test_max, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbas_q(qs = [0.5, 0.75, 0.95, 1]):\n",
    "    it = 0\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    \n",
    "    test_kwargs = [ {'weights_type':'cbas', 'quantile': q} for q in qs]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters':100\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for j in range(len(test_kwargs)):\n",
    "        test_name = test_kwargs[j]['weights_type']\n",
    "        qj = test_kwargs[j]['quantile']\n",
    "        suffix = \"_qtest_%s_%i_%.2f\" % (train_size_str, RANDOM_STATE, qj)\n",
    "        print(suffix)\n",
    "        kwargs = {}\n",
    "        kwargs.update(test_kwargs[j])\n",
    "        kwargs.update(base_kwargs)\n",
    "        test_traj, test_oracle_samples, test_gt_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "        \n",
    "        np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "        np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "        np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "\n",
    "        with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "            json.dump(test_max, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'count_substring_mismatch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6fd43bd34799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcount_substring_mismatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'count_substring_mismatch'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from util import count_substring_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_experimental_vaes()\n",
    "# train_experimental_oracles()\n",
    "\n",
    "# for i in range(1):\n",
    "#     run_experimental_weighted_ml(i, repeats=1)\n",
    "\n",
    "test_cbas_q(qs=[0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_density():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    RANDOM_STATE = it + 1\n",
    "\n",
    "    X_train, y_train, gt_train, X_test, y_test, gt_test  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE, return_test=True)\n",
    "\n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "\n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "              n_tokens=20, \n",
    "              seq_length=X_train.shape[1],\n",
    "              enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    X_shuffle = np.zeros_like(X_train)\n",
    "    for i in range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
