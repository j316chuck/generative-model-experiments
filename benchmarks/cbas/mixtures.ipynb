{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from random import shuffle\n",
    "from tensorflow_probability import distributions as tfd\n",
    "# tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalMixture(object):\n",
    "    \n",
    "    def __init__(self, K, L, m):\n",
    "        self.K = K  # number of components\n",
    "        self.L = L  # sequence length\n",
    "        self.m = m  # number of categories\n",
    "        \n",
    "        self.z_logits = tf.Variable(tf.random.uniform((K,)), dtype=tf.float32)\n",
    "        self.p_logits = [tf.Variable(tf.random.uniform((L, m,)), dtype=tf.float32) for _ in range(K)]\n",
    "        self.z = tf.nn.softmax(self.z_logits)\n",
    "        self.ps = [tf.nn.softmax(self.p_logits[i]) for i in range(K)]\n",
    "        self.components = [tfd.Independent(tfd.Categorical(probs=self.ps[i]), reinterpreted_batch_ndims=1) for i in range(K)]\n",
    "        self.model = tfd.Mixture(\n",
    "            cat = tfd.Categorical(probs=self.z),\n",
    "            components=self.components           \n",
    "        )\n",
    "        \n",
    "        self.X_train = tf.placeholder(name=\"X_train\",shape=[None, L], dtype=tf.float32)\n",
    "        self.weights = tf.placeholder(name=\"loss_weights\", shape=[None], dtype=tf.float32)\n",
    "        self.loss = -tf.reduce_mean(self.weights * self.model.log_prob(self.X_train))\n",
    "        self.train_op = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _iterate_minibatches(self, inputs1, inputs2, batch_size, shuffle=True):\n",
    "        if shuffle:\n",
    "            indices = np.arange(inputs1.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, inputs1.shape[0] - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs1[excerpt], inputs2[excerpt]\n",
    "        \n",
    "    def train(self, X, W, epochs=100, batch_size=10, shuffle=True, verbose=False, one_hot=False, print_every=100):\n",
    "        if one_hot:\n",
    "            X = np.argmax(X, axis=-1)\n",
    "        for t in range(epochs):\n",
    "            e_loss = 0\n",
    "            n_batches = 0\n",
    "            for batch in self._iterate_minibatches(X, W, batch_size, shuffle=shuffle):\n",
    "                xi, wi = batch\n",
    "                _, np_loss = self.sess.run([self.train_op, self.loss], feed_dict={self.X_train: xi, self.weights:wi})\n",
    "                e_loss += np_loss\n",
    "                n_batches += 1\n",
    "            if verbose:\n",
    "                if t % print_every == 0 or t == epochs-1:\n",
    "                    print(\"Training loss at %i/%i: %.3f\" % (t, epochs, e_loss/n_batches))\n",
    "                \n",
    "    def sample(self, n, one_hot=False):\n",
    "        samples = self.model.sample(n).eval(session=self.sess)\n",
    "        if one_hot:\n",
    "            samples_one_hot = np.zeros((n, self.L, self.m))\n",
    "            samples_one_hot[np.arange(n).reshape(n, 1), np.arange(self.L_), samples] = 1\n",
    "            samples = samples_one_hot\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sequences(length, arr=False):\n",
    "    N = 4**length\n",
    "    if arr:\n",
    "        base = [np.array([[1, 0, 0, 0]]), np.array([[0, 1, 0, 0]]), \n",
    "                np.array([[0, 0, 1, 0]]), np.array([[0, 0, 0, 1]])]\n",
    "    else:\n",
    "        base = 'ATCG'\n",
    "    seq_lists = list(itertools.product(base, repeat=length))\n",
    "    if arr:\n",
    "        all_seq = np.zeros((N, length, 4))\n",
    "    else:\n",
    "        all_seq = [\"A\" * length] * N\n",
    "    \n",
    "    for i in range(N):\n",
    "        if i % int(10**6) == 0 and i > 0:\n",
    "            print(\"Sequences constructed: %i / %i\" % (i, N))\n",
    "#         elif idx:\n",
    "#             all_seq[i] = np.concqtenate(np.argmax(seq_lists[i], axis=-1), axis=0)\n",
    "        if arr:\n",
    "            all_seq[i] = np.concatenate(seq_lists[i], axis=0)\n",
    "        else:\n",
    "            all_seq[i] = \"\".join(seq_lists[i])\n",
    "    return all_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences constructed: 1000000 / 1048576\n"
     ]
    }
   ],
   "source": [
    "X_all = get_all_sequences(10, arr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = CategoricalMixture(4, 10, 4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at 0/500: 7.271\n",
      "Training loss at 100/500: 4.344\n",
      "Training loss at 200/500: 3.252\n",
      "Training loss at 300/500: 2.890\n",
      "Training loss at 400/500: 2.735\n",
      "Training loss at 499/500: 2.657\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 10000\n",
    "N = 100\n",
    "\n",
    "xt = X_all[200:200+int(N/4)]\n",
    "xt = np.concatenate([xt, X_all[100000:100000+int(N/4)]])\n",
    "xt = np.concatenate([xt, X_all[500000:500000+int(N/4)]])\n",
    "xt = np.concatenate([xt, X_all[1000000:1000000+int(N/4)]])\n",
    "w = np.random.rand(N)\n",
    "\n",
    "mix.train(xt, w, epochs=500, batch_size=10, shuffle=True, one_hot=True, verbose=True, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.72657  , -14.576713 ,  -9.229977 ,  -4.6774077, -10.106009 ,\n",
       "        -4.4184985,  -5.7726083,  -4.44662  ,  -4.7952538,  -4.74765  ,\n",
       "        -5.192702 ,  -5.8997326,  -4.2965436,  -5.4116755,  -5.487743 ,\n",
       "        -5.1537056,  -5.2462425,  -5.080471 ,  -4.67406  ,  -5.569378 ,\n",
       "        -4.9103756,  -4.408775 ,  -4.4054265,  -5.274855 ,  -5.2894278,\n",
       "        -5.034159 ,  -4.6996064,  -8.545227 ,  -4.44662  ,  -4.8215613,\n",
       "        -4.72657  ,  -6.099427 ,  -4.673725 ,  -5.2894278, -10.921806 ,\n",
       "        -5.3745747,  -5.1537056,  -4.4054265, -10.177364 ,  -4.989058 ,\n",
       "       -13.264703 ,  -4.8118377,  -4.5983047,  -4.537695 ,  -4.8678236,\n",
       "        -4.8678236,  -5.511746 ,  -5.125141 ,  -5.6109385,  -5.872328 ,\n",
       "        -4.408775 ,  -4.74765  ,  -9.093495 ,  -4.408775 ,  -5.3752794,\n",
       "        -9.996938 ,  -5.1674185,  -5.040514 ,  -9.483656 , -10.359264 ,\n",
       "       -10.35176  , -14.3752985,  -4.789639 ,  -4.41515  ,  -5.030811 ,\n",
       "       -10.279336 ,  -4.6696653, -14.439966 ,  -4.649926 ,  -9.711937 ,\n",
       "        -4.786291 ,  -4.441694 ,  -5.818086 , -10.2179365,  -4.6996064,\n",
       "        -5.080471 ,  -5.274855 ,  -4.4985847,  -4.408775 ,  -4.730488 ,\n",
       "        -4.5737634,  -5.030811 ,  -4.76019  ,  -7.2246566,  -5.2573786,\n",
       "        -4.7993627,  -4.6487703,  -5.614574 ,  -5.084048 ,  -4.833316 ,\n",
       "        -9.2986555,  -4.989058 ,  -9.200348 ,  -4.8678236,  -5.5461726,\n",
       "        -6.025502 ,  -5.2462425,  -4.9185596,  -5.511746 ,  -4.379959 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = mix.sample(100, one_hot=False)\n",
    "mix.model.log_prob(xs).eval(session=mix.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyConda2",
   "language": "python",
   "name": "myconda2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
