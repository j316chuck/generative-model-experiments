{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_characters = 21\n",
    "seq_length = 238\n",
    "original_dim = num_characers * seq_length\n",
    "latent_dim = 20\n",
    "intermediate_dim = 50\n",
    "epochs = 100\n",
    "epsilon_std = 1.0\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='elu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='elu')\n",
    "decoder_mean = Dense(original_dim, activation='elu')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded = decoder_mean(h_decoded)\n",
    "x_reshape = Reshape((seq_length, num_characters))(x_decoded)\n",
    "x_decoded_mean = Dense(num_characters, activation='softmax')(x_reshape)\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "# Custom loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = K.sum(K.categorical_crossentropy(x, x_decoded_mean), axis = -1)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.sum(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "vae.compile(optimizer='rmsprop', loss=[zero_loss])\n",
    "\n",
    "#checkpoint\n",
    "cp = [callbacks.ModelCheckpoint(filepath=\"/home/ubuntu/pynb/model.h5\", verbose=1, save_best_only=True)]\n",
    "\n",
    "#train\n",
    "vae.fit(train, train.reshape(-1, seq_length, num_characters),\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test, test), callbacks=cp)\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda, Input, Dense, Reshape\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy, categorical_crossentropy\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    print(z_mean)\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 4998)\n"
     ]
    }
   ],
   "source": [
    "from util import load_gfp_data, count_substring_mismatch, get_all_amino_acids, get_wild_type_amino_acid_sequence\n",
    "from util import one_hot_encode\n",
    "train_size = 1100\n",
    "num_amino_acids = 21\n",
    "seq_length = 238\n",
    "input_length = num_amino_acids * seq_length\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"../../data/gfp_amino_acid_\")\n",
    "X_train = one_hot_encode(X_train[0:train_size], get_all_amino_acids())\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"z_mean/BiasAdd:0\", shape=(?, 20), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 4998)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           249950      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 20)           1020        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 20)           1020        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 20)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,990\n",
      "Trainable params: 251,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4998)              254898    \n",
      "_________________________________________________________________\n",
      "d4 (Reshape)                 (None, 238, 21)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 238, 21)           462       \n",
      "=================================================================\n",
      "Total params: 256,410\n",
      "Trainable params: 256,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Tensor(\"encoder/z_mean/BiasAdd:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# network parameters\n",
    "input_shape = (input_length, )\n",
    "intermediate_dim = 50\n",
    "batch_size = 10\n",
    "latent_dim = 20\n",
    "epochs = 100\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "expected = Reshape((seq_length, num_amino_acids))(inputs)\n",
    "x = Dense(intermediate_dim, activation='elu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='./logs/vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='elu')(latent_inputs)\n",
    "x = Dense(input_length)(x)        \n",
    "x = Reshape((seq_length, num_amino_acids), name='d4')(x)\n",
    "outputs = Dense(num_amino_acids, activation='softmax')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "print(decoder.summary())\n",
    "plot_model(decoder, to_file='./logs/vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 4998)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 20), (None, 20),  251990    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 238, 21)           256410    \n",
      "=================================================================\n",
      "Total params: 508,400\n",
      "Trainable params: 508,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 990 samples, validate on 110 samples\n",
      "Epoch 1/100\n",
      "990/990 [==============================] - 4s 4ms/step - loss: 359.7659 - val_loss: 987.6673\n",
      "Epoch 2/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 343.8110 - val_loss: 971.0567\n",
      "Epoch 3/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 339.0501 - val_loss: 954.6153\n",
      "Epoch 4/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 335.4052 - val_loss: 940.7617\n",
      "Epoch 5/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 327.2361 - val_loss: 949.0007\n",
      "Epoch 6/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 325.2028 - val_loss: 939.1269\n",
      "Epoch 7/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 323.4513 - val_loss: 959.8654\n",
      "Epoch 8/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 318.5606 - val_loss: 964.6063\n",
      "Epoch 9/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 315.6541 - val_loss: 957.9196\n",
      "Epoch 10/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 317.7223 - val_loss: 952.0170\n",
      "Epoch 11/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 316.7249 - val_loss: 971.1829\n",
      "Epoch 12/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 318.5440 - val_loss: 956.7277\n",
      "Epoch 13/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 311.5241 - val_loss: 970.6535\n",
      "Epoch 14/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 313.4178 - val_loss: 963.7875\n",
      "Epoch 15/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 312.0446 - val_loss: 985.6030\n",
      "Epoch 16/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 309.5446 - val_loss: 978.9502\n",
      "Epoch 17/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 304.9904 - val_loss: 965.9409\n",
      "Epoch 18/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 306.6860 - val_loss: 960.9836\n",
      "Epoch 19/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 306.0378 - val_loss: 977.8765\n",
      "Epoch 20/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 304.7533 - val_loss: 968.3093\n",
      "Epoch 21/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 306.3960 - val_loss: 979.4479\n",
      "Epoch 22/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 301.3069 - val_loss: 984.6360\n",
      "Epoch 23/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 302.0490 - val_loss: 996.1579\n",
      "Epoch 24/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 302.2461 - val_loss: 995.4714\n",
      "Epoch 25/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 301.7469 - val_loss: 1006.8784\n",
      "Epoch 26/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 301.1880 - val_loss: 989.3105\n",
      "Epoch 27/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 299.7701 - val_loss: 998.6713\n",
      "Epoch 28/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 295.0261 - val_loss: 995.3083\n",
      "Epoch 29/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 298.8603 - val_loss: 1022.4424\n",
      "Epoch 30/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 297.1902 - val_loss: 1017.3761\n",
      "Epoch 31/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 298.2130 - val_loss: 1018.5970\n",
      "Epoch 32/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 293.9245 - val_loss: 1009.0263\n",
      "Epoch 33/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 294.1718 - val_loss: 1028.5182\n",
      "Epoch 34/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 293.5460 - val_loss: 1024.9554\n",
      "Epoch 35/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 290.8772 - val_loss: 1029.5150\n",
      "Epoch 36/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 289.2852 - val_loss: 1069.7906\n",
      "Epoch 37/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 291.1171 - val_loss: 1010.1458\n",
      "Epoch 38/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 290.0290 - val_loss: 1026.8369\n",
      "Epoch 39/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 287.3018 - val_loss: 1024.6771\n",
      "Epoch 40/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 283.3545 - val_loss: 1020.8052\n",
      "Epoch 41/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 284.9253 - val_loss: 1053.9605\n",
      "Epoch 42/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 283.7109 - val_loss: 1058.6709\n",
      "Epoch 43/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 278.5611 - val_loss: 1056.9221\n",
      "Epoch 44/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 284.9644 - val_loss: 1058.9080\n",
      "Epoch 45/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 280.4786 - val_loss: 1065.2972\n",
      "Epoch 46/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 280.4627 - val_loss: 1106.5868\n",
      "Epoch 47/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 283.8304 - val_loss: 1110.9049\n",
      "Epoch 48/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 280.6048 - val_loss: 1074.5803\n",
      "Epoch 49/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 279.4293 - val_loss: 1064.2515\n",
      "Epoch 50/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 276.0350 - val_loss: 1088.3588\n",
      "Epoch 51/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 280.6251 - val_loss: 1079.5726\n",
      "Epoch 52/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 274.8339 - val_loss: 1098.5502\n",
      "Epoch 53/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 274.0998 - val_loss: 1078.8804\n",
      "Epoch 54/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 274.3247 - val_loss: 1112.6031\n",
      "Epoch 55/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 275.7750 - val_loss: 1098.3872\n",
      "Epoch 56/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 271.0718 - val_loss: 1104.5874\n",
      "Epoch 57/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 275.0979 - val_loss: 1132.7089\n",
      "Epoch 58/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 272.4638 - val_loss: 1092.2786\n",
      "Epoch 59/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 268.9216 - val_loss: 1133.9915\n",
      "Epoch 60/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 271.5898 - val_loss: 1133.8222\n",
      "Epoch 61/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 270.6280 - val_loss: 1121.4530\n",
      "Epoch 62/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 269.8378 - val_loss: 1134.6028\n",
      "Epoch 63/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 272.1225 - val_loss: 1119.3378\n",
      "Epoch 64/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 266.2621 - val_loss: 1133.0417\n",
      "Epoch 65/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 271.3460 - val_loss: 1112.4852\n",
      "Epoch 66/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 266.1106 - val_loss: 1125.3571\n",
      "Epoch 67/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 264.4283 - val_loss: 1120.2741\n",
      "Epoch 68/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 267.3980 - val_loss: 1147.2365\n",
      "Epoch 69/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 262.0069 - val_loss: 1155.4384\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 2s 2ms/step - loss: 262.2282 - val_loss: 1137.5557\n",
      "Epoch 71/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 261.3621 - val_loss: 1130.2733\n",
      "Epoch 72/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 262.8988 - val_loss: 1166.3161\n",
      "Epoch 73/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 260.1511 - val_loss: 1129.6734\n",
      "Epoch 74/100\n",
      "990/990 [==============================] - ETA: 0s - loss: 261.300 - 2s 2ms/step - loss: 262.2736 - val_loss: 1134.3404\n",
      "Epoch 75/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 261.2003 - val_loss: 1152.3390\n",
      "Epoch 76/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 264.7140 - val_loss: 1156.6776\n",
      "Epoch 77/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 260.4358 - val_loss: 1171.0082\n",
      "Epoch 78/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 257.8718 - val_loss: 1212.9316\n",
      "Epoch 79/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 259.2489 - val_loss: 1179.5890\n",
      "Epoch 80/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 258.9025 - val_loss: 1151.9378\n",
      "Epoch 81/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 257.2123 - val_loss: 1184.0409\n",
      "Epoch 82/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 255.6091 - val_loss: 1165.5128\n",
      "Epoch 83/100\n",
      "990/990 [==============================] - 2s 3ms/step - loss: 255.6447 - val_loss: 1194.8251\n",
      "Epoch 84/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 256.4710 - val_loss: 1211.9977\n",
      "Epoch 85/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 254.1315 - val_loss: 1189.8472\n",
      "Epoch 86/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 256.5919 - val_loss: 1204.6022\n",
      "Epoch 87/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 251.5507 - val_loss: 1211.4190\n",
      "Epoch 88/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 254.5743 - val_loss: 1179.8816\n",
      "Epoch 89/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 248.0535 - val_loss: 1205.0491\n",
      "Epoch 90/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 251.4856 - val_loss: 1195.6192\n",
      "Epoch 91/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 248.6068 - val_loss: 1193.3708\n",
      "Epoch 92/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 252.8296 - val_loss: 1189.7062\n",
      "Epoch 93/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 250.2362 - val_loss: 1224.1211\n",
      "Epoch 94/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 254.5218 - val_loss: 1214.2243\n",
      "Epoch 95/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 247.4741 - val_loss: 1198.9073\n",
      "Epoch 96/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 250.8203 - val_loss: 1199.2488\n",
      "Epoch 97/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 248.2485 - val_loss: 1244.1811\n",
      "Epoch 98/100\n",
      "990/990 [==============================] - 2s 2ms/step - loss: 243.3556 - val_loss: 1225.4630s - \n",
      "Epoch 99/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 247.6612 - val_loss: 1249.4054\n",
      "Epoch 100/100\n",
      "990/990 [==============================] - 3s 3ms/step - loss: 250.3281 - val_loss: 1246.3125\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss = K.sum(K.categorical_crossentropy(expected, outputs), axis=-1)\n",
    "kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "print(vae.summary())\n",
    "plot_model(vae,\n",
    "           to_file='vae_mlp.png',\n",
    "           show_shapes=True)\n",
    "load_weights=False\n",
    "if load_weights:\n",
    "    vae.load_weights(args.weights)\n",
    "else:\n",
    "    # train the autoencoder\n",
    "    vae.fit(X_train, \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split = 0.1)\n",
    "    vae.save_weights('./models/vae_mlp_mnist.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
