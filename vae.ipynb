{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchviz import make_dot\n",
    "from utils import one_hot_encode, one_hot_decode, get_all_amino_acids, get_wild_type_amino_acid_sequence\n",
    "from utils import load_gfp_data, count_substring_mismatch, get_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    # change architecture later to make it deeper if it's not good enough to capture all data\n",
    "    def __init__(self, input_size, hidden_size, latent_dim, num_characters):\n",
    "        super(VAE, self).__init__() \n",
    "        self.num_characters = num_characters\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc21 = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, input_size)\n",
    "        self.fc5 = nn.Linear(self.num_characters, self.num_characters)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # input should be one hot encoded. shape - (batch_size, alphabet x sequence_length)\n",
    "        h1 = F.elu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z, softmax=False):\n",
    "        batch_size = z.shape[0]\n",
    "        h3 = F.elu(self.fc3(z))\n",
    "        h4 = F.elu(self.fc4(h3)).view(batch_size, -1, self.num_characters) ## may need to add RELU here. \n",
    "        #F.elu(self.fc4(h3)).view(batch_size, -1, self.num_characters)\n",
    "        h5 = self.fc5(h4)\n",
    "        if softmax:\n",
    "            return F.softmax(h5, dim=2)\n",
    "        else:\n",
    "            return h5\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_size))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, softmax=False), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeVAE(): \n",
    "    \n",
    "    def __init__(self, args):     \n",
    "        \"\"\"\n",
    "        Initializes the VAE to be a generative VAE\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the neural network\n",
    "        args.name : string \n",
    "            defines the name of the neural network\n",
    "        args.description: string\n",
    "            describes the architecture of the neural network\n",
    "        args.input : int\n",
    "            the size of the input\n",
    "        args.hidden_size : int\n",
    "            the size of the hidden layer\n",
    "        args.latent_dim: int \n",
    "            the size of the latent dimension\n",
    "        args.device : device\n",
    "            the device used: cpu or gpu\n",
    "        args.learning_rate : float\n",
    "            sets the learning rate\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        args.beta : float\n",
    "            sets the beta parameter for the KL divergence loss\n",
    "        args.vocabulary : string\n",
    "            all the characters in the context of the problem\n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.description = args[\"description\"]\n",
    "        self.input = args[\"input\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.latent_dim = args[\"latent_dim\"]\n",
    "        self.device = args[\"device\"]\n",
    "        self.learning_rate = args[\"learning_rate\"]\n",
    "        self.epochs = args[\"epochs\"]\n",
    "        self.beta = args[\"beta\"]\n",
    "        self.all_characters = args[\"vocabulary\"]\n",
    "        self.num_characters = len(self.all_characters)\n",
    "        self.character_to_int = dict(zip(self.all_characters, range(self.num_characters)))\n",
    "        self.int_to_character = dict(zip(range(self.num_characters), self.all_characters))\n",
    "        self.model = VAE(self.input, self.hidden_size, self.latent_dim, self.num_characters)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.train_loss_history = []\n",
    "        self.test_loss_history = []\n",
    "        \n",
    "    # Reconstruction + KL divergence losses summed over all elements in batch\n",
    "    def elbo_loss(self, recon_x, x, mu, logvar):\n",
    "        \"\"\"\n",
    "        Input: x is the one hot encoded batch_size x (seq_length * len(all_characters)) \n",
    "               recon_x is the unormalized outputs of the decoder in the same shape as x\n",
    "               mu and logvar are the hidden states of size self.hidden_size\n",
    "        Output: elbo_loss\n",
    "        \"\"\"\n",
    "        # get the argmax of each batch_size x seq_length * len(all_characters) matrix. Output is in batch_size x seq_length form\n",
    "        # print(labels)\n",
    "        # reshapes the recon_x vector to be of shape batch_size x len(all_characters) x seq_length so that it fits according to PyTorch's CrossEntropyLoss\n",
    "        # permute is transpose function so at each 1, 2 dimension we take the transpose\n",
    "        # print(recon_x.shape)\n",
    "        # print(reshape_x[0,:,0])\n",
    "        outputs = F.log_softmax(recon_x, dim = 2)\n",
    "        CE = (-1 * outputs * x.view(x.shape[0], -1, len(self.all_characters))).sum()\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        #print(\"log var shape:\", logvar.shape, \"mu shape: \", mu.shape, \"logvar: \", logvar.sum(dim=1))\n",
    "        #print(\"mu: \", mu.sum(dim=1))\n",
    "        #print((1 + logvar - mu.pow(2) - logvar.exp()).shape)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        #print(\"CE Loss: \", CE, \"KLD Loss:\", KLD, file=logger)\n",
    "        return CE + KLD\n",
    "    \n",
    "    def NLLoss(self, recon_x, x): \n",
    "        outputs = F.log_softmax(recon_x, dim = 2)\n",
    "        return (-1 * outputs * x.view(x.shape[0], -1, len(self.all_characters))).sum()\n",
    "    \n",
    "    def KLD(self, mu, logvar): \n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    def fit(self, train_dataloader, test_dataloader=None, verbose=True, logger=None, save_model=True):\n",
    "        # amino acid dataset specific checks\n",
    "        wild_type = get_wild_type_amino_acid_sequence()\n",
    "        three_mutation = get_mutation(wild_type, num_mutations=3, alphabet=self.all_characters)\n",
    "        ten_mutation = get_mutation(wild_type, num_mutations=10, alphabet=self.all_characters)\n",
    "        \n",
    "        if not os.path.isdir(\"./models/{0}\".format(self.name)):\n",
    "            os.mkdir(\"./models/{0}\".format(self.name))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.train_loss_history, self.test_loss_history = [], []\n",
    "        self.reconstruction_loss_history, self.kld_loss_history = [], []\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            #train model\n",
    "            self.model.train()\n",
    "            train_loss, reconstruction_loss, kld_loss = 0, 0, 0\n",
    "            for batch_idx, (x, _) in enumerate(train_dataloader):\n",
    "                x = x.to(self.device)\n",
    "                #labels = x.view(x.shape[0], -1, len(self.all_characters)).argmax(dim = 2)\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                rloss, kloss = self.NLLoss(recon_x, x), self.KLD(mu, logvar)\n",
    "                loss = rloss + kloss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                reconstruction_loss += rloss.item()\n",
    "                kld_loss += kloss.item()\n",
    "            self.train_loss_history.append(train_loss / len(train_dataloader.dataset))\n",
    "            self.reconstruction_loss_history.append(reconstruction_loss / len(train_dataloader.dataset))\n",
    "            self.kld_loss_history.append(kld_loss / len(train_dataloader.dataset))\n",
    "            #evaluate model\n",
    "            self.model.eval()\n",
    "            decoder_outputs, _ = self.sample(num_samples=10, softmax=True)\n",
    "            generated_sequences = [self.sample_tensor_to_string(tensor, softmax=False) for tensor in decoder_outputs]\n",
    "            mismatches = [count_substring_mismatch(wild_type, sequence) for sequence in generated_sequences]\n",
    "            wild_prob, mutation_three_prob, mutation_ten_prob = self.predict_elbo_prob([wild_type]), self.predict_elbo_prob([three_mutation]), self.predict_elbo_prob([ten_mutation])\n",
    "            \n",
    "            if verbose: \n",
    "                print('<====> Epoch: {0}. Average loss: {1:.4f}. Reconstruction loss: {2:.2f}. KLD loss: {3:.2f}. Time: {4:.2f} seconds'.format(\n",
    "                      epoch, self.train_loss_history[-1], self.reconstruction_loss_history[-1], self.kld_loss_history[-1], time.time() - start_time), file = logger)\n",
    "                print(\"Sample generated sequence: {0}\\nAverage mismatches from the wild type: {1}\".format(generated_sequences[0], np.mean(mismatches)), file = logger) \n",
    "                print(\"wild type elbo prob: {0}. 3 mutations elbo prob: {1}. 10 mutations elbo prob: {2}.\" \\\n",
    "                      .format(wild_prob, mutation_three_prob, mutation_ten_prob), file = logger)\n",
    "            if test_dataloader:\n",
    "                test_loss = self.evaluate(test_dataloader, verbose, logger)\n",
    "                self.test_loss_history.append(test_loss)\n",
    "            if epoch % 100 == 0 and save_model:\n",
    "                self.save_model(epoch, train_loss)\n",
    "                print(\"finished saving model\", file=logger)\n",
    "     \n",
    "    def sample_tensor_to_string(self, x, softmax=False):\n",
    "        assert(type(x) == torch.Tensor)\n",
    "        assert(x.shape[0] % self.num_characters == 0 or x.shape[1] % self.num_characters == 0)\n",
    "        x = x.reshape(-1, self.num_characters)\n",
    "        if softmax:\n",
    "            x = F.softmax(x, dim=1)\n",
    "        string = []\n",
    "        for dist in x: \n",
    "            index = torch.multinomial(dist, 1).item()\n",
    "            string.append(self.int_to_character[index])\n",
    "        return \"\".join(string)\n",
    "    \n",
    "    def tensor_to_string(self, x):\n",
    "        \"\"\"\n",
    "        Input: A sequence in tensor format\n",
    "        Output: A sequence in string format\n",
    "        Example: tensor_to_string(torch.tensor([0, 0, 1, 0, 0, 0, 1, 0])) = \"TT\"\n",
    "        tensor_to_string(torch.tensor([0.8, 0.15, 0.05, 0, 0, 0.9, 0.1, 0])) = \"AC\"\n",
    "        note: alphabet is \"ACTG\" in this example\n",
    "        \"\"\"\n",
    "        assert(type(x) == torch.Tensor)\n",
    "        assert(len(x) % self.num_characters == 0)\n",
    "        x = x.reshape(-1, self.num_characters)\n",
    "        _, index = x.max(dim = 1)\n",
    "        return \"\".join([self.int_to_character[i] for i in index.numpy()])\n",
    "        \n",
    "    def predict_elbo_prob(self, sequences, string=True):\n",
    "        \"\"\"\n",
    "        Input: list of sequences in string or one_hot_encoded form\n",
    "        Output: list of the elbo probability for each sequence\n",
    "        Example: predict_elbo_prob([\"ACT\", \"ACG\"]) = [0.2, 0.75]\n",
    "        predict_elbo_prob([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],  \n",
    "                        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]]) = [0.2, 0.75]\n",
    "        note: alphabet in this example is ACTG and the wild type is probably ACG***\n",
    "        \"\"\"\n",
    "        if string: \n",
    "            sequences = one_hot_encode(sequences, self.all_characters)\n",
    "        if type(sequences) != torch.Tensor:\n",
    "            x = self.to_tensor(sequences)\n",
    "        recon_x, mu, logvar = self.model(x)\n",
    "        return self.elbo_loss(recon_x, x, mu, logvar)\n",
    "    \n",
    "    def evaluate(self, dataloader, verbose=True, logger=None):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        mismatches = []\n",
    "        wild_type_mismatches, wild_type = [], get_wild_type_amino_acid_sequence()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, _) in enumerate(dataloader):\n",
    "                x = x.to(self.device)\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                test_loss += self.elbo_loss(recon_x, x, mu, logvar).item()\n",
    "                recon_str, x_str = self.sample_tensor_to_string(recon_x[0], softmax=True), self.tensor_to_string(x[0])\n",
    "                mismatches.append(count_substring_mismatch(x_str, recon_str))\n",
    "                wild_type_mismatches.append(count_substring_mismatch(wild_type, recon_str))\n",
    "        test_loss /= len(dataloader.dataset)\n",
    "        if verbose: \n",
    "            print('Test set loss: {0:.4f} Average Mismatches: {1:.4f} Wild Type Mismatches {2:.4f} <====> \\n'.format(test_loss, np.mean(mismatches), np.mean(wild_type_mismatches)), file=logger)\n",
    "        return test_loss\n",
    "    \n",
    "    def to_tensor(self, x): \n",
    "        assert(type(x) == np.ndarray)\n",
    "        return torch.from_numpy(x).float().to(self.device)\n",
    "    \n",
    "    def decoder(self, z, softmax=False):\n",
    "        \"\"\" Note that the outputs are unnormalized \"\"\"\n",
    "        assert(z.shape[1] == self.latent_dim)\n",
    "        if type(z) != torch.Tensor:\n",
    "            z = self.to_tensor(z)\n",
    "        return self.model.decode(z, softmax=softmax)\n",
    "    \n",
    "    def encoder(self, x, reparameterize=False): \n",
    "        assert(x.shape[1] == self.input)\n",
    "        if type(x) != torch.Tensor:\n",
    "            x = self.to_tensor(x)\n",
    "        mu, log_var = self.model.encode(x)\n",
    "        if reparameterize: \n",
    "            return self.model.reparameterize(mu, log_var), mu, log_var\n",
    "        else: \n",
    "            return mu, log_var\n",
    "        \n",
    "    def sample(self, num_samples = 1, z = None, softmax=True): \n",
    "        if z is None: \n",
    "            z = torch.randn(num_samples, self.latent_dim).to(self.device)\n",
    "        return self.decoder(z, softmax=softmax), z\n",
    "            \n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    def save_model(self, epoch=None, loss=None): \n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict()\n",
    "                }, \"./models/{0}/checkpoint_{1}.pt\".format(self.name, epoch))\n",
    "\n",
    "    def show_model(self, logger=None): \n",
    "        print(self.model, file=logger)\n",
    "    \n",
    "    def plot_model(self, save_dir, verbose=False): \n",
    "        wild_type = get_wild_type_amino_acid_sequence()\n",
    "        one_hot_wild_type = one_hot_encode([wild_type], self.all_characters)\n",
    "        one_hot_tensor_wild_type = self.to_tensor(one_hot_wild_type)\n",
    "        out, _, _ = self.model(one_hot_tensor_wild_type)\n",
    "        graph = make_dot(out)\n",
    "        if save_dir is not None:\n",
    "            graph.format = \"png\"\n",
    "            graph.render(save_dir) \n",
    "        if verbose:\n",
    "            graph.view()\n",
    "            \n",
    "    def print_vars(self):\n",
    "        print(self.__dict__)\n",
    "        \n",
    "    def plot_history(self, save_fig_dir): \n",
    "        plt.figure()\n",
    "        plt.title(\"{0} Training Loss Curve\".format(self.name))\n",
    "        plt.plot(self.train_loss_history, label=\"train\")\n",
    "        if \"test_loss_history\" in self.__dict__:\n",
    "            plt.plot(self.test_loss_history, label=\"validation\")\n",
    "        if \"reconstruction_loss_history\" in self.__dict__:\n",
    "            plt.plot(self.reconstruction_loss_history, label=\"reconstruction_loss\")\n",
    "        if \"kld_loss_history\" in self.__dict__:\n",
    "            plt.plot(self.kld_loss_history, label=\"kld_loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        if save_fig_dir:\n",
    "            plt.savefig(save_fig_dir)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_args():\n",
    "    args = {\n",
    "        \"name\" : \"vae_fc5_epochs_1000\",\n",
    "        \"input\" : 21 * 238, \n",
    "        \"hidden_size\" : 50,\n",
    "        \"latent_dim\" : 20,\n",
    "        \"device\" : torch.device(\"cpu\"),\n",
    "        \"learning_rate\" : 0.001,\n",
    "        \"epochs\" : 1000,\n",
    "        \"beta\" : 1.0,\n",
    "        \"vocabulary\" : get_all_amino_acids(),\n",
    "        \"num_data\" : 1000, \n",
    "        \"batch_size\" : 10\n",
    "    }\n",
    "    args[\"description\"] = \"name: {0}, input size {1}, hidden size {2}, latent_dim {3}, lr {4}, epochs {5}\".format(\n",
    "                args[\"name\"], args[\"input\"], args[\"hidden_size\"], args[\"latent_dim\"], args[\"learning_rate\"], args[\"epochs\"])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_amino_acid_shuffle_\")\n",
    "args = get_test_args()\n",
    "amino_acid_alphabet = get_all_amino_acids()\n",
    "amino_acid_wild_type = get_wild_type_amino_acid_sequence()\n",
    "one_hot_X_train = one_hot_encode(X_train[:args[\"num_data\"]], amino_acid_alphabet)\n",
    "one_hot_X_test = one_hot_encode(X_test[:args[\"num_data\"]], amino_acid_alphabet)\n",
    "y_train, y_test = y_train[:args[\"num_data\"]], y_test[:args[\"num_data\"]]\n",
    "train_dataset = TensorDataset(torch.from_numpy(one_hot_X_train).float(), torch.from_numpy(y_train.reshape(-1, 1)).float())\n",
    "test_dataset = TensorDataset(torch.from_numpy(one_hot_X_test).float(), torch.from_numpy(y_test.reshape(-1, 1)).float())\n",
    "train_loader, test_loader = DataLoader(train_dataset, batch_size=args[\"batch_size\"], shuffle=True), DataLoader(test_dataset, batch_size=args[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = GenerativeVAE(args)\n",
    "logger = open(\"./logs/vae/{0}.txt\".format(vae.name), 'w')\n",
    "vae.fit(train_loader, test_loader, True, logger, \"./models/{0}/\".format(vae.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.plot_model(\"./logs/vae/{0}_model_architecture\".format(vae.name))\n",
    "vae.plot_history(\"./logs/vae/{0}_training_history\".format(vae.name))\n",
    "vae.show_model(logger)\n",
    "if logger: \n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vae = GenerativeVAE(args)\n",
    "load_vae.load_model(\"./models/{0}/checkpoint_1000.pt\".format(vae.name))\n",
    "for parameter_name, load_weights in load_vae.model.state_dict().items():\n",
    "    vae_weights = vae.model.state_dict()[parameter_name]\n",
    "    assert(torch.all(torch.eq(load_weights, vae_weights)).item())\n",
    "\n",
    "for (x, _) in test_loader:         \n",
    "    x = x.to(load_vae.device)\n",
    "    z, z_mean, z_var = load_vae.encoder(x, reparameterize=True)\n",
    "    z_mean_2, z_var_2 = vae.encoder(x)\n",
    "    assert(torch.all(torch.eq(z_mean, z_mean_2)).item())\n",
    "    assert(torch.all(torch.eq(z_var, z_var_2)).item())\n",
    "    recon_x = load_vae.decoder(z)\n",
    "    recon_x_2 = vae.decoder(z)\n",
    "    loss_1 = vae.elbo_loss(recon_x, x, z_mean, z_var).item()\n",
    "    loss_2 = vae.elbo_loss(recon_x_2, x, z_mean, z_var).item()\n",
    "    np.testing.assert_equal(loss_1, loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "vocabulary = get_all_amino_acids()\n",
    "num_characters = len(get_all_amino_acids())\n",
    "index_to_character = dict(zip(range(num_characters), vocabulary))\n",
    "\n",
    "z = np.random.sample((num_samples, 20))\n",
    "outputs = vae.model.decode(torch.tensor(z).float())\n",
    "outputs = outputs.view(outputs.shape[0], -1, num_characters)\n",
    "outputs = F.softmax(outputs, dim=2)\n",
    "outputs = outputs.detach().numpy()\n",
    "print(outputs.shape)\n",
    "mismatches, all_strings = [], []\n",
    "for i in range(outputs.shape[0]):\n",
    "    string = []\n",
    "    for j in range(outputs.shape[1]):\n",
    "        k = np.random.choice(list(range(num_characters)), p = outputs[i, j])\n",
    "        string.append(index_to_character[k])\n",
    "    all_strings.append(\"\".join(string))\n",
    "    mismatches.append(count_substring_mismatch(wild_type, all_strings[-1]))\n",
    "    if i % 100 == 0:\n",
    "        print(\"finished {0}/{1} samples\".format(i, num_samples))\n",
    "        \n",
    "plt.title(\"number of mismatches\")\n",
    "plt.hist(mismatches)\n",
    "plt.xlabel(\"mismatches\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.savefig(\"./logs/vae/{0}_mismatches_from_wild_type\".format(vae.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mismatches = []\n",
    "for amino_acid_seq in X_train[0:1000]:\n",
    "    data_mismatches.append(count_substring_mismatch(amino_acid_seq, wild_type))\n",
    "plt.title(\"number of mismatches\")\n",
    "plt.hist(data_mismatches, bins = 15)\n",
    "plt.xlabel(\"mismatches\")\n",
    "plt.ylabel(\"counts\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logger:\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(784, 400, 20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'logs/vae/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\"\"\"\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 2\n",
    "        self.no_cuda = True\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        \n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_index(string, alphabet):\n",
    "    return np.array([alphabet.index(s) for s in string])\n",
    "\n",
    "wild_type_index = string_to_index(get_wild_type_amino_acid_sequence(), alphabet = get_all_amino_acids())\n",
    "wild_type_index_tensor = torch.from_numpy(wild_type_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0822, 0.0137, 0.0320, 0.0091, 0.0639, 0.0776, 0.0046, 0.0502, 0.0776,\n",
      "         0.0913, 0.0594, 0.0137, 0.0000, 0.0868, 0.0228, 0.0320, 0.0411, 0.0548,\n",
      "         0.0411, 0.0776, 0.0685]], dtype=torch.float64) tensor([1.0000, 1.0000], dtype=torch.float64)\n",
      "tensor([1.0000, 0.9087], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "normalized_prob = np.random.randint(0, 21, 21)\n",
    "normalized_prob = normalized_prob / normalized_prob.sum()\n",
    "x = torch.tensor([[0] * 15 + [1] + [0] * 5, normalized_prob])\n",
    "wild_type_probs = []\n",
    "for probs, index in zip(x, wild_type_index):\n",
    "    wild_type_probs.append(probs[index])\n",
    "\n",
    "sums = x.sum(dim = 1)\n",
    "print(x, sums)\n",
    "sums = sums - torch.tensor(wild_type_probs)\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(2, 3, 4, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4653, -1.0134,  0.0671, -2.0208, -0.0811],\n",
      "        [-0.5681,  1.4572,  1.2459, -0.1435,  0.7575],\n",
      "        [ 0.2731, -2.1939,  0.1123, -0.6824,  0.4075]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 5)\n",
    "print(x)\n",
    "x.argmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 210])\n",
      "torch.Size([2, 10])\n",
      "tensor([-1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08,  1.0000e+00, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08], dtype=torch.float64)\n",
      "torch.Size([2, 21, 10])\n",
      "tensor([-1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08,  1.0000e+00, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08], dtype=torch.float64)\n",
      "tensor([ 1.0000e+00, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-69d7a6fefe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_amino_acids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_amino_acids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             ret = torch._C._nn.nll_loss2d(\n\u001b[0;32m-> 1806\u001b[0;31m                 input, target, weight, reduction_enum, ignore_index)\n\u001b[0m\u001b[1;32m   1807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m             out = torch._C._nn.nll_loss2d(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "length = 10\n",
    "wild_type = get_wild_type_amino_acid_sequence()\n",
    "one_hot = one_hot_encode([wild_type[0:length], wild_type[0:length]], get_all_amino_acids())\n",
    "for i in range(one_hot.shape[0]): \n",
    "    for j in range(one_hot.shape[1]): \n",
    "        if not one_hot[i, j]:\n",
    "            one_hot[i, j] = eps\n",
    "        else:\n",
    "            one_hot[i, j] = 1\n",
    "            \n",
    "one_hot_tensor = torch.from_numpy(one_hot)\n",
    "print(one_hot_tensor.shape)\n",
    "labels = one_hot_tensor.view(2, length, len(get_all_amino_acids())).argmax(dim = 2).float()\n",
    "print(labels.shape)\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids()))[0][0])\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1).shape)\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1)[0, :, 0])\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1)[0, 16])\n",
    "x = one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1)\n",
    "z = nn.CrossEntropyLoss(reduction='sum')(x, labels).item()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = -1e8\n",
    "x = torch.tensor(np.array([[1, eps, eps], [eps, 1, eps]])).float()\n",
    "labels = torch.tensor(np.array([0, 1]))\n",
    "print(torch.all(torch.eq(x.argmax(1), labels)).item() == 1)\n",
    "F.cross_entropy(x, labels, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 63)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x if x else eps for x in one_hot[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1489, -1.4456, -3.2613, -3.3038, -0.5574],\n",
       "         [-2.2018, -1.3564, -1.8980, -0.8716, -2.7541],\n",
       "         [-1.0349, -1.3823, -1.8581, -3.3522, -1.5958]],\n",
       "\n",
       "        [[-1.8522, -3.5563, -0.7642, -3.0691, -1.1961],\n",
       "         [-2.2367, -2.4859, -0.9454, -2.5285, -1.0740],\n",
       "         [-1.3448, -2.4309, -1.0460, -2.8123, -1.4270]],\n",
       "\n",
       "        [[-3.8434, -0.5309, -2.8926, -2.6222, -1.3378],\n",
       "         [-2.8145, -2.0604, -0.9137, -1.0996, -2.5432],\n",
       "         [-0.4621, -2.3404, -2.8733, -2.2319, -2.2081]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1489, -1.4456, -3.2613, -3.3038, -0.5574],\n",
       "         [-2.2018, -1.3564, -1.8980, -0.8716, -2.7541],\n",
       "         [-1.0349, -1.3823, -1.8581, -3.3522, -1.5958]],\n",
       "\n",
       "        [[-1.8522, -3.5563, -0.7642, -3.0691, -1.1961],\n",
       "         [-2.2367, -2.4859, -0.9454, -2.5285, -1.0740],\n",
       "         [-1.3448, -2.4309, -1.0460, -2.8123, -1.4270]],\n",
       "\n",
       "        [[-3.8434, -0.5309, -2.8926, -2.6222, -1.3378],\n",
       "         [-2.8145, -2.0604, -0.9137, -1.0996, -2.5432],\n",
       "         [-0.4621, -2.3404, -2.8733, -2.2319, -2.2081]]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = F.log_softmax(x, dim=2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(3, 3, 5)\n",
    "x[:, :, 4] = 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6936)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(z * x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
