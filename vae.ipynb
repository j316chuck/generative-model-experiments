{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchviz import make_dot\n",
    "from utils import one_hot_encode, one_hot_decode, get_all_amino_acids, get_wild_type_amino_acid_sequence\n",
    "from utils import load_gfp_data, count_substring_mismatch, get_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # change architecture later to make it deeper if it's not good enough to capture all data\n",
    "    def __init__(self, input_size, hidden_size, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc21 = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, input_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # input should be one hot encoded. shape - (batch_size, alphabet x sequence_length)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_size))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeVAE(): \n",
    "    \n",
    "    def __init__(self, args):     \n",
    "        \"\"\"\n",
    "        Initializes the VAE to be a generative VAE\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the neural network\n",
    "        args.name : string \n",
    "            defines the name of the neural network\n",
    "        args.description: string\n",
    "            describes the architecture of the neural network\n",
    "        args.input : int\n",
    "            the size of the input\n",
    "        args.hidden_size : int\n",
    "            the size of the hidden layer\n",
    "        args.latent_dim: int \n",
    "            the size of the latent dimension\n",
    "        args.device : device\n",
    "            the device used: cpu or gpu\n",
    "        args.learning_rate : float\n",
    "            sets the learning rate\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        args.beta : float\n",
    "            sets the beta parameter for the KL divergence loss\n",
    "        args.vocabulary : string\n",
    "            all the characters in the context of the problem\n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.description = args[\"description\"]\n",
    "        self.input = args[\"input\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.latent_dim = args[\"latent_dim\"]\n",
    "        self.device = args[\"device\"]\n",
    "        self.learning_rate = args[\"learning_rate\"]\n",
    "        self.epochs = args[\"epochs\"]\n",
    "        self.beta = args[\"beta\"]\n",
    "        self.all_characters = args[\"vocabulary\"]\n",
    "        self.num_characters = len(self.all_characters)\n",
    "        self.character_to_int = dict(zip(self.all_characters, range(self.num_characters)))\n",
    "        self.int_to_character = dict(zip(range(self.num_characters), self.all_characters))\n",
    "        self.model = VAE(self.input, self.hidden_size, self.latent_dim)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_history = []\n",
    "        \n",
    "    # Reconstruction + KL divergence losses summed over all elements in batch\n",
    "    def elbo_loss(self, recon_x, x, mu, logvar):\n",
    "        \"\"\"\n",
    "        Input: x is the one hot encoded batch_size x (seq_length * len(all_characters)) \n",
    "               recon_x is the unormalized outputs of the decoder in the same shape as x\n",
    "               mu and logvar are the hidden states of size self.hidden_size\n",
    "        Output: elbo_loss\n",
    "        \"\"\"\n",
    "        # get the argmax of each batch_size x seq_length * len(all_characters) matrix. Output is in batch_size x seq_length form\n",
    "        with torch.no_grad(): \n",
    "            labels = x.view(x.shape[0], -1, len(self.all_characters)).argmax(dim = 2)\n",
    "        \n",
    "        #print(labels)\n",
    "        # reshapes the recon_x vector to be of shape batch_size x len(all_characters) x seq_length so that it fits according to PyTorch's CrossEntropyLoss\n",
    "        # permute is transpose function so at each 1, 2 dimension we take the transpose\n",
    "        #print(recon_x.shape)\n",
    "        #print(reshape_x[0,:,0])\n",
    "        new_x = recon_x.view(recon_x.shape[0], -1, len(self.all_characters)).permute(0, 2, 1)\n",
    "        CE = F.cross_entropy(new_x, labels, reduction = 'sum')\n",
    "        \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        #print(\"log var shape:\", logvar.shape, \"mu shape: \", mu.shape, \"logvar: \", logvar.sum(dim=1))\n",
    "        #print(\"mu: \", mu.sum(dim=1))\n",
    "        #print((1 + logvar - mu.pow(2) - logvar.exp()).shape)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * self.beta\n",
    "\n",
    "        print(\"CE Loss: \", CE, \"KLD Loss:\", KLD, file=logger)\n",
    "        return CE + KLD\n",
    "    \n",
    "    def fit(self, train_dataloader, test_dataloader=None, verbose=True, logger=None, save_model=True):\n",
    "        # amino acid dataset specific checks\n",
    "        wild_type = get_wild_type_amino_acid_sequence()\n",
    "        three_mutation = get_mutation(wild_type, num_mutations=3, alphabet=self.all_characters)\n",
    "        ten_mutation = get_mutation(wild_type, num_mutations=10, alphabet=self.all_characters)\n",
    "        \n",
    "        if not os.path.isdir(\"./models/{0}\".format(self.name)):\n",
    "            os.mkdir(\"./models/{0}\".format(self.name))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.loss_history = []\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            \n",
    "            #train model\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch_idx, (x, _) in enumerate(train_dataloader):\n",
    "                x = x.to(self.device)\n",
    "                labels = x.view(x.shape[0], -1, len(self.all_characters)).argmax(dim = 2)\n",
    "        \n",
    "                self.optimizer.zero_grad()\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                loss = self.elbo_loss(recon_x, x, mu, logvar)\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()  \n",
    "                self.optimizer.step()\n",
    "            self.loss_history.append(train_loss / len(train_dataloader.dataset))\n",
    "             \n",
    "            #evaluate model\n",
    "            self.model.eval()\n",
    "            decoder_outputs, latent_z = self.sample(num_samples = 10)\n",
    "            generated_sequences = [self.tensor_to_string(tensor) for tensor in decoder_outputs]\n",
    "            mismatches = [count_substring_mismatch(wild_type, sequence) for sequence in generated_sequences]\n",
    "            wild_prob, mutation_three_prob, mutation_ten_prob = self.predict_elbo_prob([wild_type]), self.predict_elbo_prob([three_mutation]), self.predict_elbo_prob([ten_mutation])\n",
    "            \n",
    "            if verbose: \n",
    "                print('<====> Epoch: {0}. Average loss: {1:.4f}. Time: {2:.2f} seconds'.format(\n",
    "                      epoch, self.loss_history[-1], time.time() - start_time), file = logger)\n",
    "                print(\"Sample generated sequence: {0}\\nAverage mismatches from the wild type: {1}\".format(generated_sequences[0], np.mean(mismatches)), file = logger) \n",
    "                print(\"wild type elbo prob: {0}. 3 mutations elbo prob: {1}. 10 mutations elbo prob: {2}.\" \\\n",
    "                      .format(wild_prob, mutation_three_prob, mutation_ten_prob), file = logger)\n",
    "            if test_dataloader:\n",
    "                test_loss = self.evaluate(test_dataloader, verbose, logger)\n",
    "            if epoch % 10 == 0 and save_model:\n",
    "                self.save_model(epoch, train_loss)\n",
    "                print(\"finished saving model\", file=logger)\n",
    "        \n",
    "    def tensor_to_string(self, x):\n",
    "        \"\"\"\n",
    "        Input: A sequence in tensor format\n",
    "        Output: A sequence in string format\n",
    "        Example: tensor_to_string(torch.tensor([0, 0, 1, 0, 0, 0, 1, 0])) = \"TT\"\n",
    "        tensor_to_string(torch.tensor([0.8, 0.15, 0.05, 0, 0, 0.9, 0.1, 0])) = \"AC\"\n",
    "        note: alphabet is \"ACTG\" in this example\n",
    "        \"\"\"\n",
    "        assert(type(x) == torch.Tensor)\n",
    "        assert(len(x) % self.num_characters == 0)\n",
    "        x = x.reshape(-1, self.num_characters)\n",
    "        _, index = x.max(dim = 1)\n",
    "        return \"\".join([self.int_to_character[i] for i in index.numpy()])\n",
    "        \n",
    "    def predict_elbo_prob(self, sequences, string=True):\n",
    "        \"\"\"\n",
    "        Input: list of sequences in string or one_hot_encoded form\n",
    "        Output: list of the elbo probability for each sequence\n",
    "        Example: predict_elbo_prob([\"ACT\", \"ACG\"]) = [0.2, 0.75]\n",
    "        predict_elbo_prob([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],  \n",
    "                        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]]) = [0.2, 0.75]\n",
    "        note: alphabet in this example is ACTG and the wild type is probably ACG***\n",
    "        \"\"\"\n",
    "        if string: \n",
    "            sequences = one_hot_encode(sequences, self.all_characters)\n",
    "        if type(sequences) != torch.Tensor:\n",
    "            x = self.to_tensor(sequences)\n",
    "        recon_x, mu, logvar = self.model(x)\n",
    "        return self.elbo_loss(recon_x, x, mu, logvar)\n",
    "    \n",
    "    def evaluate(self, dataloader, verbose=True, logger=None):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        mismatches = []\n",
    "        wild_type_mismatches, wild_type = [], get_wild_type_amino_acid_sequence()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, _) in enumerate(dataloader):\n",
    "                x = x.to(self.device)\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                test_loss += self.elbo_loss(recon_x, x, mu, logvar).item()\n",
    "                recon_str, x_str = self.tensor_to_string(recon_x[0]), self.tensor_to_string(x[0])\n",
    "                mismatches.append(count_substring_mismatch(x_str, recon_str))\n",
    "                wild_type_mismatches.append(count_substring_mismatch(wild_type, recon_str))\n",
    "        test_loss /= len(dataloader.dataset)\n",
    "        if verbose: \n",
    "            print('Test set loss: {0:.4f} Average Mismatches: {1:.4f} Wild Type Mismatches {2:.4f} <====> \\n'.format(test_loss, np.mean(mismatches), np.mean(wild_type_mismatches)), file=logger)\n",
    "        return test_loss\n",
    "    \n",
    "    def to_tensor(self, x): \n",
    "        assert(type(x) == np.ndarray)\n",
    "        return torch.from_numpy(x).float().to(self.device)\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        assert(z.shape[1] == self.latent_dim)\n",
    "        if type(z) != torch.Tensor:\n",
    "            z = self.to_tensor(z)\n",
    "        return self.model.decode(z)\n",
    "    \n",
    "    def encoder(self, x, reparameterize=False): \n",
    "        assert(x.shape[1] == self.input)\n",
    "        if type(x) != torch.Tensor:\n",
    "            x = self.to_tensor(x)\n",
    "        mu, log_var = self.model.encode(x)\n",
    "        if reparameterize: \n",
    "            return self.model.reparameterize(mu, log_var), mu, log_var\n",
    "        else: \n",
    "            return mu, log_var\n",
    "        \n",
    "    def sample(self, num_samples = 1, z = None): \n",
    "        if z is None: \n",
    "            z = torch.randn(num_samples, self.latent_dim).to(self.device)\n",
    "        return self.decoder(z), z\n",
    "            \n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    def save_model(self, epoch=None, loss=None): \n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict()\n",
    "                }, \"./models/{0}/checkpoint_{1}.pt\".format(self.name, epoch))\n",
    "\n",
    "    def show_model(self, logger=None): \n",
    "        print(self.model, file=logger)\n",
    "    \n",
    "    def plot_model(self, save_dir, verbose=False): \n",
    "        wild_type = get_wild_type_amino_acid_sequence()\n",
    "        one_hot_wild_type = one_hot_encode([wild_type], self.all_characters)\n",
    "        one_hot_tensor_wild_type = self.to_tensor(one_hot_wild_type)\n",
    "        out, _, _ = self.model(one_hot_tensor_wild_type)\n",
    "        graph = make_dot(out)\n",
    "        if save_dir is not None:\n",
    "            graph.format = \"png\"\n",
    "            graph.render(save_dir) \n",
    "        if verbose:\n",
    "            graph.view()\n",
    "            \n",
    "    def plot_history(self, save_fig_dir): \n",
    "        plt.figure()\n",
    "        plt.title(\"{0} Training Loss Curve\".format(self.name))\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xticks(range(self.epochs))\n",
    "        if save_fig_dir:\n",
    "            plt.savefig(save_fig_dir)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_args():\n",
    "    args = {\n",
    "        \"name\" : \"vae_test_sample\",\n",
    "        \"input\" : 21 * 238, \n",
    "        \"hidden_size\" : 400,\n",
    "        \"latent_dim\" : 20,\n",
    "        \"device\" : torch.device(\"cpu\"),\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"epochs\" : 30,\n",
    "        \"beta\" : 1.0,\n",
    "        \"vocabulary\" : get_all_amino_acids(),\n",
    "        \"num_data\" : 1000, \n",
    "        \"batch_size\" : 100\n",
    "    }\n",
    "    args[\"description\"] = \"name: {0}, input size {1}, hidden size {2}, latent_dim {3}, lr {4}, epochs {5}\".format(\n",
    "                args[\"name\"], args[\"input\"], args[\"hidden_size\"], args[\"latent_dim\"], args[\"learning_rate\"], args[\"epochs\"])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_amino_acid_\")\n",
    "args = get_test_args()\n",
    "amino_acid_alphabet = get_all_amino_acids()\n",
    "amino_acid_wild_type = get_wild_type_amino_acid_sequence()\n",
    "one_hot_X_train = one_hot_encode(X_train[:args[\"num_data\"]], amino_acid_alphabet)\n",
    "one_hot_X_test = one_hot_encode(X_test[:args[\"num_data\"]], amino_acid_alphabet)\n",
    "y_train, y_test = y_train[:args[\"num_data\"]], y_test[:args[\"num_data\"]]\n",
    "train_dataset = TensorDataset(torch.from_numpy(one_hot_X_train).float(), torch.from_numpy(y_train.reshape(-1, 1)).float())\n",
    "test_dataset = TensorDataset(torch.from_numpy(one_hot_X_test).float(), torch.from_numpy(y_test.reshape(-1, 1)).float())\n",
    "train_loader, test_loader = DataLoader(train_dataset, batch_size=args[\"batch_size\"], shuffle=True), DataLoader(test_dataset, batch_size=args[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(73064.3906, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(4.5386, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(42160.3906, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(2147.3135, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1934.5663, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9934.9854, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1876.0782, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(11588.0713, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(2181.7178, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7718.0918, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1997.8119, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(3473.4978, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1660.1810, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(1079.8041, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(4332.4580, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(300.1061, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3305.1521, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(220.5041, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1948.0272, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(306.1575, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.6259, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(4.7463, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(29.1449, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(4.7424, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(95.9556, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(4.3538, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 1. Average loss: 171.2338. Time: 1.06 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 5.372117042541504. 3 mutations elbo prob: 33.88724899291992. 10 mutations elbo prob: 100.3093490600586.\n",
      "CE Loss:  tensor(3306.0518) KLD Loss: tensor(449.0092)\n",
      "CE Loss:  tensor(3392.9380) KLD Loss: tensor(448.0615)\n",
      "CE Loss:  tensor(3326.6057) KLD Loss: tensor(446.8949)\n",
      "CE Loss:  tensor(3402.7371) KLD Loss: tensor(451.1811)\n",
      "CE Loss:  tensor(3326.2959) KLD Loss: tensor(447.0683)\n",
      "CE Loss:  tensor(3432.0962) KLD Loss: tensor(448.4275)\n",
      "CE Loss:  tensor(3473.4412) KLD Loss: tensor(450.3539)\n",
      "CE Loss:  tensor(3419.4861) KLD Loss: tensor(447.3996)\n",
      "CE Loss:  tensor(3368.6333) KLD Loss: tensor(445.2364)\n",
      "CE Loss:  tensor(3248.8262) KLD Loss: tensor(452.0452)\n",
      "Test set loss: 38.1828 Average Mismatches: 4.3000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1454.6776, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(460.0888, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1467.8220, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(601.9987, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1606.6229, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(647.5104, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1776.1515, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(543.2114, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1532.1150, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(361.6342, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1072.9869, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(208.0426, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1547.7948, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(121.7859, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1240.9757, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(91.1872, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1303.1434, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(83.8091, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1394.9806, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(91.5934, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.3295, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(1.2158, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(35.1480, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(1.1639, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(88.8216, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(1.1007, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 2. Average loss: 17.6081. Time: 3.38 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.5452942848205566. 3 mutations elbo prob: 36.3118782043457. 10 mutations elbo prob: 89.9222640991211.\n",
      "CE Loss:  tensor(3655.0093) KLD Loss: tensor(107.2418)\n",
      "CE Loss:  tensor(3732.7297) KLD Loss: tensor(107.4688)\n",
      "CE Loss:  tensor(3394.2505) KLD Loss: tensor(112.0429)\n",
      "CE Loss:  tensor(3683.9431) KLD Loss: tensor(110.9529)\n",
      "CE Loss:  tensor(3445.6890) KLD Loss: tensor(107.3478)\n",
      "CE Loss:  tensor(3515.7937) KLD Loss: tensor(108.6370)\n",
      "CE Loss:  tensor(3418.7773) KLD Loss: tensor(111.5213)\n",
      "CE Loss:  tensor(3583.1558) KLD Loss: tensor(109.1756)\n",
      "CE Loss:  tensor(3546.5862) KLD Loss: tensor(107.5587)\n",
      "CE Loss:  tensor(3839.8960) KLD Loss: tensor(107.5321)\n",
      "Test set loss: 36.9053 Average Mismatches: 3.8000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1182.0839, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(113.5619, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1405.7740, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(127.5668, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1418.8524, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(126.0130, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1233.3550, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(109.6401, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1117.8284, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(87.2689, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1613.7140, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(66.1469, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1066.2314, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(51.1912, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1535.0321, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.6951, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1045.8625, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(38.9036, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1200.4900, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.7742, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(19.4822, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.3085, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(33.3571, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.3041, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(104.9144, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2931, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 3. Average loss: 13.6170. Time: 5.64 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 19.790674209594727. 3 mutations elbo prob: 33.66120147705078. 10 mutations elbo prob: 105.2074203491211.\n",
      "CE Loss:  tensor(3337.5237) KLD Loss: tensor(30.1078)\n",
      "CE Loss:  tensor(3460.8018) KLD Loss: tensor(30.2702)\n",
      "CE Loss:  tensor(3563.0500) KLD Loss: tensor(30.3278)\n",
      "CE Loss:  tensor(3369.4041) KLD Loss: tensor(30.2570)\n",
      "CE Loss:  tensor(3421.6299) KLD Loss: tensor(30.1573)\n",
      "CE Loss:  tensor(3155.1519) KLD Loss: tensor(30.3064)\n",
      "CE Loss:  tensor(3243.2681) KLD Loss: tensor(30.2431)\n",
      "CE Loss:  tensor(3174.3447) KLD Loss: tensor(30.1154)\n",
      "CE Loss:  tensor(3193.2839) KLD Loss: tensor(30.3080)\n",
      "CE Loss:  tensor(3460.1372) KLD Loss: tensor(30.3390)\n",
      "Test set loss: 33.6810 Average Mismatches: 3.8000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1043.7413, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.0840, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1328.4229, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.9680, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1319.9742, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(22.7978, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1353.0773, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(23.3425, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1219.1627, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.4568, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1173.8516, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(28.5166, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1213.2548, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(29.5782, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(1208.6235, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(28.4469, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1156.3618, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.7229, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1221.7926, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.4817, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.5729, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2247, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(36.1691, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(140.1130, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1877, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 4. Average loss: 12.5017. Time: 7.85 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.797542929649353. 3 mutations elbo prob: 36.342803955078125. 10 mutations elbo prob: 140.30068969726562.\n",
      "CE Loss:  tensor(3355.0898) KLD Loss: tensor(16.4534)\n",
      "CE Loss:  tensor(3454.4229) KLD Loss: tensor(16.2594)\n",
      "CE Loss:  tensor(3573.4761) KLD Loss: tensor(15.7005)\n",
      "CE Loss:  tensor(3412.8164) KLD Loss: tensor(16.7048)\n",
      "CE Loss:  tensor(3528.3967) KLD Loss: tensor(15.4238)\n",
      "CE Loss:  tensor(3137.0645) KLD Loss: tensor(16.5313)\n",
      "CE Loss:  tensor(3578.4685) KLD Loss: tensor(15.4452)\n",
      "CE Loss:  tensor(3405.5605) KLD Loss: tensor(15.8250)\n",
      "CE Loss:  tensor(3394.1931) KLD Loss: tensor(16.3642)\n",
      "CE Loss:  tensor(3211.6953) KLD Loss: tensor(16.5372)\n",
      "Test set loss: 34.2124 Average Mismatches: 3.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1045.0652, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(18.7406, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1250.5703, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(14.8863, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1244.3698, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(12.5330, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1153.4708, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(11.1673, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1330.2181, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.0043, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1263.1788, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.5545, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1121.9263, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.8944, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1134.6362, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.7317, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1176.1113, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.8662, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1169.7220, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.4361, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.2222, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1251, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(26.4327, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0823, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(94.5519, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1002, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 5. Average loss: 12.0021. Time: 10.30 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.347271203994751. 3 mutations elbo prob: 26.51500129699707. 10 mutations elbo prob: 94.652099609375.\n",
      "CE Loss:  tensor(3362.7095) KLD Loss: tensor(7.4349)\n",
      "CE Loss:  tensor(3238.9204) KLD Loss: tensor(7.8757)\n",
      "CE Loss:  tensor(3430.6326) KLD Loss: tensor(7.8359)\n",
      "CE Loss:  tensor(3477.1550) KLD Loss: tensor(7.4376)\n",
      "CE Loss:  tensor(3387.9819) KLD Loss: tensor(7.7597)\n",
      "CE Loss:  tensor(3450.4465) KLD Loss: tensor(7.6169)\n",
      "CE Loss:  tensor(3561.9170) KLD Loss: tensor(7.5951)\n",
      "CE Loss:  tensor(3170.3120) KLD Loss: tensor(8.3416)\n",
      "CE Loss:  tensor(3459.9712) KLD Loss: tensor(8.0032)\n",
      "CE Loss:  tensor(3177.6975) KLD Loss: tensor(8.0921)\n",
      "Test set loss: 33.7957 Average Mismatches: 3.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1346.3772, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.6017, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1242.4567, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.7225, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1181.7771, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(12.0149, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1189.0596, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.5596, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(871.3457, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.8608, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(973.5403, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.2914, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1168.8740, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.5832, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1184.6942, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.0516, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1268.7296, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.8013, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1231.9291, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.0495, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.6690, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0839, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(27.4342, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0488, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(91.4407, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 6. Average loss: 11.7583. Time: 12.54 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.7528927326202393. 3 mutations elbo prob: 27.482988357543945. 10 mutations elbo prob: 91.50381469726562.\n",
      "CE Loss:  tensor(3068.5308) KLD Loss: tensor(5.6347)\n",
      "CE Loss:  tensor(3180.4163) KLD Loss: tensor(5.1146)\n",
      "CE Loss:  tensor(3491.5825) KLD Loss: tensor(5.4393)\n",
      "CE Loss:  tensor(3307.1677) KLD Loss: tensor(5.1889)\n",
      "CE Loss:  tensor(3334.6399) KLD Loss: tensor(5.1376)\n",
      "CE Loss:  tensor(3471.4563) KLD Loss: tensor(5.5403)\n",
      "CE Loss:  tensor(3292.7273) KLD Loss: tensor(5.4501)\n",
      "CE Loss:  tensor(3287.5876) KLD Loss: tensor(5.4602)\n",
      "CE Loss:  tensor(3481.5474) KLD Loss: tensor(5.3023)\n",
      "CE Loss:  tensor(3441.3767) KLD Loss: tensor(5.1467)\n",
      "Test set loss: 33.4104 Average Mismatches: 4.2000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1272.6798, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.3986, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1292.6785, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.7543, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(930.1885, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(6.7832, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1040.2114, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.1896, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1226.4103, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.4184, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1064.8162, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(6.4600, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1179.3380, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.1615, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(923.9059, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.1385, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1175.3142, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.6063, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1286.5714, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.2190, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.2316, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1265, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(27.6523, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(122.2877, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0860, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 7. Average loss: 11.4682. Time: 14.59 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.3580536842346191. 3 mutations elbo prob: 27.705699920654297. 10 mutations elbo prob: 122.37370300292969.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(3496.6604) KLD Loss: tensor(6.2412)\n",
      "CE Loss:  tensor(3354.6929) KLD Loss: tensor(5.8601)\n",
      "CE Loss:  tensor(3530.0251) KLD Loss: tensor(5.3205)\n",
      "CE Loss:  tensor(3359.0117) KLD Loss: tensor(5.4615)\n",
      "CE Loss:  tensor(3118.9639) KLD Loss: tensor(5.6047)\n",
      "CE Loss:  tensor(3279.7405) KLD Loss: tensor(6.2997)\n",
      "CE Loss:  tensor(3515.3533) KLD Loss: tensor(5.7022)\n",
      "CE Loss:  tensor(3303.0693) KLD Loss: tensor(5.4375)\n",
      "CE Loss:  tensor(3178.1497) KLD Loss: tensor(5.3442)\n",
      "CE Loss:  tensor(3581.8740) KLD Loss: tensor(5.5717)\n",
      "Test set loss: 33.7744 Average Mismatches: 4.2000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(929.1046, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.2853, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1136.9833, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.3791, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1364.6570, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.6290, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(936.7088, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.8558, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1017.5522, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.6210, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(891.1786, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(7.8968, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1239.1096, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.9857, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1235.5841, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.8291, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1363.6747, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.6034, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1076.2448, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.7726, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.5516, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0683, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(27.9645, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0292, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(105.1009, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0431, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 8. Average loss: 11.2787. Time: 16.75 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.6199028491973877. 3 mutations elbo prob: 27.993663787841797. 10 mutations elbo prob: 105.14400482177734.\n",
      "CE Loss:  tensor(3337.7600) KLD Loss: tensor(4.5375)\n",
      "CE Loss:  tensor(3355.8345) KLD Loss: tensor(4.5348)\n",
      "CE Loss:  tensor(3414.4148) KLD Loss: tensor(5.1804)\n",
      "CE Loss:  tensor(3513.2305) KLD Loss: tensor(5.1357)\n",
      "CE Loss:  tensor(3353.7378) KLD Loss: tensor(5.1043)\n",
      "CE Loss:  tensor(3003.7302) KLD Loss: tensor(5.0489)\n",
      "CE Loss:  tensor(3442.0764) KLD Loss: tensor(5.1993)\n",
      "CE Loss:  tensor(3329.2024) KLD Loss: tensor(4.7957)\n",
      "CE Loss:  tensor(3328.7812) KLD Loss: tensor(5.0162)\n",
      "CE Loss:  tensor(3214.6816) KLD Loss: tensor(4.4190)\n",
      "Test set loss: 33.3424 Average Mismatches: 3.9000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1089.7864, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.4490, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1202.6266, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(8.8963, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1116.4132, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.3918, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1388.2715, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.1844, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1089.9856, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.2098, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1082.8296, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.6546, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(910.6714, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.1342, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(922.4658, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.0813, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1232.1616, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(11.0655, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1081.8796, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.5854, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.6332, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(34.1590, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(96.7630, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0339, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 9. Average loss: 11.2167. Time: 18.94 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.7030569314956665. 3 mutations elbo prob: 34.17735290527344. 10 mutations elbo prob: 96.79696655273438.\n",
      "CE Loss:  tensor(3360.2527) KLD Loss: tensor(4.2551)\n",
      "CE Loss:  tensor(3087.3328) KLD Loss: tensor(4.1405)\n",
      "CE Loss:  tensor(3345.3018) KLD Loss: tensor(4.3751)\n",
      "CE Loss:  tensor(3420.7913) KLD Loss: tensor(3.8728)\n",
      "CE Loss:  tensor(3182.4250) KLD Loss: tensor(5.0335)\n",
      "CE Loss:  tensor(3361.1326) KLD Loss: tensor(4.7822)\n",
      "CE Loss:  tensor(3270.4194) KLD Loss: tensor(5.1972)\n",
      "CE Loss:  tensor(3498.0664) KLD Loss: tensor(4.8944)\n",
      "CE Loss:  tensor(3303.8347) KLD Loss: tensor(5.4559)\n",
      "CE Loss:  tensor(3590.2029) KLD Loss: tensor(4.9736)\n",
      "Test set loss: 33.4667 Average Mismatches: 3.7000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1307.1639, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(11.6135, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(990.0776, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.5477, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1026.9045, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(9.2915, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1090.6155, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(13.3046, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1245.0448, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(11.1329, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(928.1486, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(10.2185, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(959.9124, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(12.6835, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1237.1396, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(14.0585, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(987.3600, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(15.2066, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1289.2537, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(15.8690, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.0719, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1287, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(33.4769, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0241, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(132.1768, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 10. Average loss: 11.1845. Time: 21.00 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.2006170749664307. 3 mutations elbo prob: 33.501014709472656. 10 mutations elbo prob: 132.23916625976562.\n",
      "CE Loss:  tensor(3417.1846) KLD Loss: tensor(6.0098)\n",
      "CE Loss:  tensor(3400.8308) KLD Loss: tensor(5.3784)\n",
      "CE Loss:  tensor(3327.3955) KLD Loss: tensor(6.4332)\n",
      "CE Loss:  tensor(3330.6611) KLD Loss: tensor(5.9544)\n",
      "CE Loss:  tensor(3065.8230) KLD Loss: tensor(5.5095)\n",
      "CE Loss:  tensor(3256.6992) KLD Loss: tensor(5.8595)\n",
      "CE Loss:  tensor(3740.0029) KLD Loss: tensor(5.4588)\n",
      "CE Loss:  tensor(3455.9077) KLD Loss: tensor(6.0828)\n",
      "CE Loss:  tensor(3466.0144) KLD Loss: tensor(5.0174)\n",
      "CE Loss:  tensor(3365.0127) KLD Loss: tensor(5.9946)\n",
      "Test set loss: 33.8832 Average Mismatches: 3.8000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "finished saving model\n",
      "CE Loss:  tensor(1136.9341, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(15.9557, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(848.0169, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(11.3259, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1174.3473, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(14.5732, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1259.6570, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(14.5666, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1291.0386, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(16.1089, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1119.3652, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(15.5702, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1040.6210, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(15.5987, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(787.3721, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(14.4797, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1114.9645, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(18.2994, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1107.5728, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(18.7922, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.1169, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1082, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(25.9275, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(127.9034, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0457, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 11. Average loss: 11.0352. Time: 23.18 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.225128412246704. 3 mutations elbo prob: 25.941482543945312. 10 mutations elbo prob: 127.94911193847656.\n",
      "CE Loss:  tensor(3215.8899) KLD Loss: tensor(6.0612)\n",
      "CE Loss:  tensor(3357.2302) KLD Loss: tensor(6.0500)\n",
      "CE Loss:  tensor(3295.7043) KLD Loss: tensor(8.3569)\n",
      "CE Loss:  tensor(3285.9089) KLD Loss: tensor(7.6236)\n",
      "CE Loss:  tensor(3338.4514) KLD Loss: tensor(6.6852)\n",
      "CE Loss:  tensor(3629.9492) KLD Loss: tensor(7.6403)\n",
      "CE Loss:  tensor(3398.6812) KLD Loss: tensor(8.7953)\n",
      "CE Loss:  tensor(3450.0217) KLD Loss: tensor(7.2574)\n",
      "CE Loss:  tensor(3229.4370) KLD Loss: tensor(5.8267)\n",
      "CE Loss:  tensor(3302.9404) KLD Loss: tensor(6.6161)\n",
      "Test set loss: 33.5751 Average Mismatches: 4.0000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1120.1143, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(18.9518, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(935.8242, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(13.3708, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(973.4820, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(14.8696, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1023.9728, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(16.2705, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1220.2952, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(23.0631, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1009.0663, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(17.9258, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(958.8597, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(17.3235, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1231.4066, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(23.7601, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(993.1426, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(19.0880, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1224.9208, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(24.1567, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(2.2064, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0894, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(38.0152, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(99.6213, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 12. Average loss: 10.8799. Time: 25.20 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 2.295766830444336. 3 mutations elbo prob: 38.026004791259766. 10 mutations elbo prob: 99.64911651611328.\n",
      "CE Loss:  tensor(3530.0842) KLD Loss: tensor(10.6854)\n",
      "CE Loss:  tensor(3148.1985) KLD Loss: tensor(8.2032)\n",
      "CE Loss:  tensor(3293.6726) KLD Loss: tensor(9.1775)\n",
      "CE Loss:  tensor(3519.9189) KLD Loss: tensor(8.8909)\n",
      "CE Loss:  tensor(3403.1377) KLD Loss: tensor(11.3070)\n",
      "CE Loss:  tensor(3146.7197) KLD Loss: tensor(9.4860)\n",
      "CE Loss:  tensor(3550.1572) KLD Loss: tensor(9.7404)\n",
      "CE Loss:  tensor(3182.7168) KLD Loss: tensor(8.1001)\n",
      "CE Loss:  tensor(3292.3611) KLD Loss: tensor(9.1071)\n",
      "CE Loss:  tensor(3426.2246) KLD Loss: tensor(8.4166)\n",
      "Test set loss: 33.5863 Average Mismatches: 3.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1014.9767, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(17.3946, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(987.9441, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(20.0859, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1106.2653, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(20.6180, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1269.2764, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(29.9202, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1036.5808, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.7219, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(938.3281, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.2303, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1057.2119, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.8996, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1056.8259, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.5623, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1055.9728, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(22.7493, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1079.1738, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(29.7657, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(2.1623, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1282, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(29.4031, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(114.4341, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0479, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 13. Average loss: 10.8475. Time: 27.32 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 2.290515661239624. 3 mutations elbo prob: 29.41982078552246. 10 mutations elbo prob: 114.48196411132812.\n",
      "CE Loss:  tensor(3285.3076) KLD Loss: tensor(11.8587)\n",
      "CE Loss:  tensor(3523.7385) KLD Loss: tensor(16.0751)\n",
      "CE Loss:  tensor(3291.7998) KLD Loss: tensor(10.8219)\n",
      "CE Loss:  tensor(3397.6794) KLD Loss: tensor(9.7146)\n",
      "CE Loss:  tensor(3191.7766) KLD Loss: tensor(12.9772)\n",
      "CE Loss:  tensor(3623.7351) KLD Loss: tensor(11.2024)\n",
      "CE Loss:  tensor(3441.9146) KLD Loss: tensor(10.5181)\n",
      "CE Loss:  tensor(3405.5283) KLD Loss: tensor(12.5051)\n",
      "CE Loss:  tensor(3206.7961) KLD Loss: tensor(8.7676)\n",
      "CE Loss:  tensor(3408.4419) KLD Loss: tensor(13.0104)\n",
      "Test set loss: 33.8942 Average Mismatches: 4.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1053.2979, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.0869, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1083.4446, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.9233, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1085.1060, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(30.0040, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1106.7931, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(28.3973, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1109.0270, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.5992, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1163.0350, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(33.7283, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(987.3355, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(27.9133, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1005.9849, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(32.3309, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(939.6276, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(23.5019, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(1015.8180, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.4645, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.1647, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2784, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(35.9660, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0323, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(122.1385, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1180, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 14. Average loss: 10.8404. Time: 29.50 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.44304749369621277. 3 mutations elbo prob: 35.99825668334961. 10 mutations elbo prob: 122.25648498535156.\n",
      "CE Loss:  tensor(3382.8494) KLD Loss: tensor(12.3325)\n",
      "CE Loss:  tensor(3592.1135) KLD Loss: tensor(12.3106)\n",
      "CE Loss:  tensor(3402.3599) KLD Loss: tensor(11.8896)\n",
      "CE Loss:  tensor(3501.4294) KLD Loss: tensor(10.3093)\n",
      "CE Loss:  tensor(3584.0007) KLD Loss: tensor(11.0763)\n",
      "CE Loss:  tensor(3389.1887) KLD Loss: tensor(14.6238)\n",
      "CE Loss:  tensor(3360.8167) KLD Loss: tensor(11.3895)\n",
      "CE Loss:  tensor(3418.1069) KLD Loss: tensor(13.9527)\n",
      "CE Loss:  tensor(3538.6221) KLD Loss: tensor(12.7870)\n",
      "CE Loss:  tensor(3375.2244) KLD Loss: tensor(10.8427)\n",
      "Test set loss: 34.6662 Average Mismatches: 3.3000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1020.7386, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.3814, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1164.2749, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.6544, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1040.1221, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(29.9469, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(903.9684, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.6876, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(928.3304, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.7495, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1038.6859, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(33.9918, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1220.5526, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.2184, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1064.3944, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.7666, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(964.6183, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(28.3204, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1185.5988, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.6905, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.0652, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0981, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(33.9229, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0257, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(142.7242, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0270, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 15. Average loss: 10.8537. Time: 31.63 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.1632810831069946. 3 mutations elbo prob: 33.94862747192383. 10 mutations elbo prob: 142.7511444091797.\n",
      "CE Loss:  tensor(3303.8162) KLD Loss: tensor(17.1859)\n",
      "CE Loss:  tensor(3474.0662) KLD Loss: tensor(20.3061)\n",
      "CE Loss:  tensor(3206.8359) KLD Loss: tensor(21.3225)\n",
      "CE Loss:  tensor(3466.6929) KLD Loss: tensor(14.4472)\n",
      "CE Loss:  tensor(3477.8682) KLD Loss: tensor(24.1095)\n",
      "CE Loss:  tensor(3444.1311) KLD Loss: tensor(15.4104)\n",
      "CE Loss:  tensor(3280.8948) KLD Loss: tensor(15.6703)\n",
      "CE Loss:  tensor(3293.1191) KLD Loss: tensor(17.1824)\n",
      "CE Loss:  tensor(3206.9475) KLD Loss: tensor(12.7078)\n",
      "CE Loss:  tensor(3226.5786) KLD Loss: tensor(17.1969)\n",
      "Test set loss: 33.5565 Average Mismatches: 3.3000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1102.0380, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(36.9241, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(941.4182, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.0986, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(918.9424, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(30.0668, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(983.5225, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.6183, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1024.7870, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(37.0628, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1069.8912, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(39.1017, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1122.7419, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(36.0045, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1059.2432, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.9053, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1085.6368, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(37.0157, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1053.0643, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.7340, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.8389, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2475, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(29.1537, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0158, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(118.4137, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0876, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 16. Average loss: 10.7318. Time: 33.67 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 2.0863771438598633. 3 mutations elbo prob: 29.169540405273438. 10 mutations elbo prob: 118.50135040283203.\n",
      "CE Loss:  tensor(3463.0657) KLD Loss: tensor(12.5172)\n",
      "CE Loss:  tensor(3315.6035) KLD Loss: tensor(12.5914)\n",
      "CE Loss:  tensor(3637.1001) KLD Loss: tensor(16.2117)\n",
      "CE Loss:  tensor(3394.0681) KLD Loss: tensor(13.2938)\n",
      "CE Loss:  tensor(3431.5583) KLD Loss: tensor(14.7089)\n",
      "CE Loss:  tensor(3495.3420) KLD Loss: tensor(14.8918)\n",
      "CE Loss:  tensor(3399.4116) KLD Loss: tensor(16.5393)\n",
      "CE Loss:  tensor(3511.3640) KLD Loss: tensor(20.1556)\n",
      "CE Loss:  tensor(3495.8999) KLD Loss: tensor(13.0341)\n",
      "CE Loss:  tensor(3314.9304) KLD Loss: tensor(15.4388)\n",
      "Test set loss: 34.6077 Average Mismatches: 3.8000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1069.5729, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.8102, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1085.0281, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.9115, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(971.3042, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(33.0104, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(981.3372, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.1395, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1148.3274, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.2579, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1075.9757, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(39.3985, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(827.8130, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(31.7302, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(973.6883, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.9868, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1031.8265, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(38.9222, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1116.8496, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(43.6876, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.7062, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1441, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(33.9049, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(141.9205, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0415, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 17. Average loss: 10.6806. Time: 35.72 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.8502558469772339. 3 mutations elbo prob: 33.927703857421875. 10 mutations elbo prob: 141.9619903564453.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(3412.9810) KLD Loss: tensor(19.3399)\n",
      "CE Loss:  tensor(3457.3232) KLD Loss: tensor(17.4423)\n",
      "CE Loss:  tensor(3218.9622) KLD Loss: tensor(17.2816)\n",
      "CE Loss:  tensor(3431.3115) KLD Loss: tensor(14.8776)\n",
      "CE Loss:  tensor(3497.7200) KLD Loss: tensor(21.2071)\n",
      "CE Loss:  tensor(3790.0085) KLD Loss: tensor(24.2632)\n",
      "CE Loss:  tensor(3305.0850) KLD Loss: tensor(20.3468)\n",
      "CE Loss:  tensor(3290.6389) KLD Loss: tensor(16.3906)\n",
      "CE Loss:  tensor(3345.3896) KLD Loss: tensor(19.7977)\n",
      "CE Loss:  tensor(3258.0271) KLD Loss: tensor(18.1188)\n",
      "Test set loss: 34.1965 Average Mismatches: 4.2000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(929.3461, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(37.5514, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1160.0901, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(60.4771, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1070.9249, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.0363, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(967.1550, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.8668, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1118.4109, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(46.2862, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(989.0643, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(38.3230, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(811.4590, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(33.2868, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1020.1577, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(33.5169, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1273.2477, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(46.0555, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(970.8176, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(39.7238, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.5850, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(26.8806, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0353, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(127.7359, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 18. Average loss: 10.7238. Time: 37.88 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.6762585639953613. 3 mutations elbo prob: 26.915924072265625. 10 mutations elbo prob: 127.75909423828125.\n",
      "CE Loss:  tensor(3188.2812) KLD Loss: tensor(18.6987)\n",
      "CE Loss:  tensor(3354.6938) KLD Loss: tensor(19.6631)\n",
      "CE Loss:  tensor(3232.5876) KLD Loss: tensor(19.4486)\n",
      "CE Loss:  tensor(3319.4263) KLD Loss: tensor(21.1874)\n",
      "CE Loss:  tensor(3566.1648) KLD Loss: tensor(21.1463)\n",
      "CE Loss:  tensor(3366.2124) KLD Loss: tensor(22.3431)\n",
      "CE Loss:  tensor(3487.9661) KLD Loss: tensor(23.2697)\n",
      "CE Loss:  tensor(3306.6670) KLD Loss: tensor(25.9567)\n",
      "CE Loss:  tensor(3413.3591) KLD Loss: tensor(20.7932)\n",
      "CE Loss:  tensor(3674.7131) KLD Loss: tensor(28.2102)\n",
      "Test set loss: 34.1308 Average Mismatches: 2.4000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(959.4296, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.0719, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1163.2924, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(46.8862, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(913.5992, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(33.1949, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(994.5876, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.1873, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(857.9492, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.0261, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(956.5793, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(46.1778, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1158.2911, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(48.0798, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(972.5449, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.5178, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1121.0762, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.0433, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1185.1443, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.8056, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.6850, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2255, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(25.6388, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(133.2551, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 19. Average loss: 10.6895. Time: 40.00 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.9105674624443054. 3 mutations elbo prob: 25.652599334716797. 10 mutations elbo prob: 133.3267364501953.\n",
      "CE Loss:  tensor(3686.4890) KLD Loss: tensor(20.2240)\n",
      "CE Loss:  tensor(3521.9355) KLD Loss: tensor(16.8259)\n",
      "CE Loss:  tensor(3660.0981) KLD Loss: tensor(18.3835)\n",
      "CE Loss:  tensor(3359.9939) KLD Loss: tensor(13.9968)\n",
      "CE Loss:  tensor(3584.1521) KLD Loss: tensor(20.5868)\n",
      "CE Loss:  tensor(3380.2717) KLD Loss: tensor(15.4838)\n",
      "CE Loss:  tensor(3345.2039) KLD Loss: tensor(15.0517)\n",
      "CE Loss:  tensor(3511.9248) KLD Loss: tensor(17.3105)\n",
      "CE Loss:  tensor(3418.0369) KLD Loss: tensor(14.4160)\n",
      "CE Loss:  tensor(3440.8806) KLD Loss: tensor(14.2381)\n",
      "Test set loss: 35.0755 Average Mismatches: 4.3000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1014.1039, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.5060, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(962.3481, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.9012, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(822.9470, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.5690, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1022.0255, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.2456, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1341.1523, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(61.2133, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1157.3907, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.4787, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(983.0188, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.6654, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(956.6533, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.8212, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1009.3987, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(43.2818, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(981.9068, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(34.3774, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(2.5498, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(32.0173, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(122.3530, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 20. Average loss: 10.6700. Time: 42.11 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 2.731407880783081. 3 mutations elbo prob: 32.04047775268555. 10 mutations elbo prob: 122.41371154785156.\n",
      "CE Loss:  tensor(3637.3582) KLD Loss: tensor(25.2788)\n",
      "CE Loss:  tensor(3301.8748) KLD Loss: tensor(15.4496)\n",
      "CE Loss:  tensor(3456.1855) KLD Loss: tensor(19.9467)\n",
      "CE Loss:  tensor(3315.5046) KLD Loss: tensor(19.6941)\n",
      "CE Loss:  tensor(3451.5518) KLD Loss: tensor(20.3448)\n",
      "CE Loss:  tensor(3472.6289) KLD Loss: tensor(15.7185)\n",
      "CE Loss:  tensor(3233.0178) KLD Loss: tensor(20.1731)\n",
      "CE Loss:  tensor(3739.3020) KLD Loss: tensor(22.2507)\n",
      "CE Loss:  tensor(3204.1782) KLD Loss: tensor(18.8770)\n",
      "CE Loss:  tensor(3576.6064) KLD Loss: tensor(19.7885)\n",
      "Test set loss: 34.5857 Average Mismatches: 3.5000 Wild Type Mismatches 0.1000 <====> \n",
      "\n",
      "finished saving model\n",
      "CE Loss:  tensor(959.1802, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.5464, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(1036.5663, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.9427, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(980.1086, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.2920, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(990.2826, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.1154, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(913.3991, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(43.6187, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1132.4857, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(55.1684, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1107.5911, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(52.1329, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1078.9850, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(49.7893, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(970.3303, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(39.1997, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1038.3135, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.8367, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.3894, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2335, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(26.2241, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0219, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(101.9800, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0858, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 21. Average loss: 10.6599. Time: 44.40 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.6229934692382812. 3 mutations elbo prob: 26.245967864990234. 10 mutations elbo prob: 102.06576538085938.\n",
      "CE Loss:  tensor(3578.4209) KLD Loss: tensor(21.2285)\n",
      "CE Loss:  tensor(3720.5303) KLD Loss: tensor(29.8937)\n",
      "CE Loss:  tensor(3743.5967) KLD Loss: tensor(17.8688)\n",
      "CE Loss:  tensor(3502.4585) KLD Loss: tensor(24.6096)\n",
      "CE Loss:  tensor(3322.9224) KLD Loss: tensor(16.1140)\n",
      "CE Loss:  tensor(3519.6577) KLD Loss: tensor(14.9897)\n",
      "CE Loss:  tensor(3627.9800) KLD Loss: tensor(23.8232)\n",
      "CE Loss:  tensor(3320.0393) KLD Loss: tensor(12.6410)\n",
      "CE Loss:  tensor(3356.9966) KLD Loss: tensor(17.2202)\n",
      "CE Loss:  tensor(3401.7061) KLD Loss: tensor(21.3701)\n",
      "Test set loss: 35.2941 Average Mismatches: 3.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(978.6904, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.1541, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(998.5397, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(49.9088, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(996.3586, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.3288, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1202.6085, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(56.6901, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1003.0435, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.9036, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(919.6274, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(52.4658, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(942.5146, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.6172, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(984.9008, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(43.9725, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(971.5752, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(51.5091, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1085.8986, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(58.6316, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.0047, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2862, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(29.1889, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(115.3717, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1127, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 22. Average loss: 10.5699. Time: 46.62 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.2909371852874756. 3 mutations elbo prob: 29.20502471923828. 10 mutations elbo prob: 115.48441314697266.\n",
      "CE Loss:  tensor(3482.2417) KLD Loss: tensor(20.6411)\n",
      "CE Loss:  tensor(3702.5571) KLD Loss: tensor(25.0688)\n",
      "CE Loss:  tensor(3428.7913) KLD Loss: tensor(16.5291)\n",
      "CE Loss:  tensor(3581.6135) KLD Loss: tensor(21.6151)\n",
      "CE Loss:  tensor(3634.0850) KLD Loss: tensor(18.4545)\n",
      "CE Loss:  tensor(3566.5095) KLD Loss: tensor(19.6309)\n",
      "CE Loss:  tensor(3447.4680) KLD Loss: tensor(17.3970)\n",
      "CE Loss:  tensor(3571.9888) KLD Loss: tensor(19.6037)\n",
      "CE Loss:  tensor(3397.6277) KLD Loss: tensor(21.0374)\n",
      "CE Loss:  tensor(3561.1807) KLD Loss: tensor(21.9554)\n",
      "Test set loss: 35.5760 Average Mismatches: 2.8000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1103.8794, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(58.6965, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1006.6555, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(53.8636, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1021.5992, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(59.3573, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(955.8000, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.1917, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1047.2256, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.4915, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1047.2909, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(50.2066, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1096.2499, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(49.9595, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(912.2308, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.2937, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1016.0602, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(49.3749, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(875.1225, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.5099, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.7035, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2611, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(36.0339, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(147.6020, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0972, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 23. Average loss: 10.5811. Time: 48.48 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.9646117687225342. 3 mutations elbo prob: 36.056095123291016. 10 mutations elbo prob: 147.69918823242188.\n",
      "CE Loss:  tensor(3671.9329) KLD Loss: tensor(24.2042)\n",
      "CE Loss:  tensor(3564.7854) KLD Loss: tensor(20.9250)\n",
      "CE Loss:  tensor(3515.2319) KLD Loss: tensor(20.2768)\n",
      "CE Loss:  tensor(3533.8259) KLD Loss: tensor(26.3164)\n",
      "CE Loss:  tensor(3344.8652) KLD Loss: tensor(20.3612)\n",
      "CE Loss:  tensor(3464.3787) KLD Loss: tensor(17.0685)\n",
      "CE Loss:  tensor(3547.0449) KLD Loss: tensor(19.5572)\n",
      "CE Loss:  tensor(3565.1985) KLD Loss: tensor(21.1506)\n",
      "CE Loss:  tensor(3711.6990) KLD Loss: tensor(19.8372)\n",
      "CE Loss:  tensor(3493.9590) KLD Loss: tensor(19.3105)\n",
      "Test set loss: 35.6219 Average Mismatches: 3.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(974.6025, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(46.7158, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(962.1360, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(52.8596, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(893.6837, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.7768, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1034.0385, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(58.6748, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1008.2679, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(51.5429, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1081.8058, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(53.0757, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1174.4576, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(52.0664, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1055.8856, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(59.9813, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1070.7498, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.3589, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(935.9495, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.2306, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1.0552, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(31.0368, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0873, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(97.0825, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0230, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 24. Average loss: 10.7059. Time: 50.33 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 1.1211352348327637. 3 mutations elbo prob: 31.124027252197266. 10 mutations elbo prob: 97.10542297363281.\n",
      "CE Loss:  tensor(3412.4680) KLD Loss: tensor(38.8125)\n",
      "CE Loss:  tensor(3185.5649) KLD Loss: tensor(25.8632)\n",
      "CE Loss:  tensor(3514.5237) KLD Loss: tensor(41.1907)\n",
      "CE Loss:  tensor(3181.7712) KLD Loss: tensor(22.2240)\n",
      "CE Loss:  tensor(3483.5227) KLD Loss: tensor(36.8829)\n",
      "CE Loss:  tensor(3412.9131) KLD Loss: tensor(37.5394)\n",
      "CE Loss:  tensor(3554.5198) KLD Loss: tensor(33.4040)\n",
      "CE Loss:  tensor(3533.2280) KLD Loss: tensor(27.7466)\n",
      "CE Loss:  tensor(3407.3293) KLD Loss: tensor(45.6948)\n",
      "CE Loss:  tensor(3464.0444) KLD Loss: tensor(45.7585)\n",
      "Test set loss: 34.5050 Average Mismatches: 3.4000 Wild Type Mismatches 0.1000 <====> \n",
      "\n",
      "CE Loss:  tensor(992.9388, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(66.0985, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1136.2194, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(52.9984, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(933.6600, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(46.9354, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(862.2678, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(49.1486, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(944.5197, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(55.6862, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1148.7574, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(61.2011, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(980.9020, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(51.9540, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(909.1588, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(48.6434, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1131.9954, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(62.9522, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1037.1444, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(51.6012, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.6590, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1578, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(29.2179, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0378, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(97.0180, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0418, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 25. Average loss: 10.6248. Time: 52.20 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.1\n",
      "wild type elbo prob: 0.8168694376945496. 3 mutations elbo prob: 29.25571060180664. 10 mutations elbo prob: 97.05979919433594.\n",
      "CE Loss:  tensor(3451.5444) KLD Loss: tensor(25.7339)\n",
      "CE Loss:  tensor(3596.5986) KLD Loss: tensor(22.3025)\n",
      "CE Loss:  tensor(3472.2627) KLD Loss: tensor(25.7874)\n",
      "CE Loss:  tensor(3439.4248) KLD Loss: tensor(21.8604)\n",
      "CE Loss:  tensor(3696.7544) KLD Loss: tensor(35.2279)\n",
      "CE Loss:  tensor(3440.2888) KLD Loss: tensor(23.7073)\n",
      "CE Loss:  tensor(3645.1357) KLD Loss: tensor(27.2068)\n",
      "CE Loss:  tensor(3466.6411) KLD Loss: tensor(30.4045)\n",
      "CE Loss:  tensor(3458.0234) KLD Loss: tensor(27.4643)\n",
      "CE Loss:  tensor(3457.3813) KLD Loss: tensor(31.4933)\n",
      "Test set loss: 35.3952 Average Mismatches: 3.8000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(851.1468, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(38.4224, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(907.6791, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(59.8045, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1071.1324, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(72.7296, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1026.4069, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(61.2259, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1104.3060, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(50.3063, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1143.8693, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(54.1355, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(908.4714, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.1159, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(890.0624, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(38.1998, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(916.4227, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.7871, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1200.6365, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(56.4672, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.4010, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(28.9830, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0206, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(140.9002, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0367, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 26. Average loss: 10.5413. Time: 54.06 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.5754139423370361. 3 mutations elbo prob: 29.003660202026367. 10 mutations elbo prob: 140.93692016601562.\n",
      "CE Loss:  tensor(3626.8323) KLD Loss: tensor(29.8626)\n",
      "CE Loss:  tensor(3337.7173) KLD Loss: tensor(20.1447)\n",
      "CE Loss:  tensor(3611.8120) KLD Loss: tensor(31.9510)\n",
      "CE Loss:  tensor(3576.6511) KLD Loss: tensor(19.3622)\n",
      "CE Loss:  tensor(3495.0698) KLD Loss: tensor(24.0877)\n",
      "CE Loss:  tensor(3501.1516) KLD Loss: tensor(24.2098)\n",
      "CE Loss:  tensor(3792.9119) KLD Loss: tensor(27.9854)\n",
      "CE Loss:  tensor(3613.4993) KLD Loss: tensor(27.6935)\n",
      "CE Loss:  tensor(3548.0317) KLD Loss: tensor(16.8825)\n",
      "CE Loss:  tensor(3298.2632) KLD Loss: tensor(19.1788)\n",
      "Test set loss: 35.6433 Average Mismatches: 4.1000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(979.5576, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(48.2876, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1197.5741, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(65.1287, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(853.7042, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(39.4997, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(964.5165, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(49.7526, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(959.2423, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(39.2663, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1061.3749, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(63.3234, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1229.0459, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(64.3775, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(965.2360, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.0639, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(977.0401, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(53.8002, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(891.3743, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(53.3142, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.3744, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2858, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(26.0932, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0307, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(145.8002, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0954, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 27. Average loss: 10.6005. Time: 55.91 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.6602272987365723. 3 mutations elbo prob: 26.123855590820312. 10 mutations elbo prob: 145.89566040039062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss:  tensor(3532.8494) KLD Loss: tensor(22.7411)\n",
      "CE Loss:  tensor(3781.8276) KLD Loss: tensor(21.7161)\n",
      "CE Loss:  tensor(3623.4026) KLD Loss: tensor(25.7995)\n",
      "CE Loss:  tensor(3541.6228) KLD Loss: tensor(17.9182)\n",
      "CE Loss:  tensor(3503.0994) KLD Loss: tensor(20.5808)\n",
      "CE Loss:  tensor(3635.7800) KLD Loss: tensor(25.0617)\n",
      "CE Loss:  tensor(3626.8860) KLD Loss: tensor(17.1663)\n",
      "CE Loss:  tensor(3476.5835) KLD Loss: tensor(28.9473)\n",
      "CE Loss:  tensor(3613.8628) KLD Loss: tensor(21.7993)\n",
      "CE Loss:  tensor(3328.4209) KLD Loss: tensor(17.9535)\n",
      "Test set loss: 35.8840 Average Mismatches: 4.5000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1009.4734, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(60.2298, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1077.0891, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(54.9362, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1056.4006, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(65.6846, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(720.7896, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.9822, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(924.3909, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(60.2212, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1055.2943, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(59.0459, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1105.7329, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(58.9608, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(865.6542, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.5982, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1107.3593, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(40.3998, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1043.6862, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(48.3972, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.7958, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(27.6143, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0401, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(136.8367, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0298, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 28. Average loss: 10.5003. Time: 57.76 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.9202265739440918. 3 mutations elbo prob: 27.654420852661133. 10 mutations elbo prob: 136.86643981933594.\n",
      "CE Loss:  tensor(3630.6699) KLD Loss: tensor(24.1698)\n",
      "CE Loss:  tensor(3532.0913) KLD Loss: tensor(25.2546)\n",
      "CE Loss:  tensor(3603.0005) KLD Loss: tensor(31.0224)\n",
      "CE Loss:  tensor(3547.9775) KLD Loss: tensor(32.4349)\n",
      "CE Loss:  tensor(3330.8704) KLD Loss: tensor(24.4994)\n",
      "CE Loss:  tensor(3612.7102) KLD Loss: tensor(24.5510)\n",
      "CE Loss:  tensor(3417.7703) KLD Loss: tensor(21.7855)\n",
      "CE Loss:  tensor(3466.9846) KLD Loss: tensor(24.4040)\n",
      "CE Loss:  tensor(3454.9451) KLD Loss: tensor(28.5364)\n",
      "CE Loss:  tensor(3586.2539) KLD Loss: tensor(39.0189)\n",
      "Test set loss: 35.4590 Average Mismatches: 4.6000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1105.1613, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(57.0827, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(970.2222, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(57.5708, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1148.4257, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(65.7237, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1036.6290, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(65.3000, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(928.7986, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(45.7696, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1118.5278, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(53.8878, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(812.1905, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(42.1523, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1019.4599, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(48.2731, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(980.5354, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(57.0526, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(854.7479, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(35.4344, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.3322, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2585, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(27.3914, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0260, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(139.4868, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0814, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 29. Average loss: 10.5029. Time: 59.63 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.5906897187232971. 3 mutations elbo prob: 27.41739273071289. 10 mutations elbo prob: 139.56813049316406.\n",
      "CE Loss:  tensor(3592.7373) KLD Loss: tensor(20.7545)\n",
      "CE Loss:  tensor(3551.9158) KLD Loss: tensor(20.1600)\n",
      "CE Loss:  tensor(3790.3455) KLD Loss: tensor(22.6966)\n",
      "CE Loss:  tensor(3564.9832) KLD Loss: tensor(24.4697)\n",
      "CE Loss:  tensor(3364.6338) KLD Loss: tensor(14.9157)\n",
      "CE Loss:  tensor(3560.1865) KLD Loss: tensor(22.1673)\n",
      "CE Loss:  tensor(3423.7256) KLD Loss: tensor(22.5175)\n",
      "CE Loss:  tensor(3799.9395) KLD Loss: tensor(20.8507)\n",
      "CE Loss:  tensor(3794.7009) KLD Loss: tensor(22.8049)\n",
      "CE Loss:  tensor(3481.8088) KLD Loss: tensor(20.7498)\n",
      "Test set loss: 36.1371 Average Mismatches: 3.2000 Wild Type Mismatches 0.0000 <====> \n",
      "\n",
      "CE Loss:  tensor(1042.0491, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(54.4860, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(956.6428, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(54.8224, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1002.6639, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.3018, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(895.1573, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(47.5311, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(812.4390, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(51.8393, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1037.6289, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(48.3736, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(804.1173, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(41.2937, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1206.8861, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(82.6168, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1159.4867, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(62.9036, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(1091.9553, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(44.2472, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(0.5340, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.2299, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(28.4754, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0302, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(133.0601, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "<====> Epoch: 30. Average loss: 10.5444. Time: 61.51 seconds\n",
      "Sample generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK*\n",
      "Average mismatches from the wild type: 0.0\n",
      "wild type elbo prob: 0.7638500332832336. 3 mutations elbo prob: 28.505542755126953. 10 mutations elbo prob: 133.12753295898438.\n",
      "CE Loss:  tensor(3313.9690) KLD Loss: tensor(20.0102)\n",
      "CE Loss:  tensor(3529.7532) KLD Loss: tensor(26.6083)\n",
      "CE Loss:  tensor(3508.9370) KLD Loss: tensor(23.7226)\n",
      "CE Loss:  tensor(3657.3826) KLD Loss: tensor(23.2241)\n",
      "CE Loss:  tensor(3548.9355) KLD Loss: tensor(23.4825)\n",
      "CE Loss:  tensor(3739.6638) KLD Loss: tensor(24.1339)\n",
      "CE Loss:  tensor(3513.5427) KLD Loss: tensor(20.3643)\n",
      "CE Loss:  tensor(3779.4099) KLD Loss: tensor(23.9830)\n",
      "CE Loss:  tensor(3567.4387) KLD Loss: tensor(19.2445)\n",
      "CE Loss:  tensor(3746.9578) KLD Loss: tensor(26.4201)\n",
      "Test set loss: 36.1372 Average Mismatches: 3.4000 Wild Type Mismatches 0.1000 <====> \n",
      "\n",
      "finished saving model\n"
     ]
    }
   ],
   "source": [
    "vae = GenerativeVAE(args)\n",
    "logger = None\n",
    "vae.fit(train_loader, test_loader, True, logger, \"./models/{0}/\".format(vae.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=4998, out_features=400, bias=True)\n",
      "  (fc21): Linear(in_features=400, out_features=20, bias=True)\n",
      "  (fc22): Linear(in_features=400, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=400, bias=True)\n",
      "  (fc4): Linear(in_features=400, out_features=4998, bias=True)\n",
      ")\n",
      "CE Loss:  tensor(3500.4109, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.4712, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3500.4109, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.4712, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3669.8552, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(23.8308, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3669.8552, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(23.8308, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3391.0305, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(17.7572, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3391.0305, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(17.7572, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3581.1194, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.8866, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3581.1194, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.8866, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3700.0635, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.5897, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3700.0635, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.5897, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3665.0317, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.4122, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3665.0317, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(25.4122, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3581.6299, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.5606, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3581.6299, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.5606, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3588.8730, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.3581, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3588.8730, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(21.3581, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3609.4851, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.8429, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3609.4851, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(26.8429, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3488.0886, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(20.4840, grad_fn=<MulBackward0>)\n",
      "CE Loss:  tensor(3488.0886, grad_fn=<NllLoss2DBackward>) KLD Loss: tensor(20.4840, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vae.plot_model(\"./logs/vae/{0}_model_architecture\".format(vae.name))\n",
    "vae.plot_history(\"./logs/vae/{0}_training_history\".format(vae.name))\n",
    "vae.show_model(None)\n",
    "load_vae = GenerativeVAE(args)\n",
    "load_vae.load_model(\"./models/{0}/checkpoint_30.pt\".format(vae.name))\n",
    "for parameter_name, load_weights in load_vae.model.state_dict().items():\n",
    "    vae_weights = vae.model.state_dict()[parameter_name]\n",
    "    assert(torch.all(torch.eq(load_weights, vae_weights)).item())\n",
    "\n",
    "for (x, _) in test_loader:         \n",
    "    x = x.to(load_vae.device)\n",
    "    z, z_mean, z_var = load_vae.encoder(x, reparameterize=True)\n",
    "    z_mean_2, z_var_2 = vae.encoder(x)\n",
    "    assert(torch.all(torch.eq(z_mean, z_mean_2)).item())\n",
    "    assert(torch.all(torch.eq(z_var, z_var_2)).item())\n",
    "    recon_x = load_vae.decoder(z)\n",
    "    recon_x_2 = vae.decoder(z)\n",
    "    loss_1 = vae.elbo_loss(recon_x, x, z_mean, z_var).item()\n",
    "    loss_2 = vae.elbo_loss(recon_x_2, x, z_mean, z_var).item()\n",
    "    np.testing.assert_equal(loss_1, loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logger:\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(784, 400, 20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'logs/vae/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "\"\"\"\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 2\n",
    "        self.no_cuda = True\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        \n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_index(string, alphabet):\n",
    "    return np.array([alphabet.index(s) for s in string])\n",
    "\n",
    "wild_type_index = string_to_index(get_wild_type_amino_acid_sequence(), alphabet = get_all_amino_acids())\n",
    "wild_type_index_tensor = torch.from_numpy(wild_type_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0822, 0.0137, 0.0320, 0.0091, 0.0639, 0.0776, 0.0046, 0.0502, 0.0776,\n",
      "         0.0913, 0.0594, 0.0137, 0.0000, 0.0868, 0.0228, 0.0320, 0.0411, 0.0548,\n",
      "         0.0411, 0.0776, 0.0685]], dtype=torch.float64) tensor([1.0000, 1.0000], dtype=torch.float64)\n",
      "tensor([1.0000, 0.9087], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "normalized_prob = np.random.randint(0, 21, 21)\n",
    "normalized_prob = normalized_prob / normalized_prob.sum()\n",
    "x = torch.tensor([[0] * 15 + [1] + [0] * 5, normalized_prob])\n",
    "wild_type_probs = []\n",
    "for probs, index in zip(x, wild_type_index):\n",
    "    wild_type_probs.append(probs[index])\n",
    "\n",
    "sums = x.sum(dim = 1)\n",
    "print(x, sums)\n",
    "sums = sums - torch.tensor(wild_type_probs)\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(2, 3, 4, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4653, -1.0134,  0.0671, -2.0208, -0.0811],\n",
      "        [-0.5681,  1.4572,  1.2459, -0.1435,  0.7575],\n",
      "        [ 0.2731, -2.1939,  0.1123, -0.6824,  0.4075]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 5)\n",
    "print(x)\n",
    "x.argmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 210])\n",
      "torch.Size([2, 10])\n",
      "tensor([-1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08,  1.0000e+00, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08], dtype=torch.float64)\n",
      "torch.Size([2, 21, 10])\n",
      "tensor([-1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08,  1.0000e+00, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08], dtype=torch.float64)\n",
      "tensor([ 1.0000e+00, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08,\n",
      "        -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08, -1.0000e+08],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-69d7a6fefe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_amino_acids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_amino_acids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             ret = torch._C._nn.nll_loss2d(\n\u001b[0;32m-> 1806\u001b[0;31m                 input, target, weight, reduction_enum, ignore_index)\n\u001b[0m\u001b[1;32m   1807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m             out = torch._C._nn.nll_loss2d(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "length = 10\n",
    "wild_type = get_wild_type_amino_acid_sequence()\n",
    "one_hot = one_hot_encode([wild_type[0:length], wild_type[0:length]], get_all_amino_acids())\n",
    "for i in range(one_hot.shape[0]): \n",
    "    for j in range(one_hot.shape[1]): \n",
    "        if not one_hot[i, j]:\n",
    "            one_hot[i, j] = eps\n",
    "        else:\n",
    "            one_hot[i, j] = 1\n",
    "            \n",
    "one_hot_tensor = torch.from_numpy(one_hot)\n",
    "print(one_hot_tensor.shape)\n",
    "labels = one_hot_tensor.view(2, length, len(get_all_amino_acids())).argmax(dim = 2).float()\n",
    "print(labels.shape)\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids()))[0][0])\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1).shape)\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1)[0, :, 0])\n",
    "print(one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1)[0, 16])\n",
    "x = one_hot_tensor.view(2, length, len(get_all_amino_acids())).permute(0, 2, 1)\n",
    "z = nn.CrossEntropyLoss(reduction='sum')(x, labels).item()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = -1e8\n",
    "x = torch.tensor(np.array([[1, eps, eps], [eps, 1, eps]])).float()\n",
    "labels = torch.tensor(np.array([0, 1]))\n",
    "print(torch.all(torch.eq(x.argmax(1), labels)).item() == 1)\n",
    "F.cross_entropy(x, labels, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 63)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x if x else eps for x in one_hot[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16,  9,  6,  4,  4, 10,  5, 17,  6, 18],\n",
       "        [16,  9,  6,  4,  4, 10,  5, 17,  6, 18]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(labels.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
