{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pomegranate import State, DiscreteDistribution, HiddenMarkovModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "from Bio.Alphabet import IUPAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeHMM(): \n",
    "    \n",
    "    def __init__(self, args, x_train=None, weights=None, verbose=True): \n",
    "        \"\"\"\n",
    "        Initializes the HMM to perform generative tasks\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the HMM\n",
    "        args.name : string \n",
    "            defines the name of the HMM\n",
    "        args.hidden_size : int \n",
    "            defines the hidden size\n",
    "        args.max_iterations: int\n",
    "            sets the max iterations\n",
    "        args.n_jobs: int\n",
    "            sets the number of cores to use\n",
    "        args.batch_size : int\n",
    "            sets the batch size\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        args.char_to_int : dict\n",
    "            a map from characters to index (integer) in the sequences\n",
    "        args.build_from_samples : boolean\n",
    "            build model from samples\n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.max_iterations = args[\"max_iterations\"]\n",
    "        self.n_jobs = args[\"n_jobs\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.epoch = args[\"epoch\"]\n",
    "        self.char_to_int = args[\"char_to_int\"]\n",
    "        self.vocabulary = [pair[0] for pair in sorted(self.char_to_int.items(), key = lambda x : x[1])]\n",
    "        self.indexes = [pair[1] for pair in sorted(self.char_to_int.items(), key = lambda x : x[1])]\n",
    "        self.emission_size = len(self.indexes)\n",
    "        if args[\"build_from_samples\"] and x_train is not None: \n",
    "            self.model = HiddenMarkovModel.from_samples(DiscreteDistribution, \n",
    "                                                    n_components = self.hidden_size, \n",
    "                                                    X = x_train, \n",
    "                                                    algorithm = 'baum-welch', \n",
    "                                                    return_history = True,\n",
    "                                                    verbose = verbose,\n",
    "                                                    max_iterations = self.max_iterations,\n",
    "                                                    n_jobs = self.n_jobs, \n",
    "                                                    weights = weights,\n",
    "                                                    #batch_size = self.batch_size,\n",
    "                                                    #batches_per_epoch = self.epochs\n",
    "                                               )[0]\n",
    "            \n",
    "        else: \n",
    "            self.build_model()\n",
    "        self.model.bake()\n",
    "\n",
    "    \n",
    "    def build_model(self): \n",
    "        distributions = []\n",
    "        for _ in range(self.hidden_size): \n",
    "            emission_probs = np.random.random(self.emission_size)\n",
    "            emission_probs = emission_probs / emission_probs.sum()\n",
    "            distributions.append(DiscreteDistribution(dict(zip(self.vocabulary, emission_probs))))\n",
    "        trans_mat = np.random.random((self.hidden_size, self.hidden_size))\n",
    "        trans_mat = trans_mat / trans_mat.sum(axis = 1, keepdims = 1)\n",
    "        starts = np.random.random((self.hidden_size))\n",
    "        starts = starts / starts.sum()\n",
    "        # testing initializations\n",
    "        np.testing.assert_almost_equal(starts.sum(), 1)\n",
    "        np.testing.assert_array_almost_equal(np.ones(self.hidden_size), trans_mat.sum(axis = 1))\n",
    "        self.model = HiddenMarkovModel.from_matrix(trans_mat, distributions, starts)\n",
    "\n",
    "    def fit(self, x_train, weights=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Fits the model on an HMM with self.hidden_size\n",
    "        \"\"\"    \n",
    "        return self.model.fit(x_train, \n",
    "                        algorithm = 'baum-welch', \n",
    "                        return_history = True, \n",
    "                        verbose = verbose,\n",
    "                        max_iterations = self.max_iterations,\n",
    "                        n_jobs = self.n_jobs, \n",
    "                        weights = weights,\n",
    "                        #batch_size = self.batch_size,\n",
    "                        #batches_per_epoch = self.epochs\n",
    "                   )\n",
    "    \n",
    "    def sample(self, n, length):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        n is number of samples\n",
    "        length is how long you want each sample to be\n",
    "        \"\"\"\n",
    "        return np.array([\"\".join(seq) for seq in self.model.sample(n = n, length = length)])\n",
    "            \n",
    "        \n",
    "    def predict(self, x_test): \n",
    "        \"\"\"\n",
    "        predict the log probability of obtaining the sequences in x_test\n",
    "        log(P(X1, X2, ..., X_test)) = sum(log(P(Xi)))\n",
    "        Input: x_test a list of sequences. should be 2 or 3 dimensional\n",
    "        \"\"\"\n",
    "        assert(len(np.array(x_test).shape) == 2 or len(np.array(x_test).shape) == 3)\n",
    "        return sum([self.model.log_probability(seq) for seq in np.array(x_test)])\n",
    "                \n",
    "    def show_model(self): \n",
    "        self.model.plot()\n",
    "        \n",
    "    def save_model(self, path): \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.model.to_json(), f)\n",
    "    \n",
    "    def load_model(self, path): \n",
    "        with open(path, 'r') as f:\n",
    "            json_model = json.load(f)\n",
    "        self.model = HiddenMarkovModel.from_json(json_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_base_args(): \n",
    "    return {\n",
    "        \"name\" : \"base HMM\",\n",
    "        \"hidden_size\" : 5,\n",
    "        \"max_iterations\" : 10,\n",
    "        \"n_jobs\" : 1,\n",
    "        \"batch_size\" : 5,\n",
    "        \"epoch\" : 2,\n",
    "        \"char_to_int\" : {\"A\" : 0, \"C\" : 1, \"T\" : 2, \"G\" : 3},\n",
    "        \"build_from_samples\" : False\n",
    "    }\n",
    "\n",
    "def hmm_build_from_samples_args(): \n",
    "    args = hmm_base_args()\n",
    "    args[\"build_from_samples\"] = True\n",
    "    return args\n",
    "\n",
    "def hmm_amino_acid_args(): \n",
    "    args = hmm_base_args()\n",
    "    amino_acids = get_all_amino_acids()\n",
    "    indexes = list(range(len(amino_acids)))\n",
    "    assert(len(amino_acids) == 21)\n",
    "    assert(amino_acids == \"*\" + IUPAC.protein.letters) #*ACDEFGHIKLMNPQRSTVWY\n",
    "    args[\"char_to_int\"] = dict(zip(amino_acids, indexes))\n",
    "    return args\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit_dna_hmm(X_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    assert(hmm.char_to_int == {\"A\" : 0, \"C\" : 1, \"T\" : 2, \"G\" : 3})\n",
    "    assert(hmm.indexes == sorted(list(range(4))))\n",
    "    assert(hmm.vocabulary == list(\"ACTG\"))\n",
    "    \n",
    "def test_fit_amino_acid_hmm(X_train_sequences):\n",
    "    args = hmm_amino_acid_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences)\n",
    "    amino_acid_alphabet = get_all_amino_acids()\n",
    "    assert(hmm.char_to_int == dict(zip(amino_acid_alphabet, list(range(len(amino_acid_alphabet))))))\n",
    "    assert(hmm.indexes == sorted(list(range(len(amino_acid_alphabet)))))\n",
    "    assert(hmm.vocabulary == list(amino_acid_alphabet))\n",
    "    \n",
    "def test_sample_and_predict_dna_hmm(x_train_sequences): \n",
    "    args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(x_train_sequences, verbose=False)\n",
    "    seq1, seq2 = tuple(hmm.sample(2, 714))\n",
    "    np.testing.assert_almost_equal(hmm.model.probability(seq1), np.e ** hmm.predict([list(seq1)]))\n",
    "    total = 0\n",
    "    for i in \"ACTG\": \n",
    "        for j in \"ACTG\": \n",
    "            for k in \"ACTG\":\n",
    "                codon = i + j + k\n",
    "                np.testing.assert_almost_equal(hmm.model.probability(codon), np.e ** hmm.predict([list(codon)]))\n",
    "                total += np.e ** hmm.predict([list(codon)])\n",
    "    np.testing.assert_almost_equal(1, total)\n",
    "    \n",
    "def test_sample_and_predict_amino_acid_hmm(x_train_sequences): \n",
    "    args = hmm_amino_acid_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(x_train_sequences, verbose=False)\n",
    "    wild_type_amino_acid = get_wild_type_amino_acid_sequence()\n",
    "    seq1, seq2 = tuple(hmm.sample(2, len(wild_type_amino_acid)))\n",
    "    print(\"{0} amino acids away from wild type!\".format(count_substring_mismatch(seq1, \n",
    "                                                            wild_type_amino_acid)))\n",
    "    np.testing.assert_almost_equal(hmm.model.probability(seq1), np.e ** hmm.predict([list(seq1)]))\n",
    "    total = 0\n",
    "    amino_acid_alphabet = get_all_amino_acids()\n",
    "    for i in amino_acid_alphabet: \n",
    "        for j in amino_acid_alphabet: \n",
    "            amino_acid = i + j\n",
    "            np.testing.assert_almost_equal(hmm.model.probability(amino_acid), np.e ** hmm.predict([list(amino_acid)]))\n",
    "            total += np.e ** hmm.predict([list(amino_acid)])\n",
    "    np.testing.assert_almost_equal(total, 1)   \n",
    "    \n",
    "def test_fit_dna_hmm_from_samples(X_train_sequences): \n",
    "    args = hmm_build_from_samples_args()\n",
    "    hmm = GenerativeHMM(args, X_train_sequences)\n",
    "    #test max iterations and n_jobs \n",
    "    args[\"max_iterations\"] = 20\n",
    "    args[\"n_jobs\"] = 5\n",
    "    hmm = GenerativeHMM(args, X_train_sequences)\n",
    "    assert(hmm.char_to_int == {\"A\" : 0, \"C\" : 1, \"T\" : 2, \"G\" : 3})\n",
    "    assert(hmm.indexes == sorted(list(range(4))))\n",
    "    assert(hmm.vocabulary == list(\"ACTG\"))\n",
    "\n",
    "def test_fit_dna_hmm_weights(X_train_sequences):\n",
    "    args = hmm_base_args()\n",
    "    weights = np.identity(4)\n",
    "    weights = np.vstack([weights, [0.25, 0.25, 0.25, 0.25]])\n",
    "    for weight in weights:\n",
    "        counts = {\"A\" : 0, \"C\" : 0, \"T\" : 0, \"G\" : 0}\n",
    "        hmm = GenerativeHMM(args)\n",
    "        hmm.fit(X_train_sequences, weight, verbose=False)\n",
    "        json_model = json.loads(hmm.model.to_json())\n",
    "        for state in json_model[\"states\"]: \n",
    "            if state is not None and state[\"distribution\"] is not None:\n",
    "                mp = state[\"distribution\"][\"parameters\"][0]\n",
    "                for k, v in mp.items(): \n",
    "                    counts[k] = counts[k] + v\n",
    "        print(\"Weights:\", weight, \"\\nCounts:\", counts)    \n",
    "        \n",
    "\n",
    "def test_fit_dna_hmm_from_samples_weights(X_train_sequences):\n",
    "    args = hmm_build_from_samples_args()\n",
    "    weights = np.identity(4)\n",
    "    weights = np.vstack([weights, [0.25, 0.25, 0.25, 0.25]])\n",
    "    for weight in weights: \n",
    "        counts = {\"A\" : 0, \"C\" : 0, \"T\" : 0, \"G\" : 0}\n",
    "        hmm = GenerativeHMM(args, X_train_sequences, weight, verbose=False)\n",
    "        json_model = json.loads(hmm.model.to_json())\n",
    "        for state in json_model[\"states\"]: \n",
    "            if state is not None and state[\"distribution\"] is not None:\n",
    "                mp = state[\"distribution\"][\"parameters\"][0]\n",
    "                for k, v in mp.items(): \n",
    "                    counts[k] = counts[k] + v\n",
    "        print(\"Weights:\", weight, \"\\nCounts:\", counts) \n",
    "        \n",
    "def test_save_and_load_hmm(X_train_sequences, amino_acid = True): \n",
    "    if amino_acid:\n",
    "        args = hmm_amino_acid_args()\n",
    "    else:\n",
    "        args = hmm_base_args()\n",
    "    hmm = GenerativeHMM(args)\n",
    "    hmm.fit(X_train_sequences, verbose=False)\n",
    "    hmm.save_model(\"./models/test.json\")\n",
    "    cached_hmm = GenerativeHMM(args)\n",
    "    cached_hmm.load_model(\"./models/test.json\")\n",
    "    for i in \"ACTG\": \n",
    "        for j in \"ACTG\": \n",
    "            for k in \"ACTG\":\n",
    "                codon = i + j + k\n",
    "                np.testing.assert_almost_equal(hmm.predict([list(codon)]), cached_hmm.predict([list(codon)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 5162.435687994788\tTime (s): 0.08515\n",
      "[2] Improvement: 230.9175405207352\tTime (s): 0.09177\n",
      "[3] Improvement: 253.68468267613207\tTime (s): 0.07873\n",
      "[4] Improvement: 270.1215415486222\tTime (s): 0.08111\n",
      "[5] Improvement: 276.78848326618026\tTime (s): 0.08375\n",
      "[6] Improvement: 274.65405682031997\tTime (s): 0.09699\n",
      "[7] Improvement: 267.2005775821308\tTime (s): 0.09024\n",
      "[8] Improvement: 258.1284609439608\tTime (s): 0.0753\n",
      "[9] Improvement: 250.87274217601225\tTime (s): 0.06963\n",
      "[10] Improvement: 251.31618862047617\tTime (s): 0.08548\n",
      "Total Training Improvement: 7496.119962149358\n",
      "Total Training Time (s): 0.9421\n",
      "[1] Improvement: 4995.463066691169\tTime (s): 0.03124\n",
      "[2] Improvement: 124.45399585046107\tTime (s): 0.03245\n",
      "[3] Improvement: 136.7491685252753\tTime (s): 0.0303\n",
      "[4] Improvement: 162.1052283977042\tTime (s): 0.03053\n",
      "[5] Improvement: 195.22918930339802\tTime (s): 0.03778\n",
      "[6] Improvement: 229.28251537683536\tTime (s): 0.05762\n",
      "[7] Improvement: 254.73194333698484\tTime (s): 0.04134\n",
      "[8] Improvement: 263.8288510042912\tTime (s): 0.04065\n",
      "[9] Improvement: 257.34297210353543\tTime (s): 0.03483\n",
      "[10] Improvement: 244.23380053612345\tTime (s): 0.03694\n",
      "[11] Improvement: 235.03704128719983\tTime (s): 0.0376\n",
      "[12] Improvement: 237.18016778334277\tTime (s): 0.02987\n",
      "[13] Improvement: 255.44268052180996\tTime (s): 0.03174\n",
      "[14] Improvement: 291.7962774626649\tTime (s): 0.03133\n",
      "[15] Improvement: 338.42163208979764\tTime (s): 0.03291\n",
      "[16] Improvement: 369.9711628683581\tTime (s): 0.04991\n",
      "[17] Improvement: 360.30924763505755\tTime (s): 0.05191\n",
      "[18] Improvement: 315.99289811648487\tTime (s): 0.04522\n",
      "[19] Improvement: 272.13311231153784\tTime (s): 0.04905\n",
      "[20] Improvement: 246.37913063792803\tTime (s): 0.04617\n",
      "Total Training Improvement: 9786.08408183996\n",
      "Total Training Time (s): 0.9479\n",
      "224 amino acids away from wild type!\n"
     ]
    }
   ],
   "source": [
    "test_fit_amino_acid_hmm(test_amino_acid_sequences)\n",
    "test_sample_and_predict_amino_acid_hmm(test_amino_acid_sequences)\n",
    "test_save_and_load_hmm(test_amino_acid_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 6083.283013946668\tTime (s): 0.2039\n",
      "[2] Improvement: 36.937274912328576\tTime (s): 0.231\n",
      "[3] Improvement: 31.219716774328845\tTime (s): 0.2442\n",
      "[4] Improvement: 28.676809412776493\tTime (s): 0.2486\n",
      "[5] Improvement: 27.394446881560725\tTime (s): 0.2233\n",
      "[6] Improvement: 26.826207615318708\tTime (s): 0.2477\n",
      "[7] Improvement: 26.83171673362085\tTime (s): 0.2344\n",
      "[8] Improvement: 27.40972604167473\tTime (s): 0.2246\n",
      "[9] Improvement: 28.619267253947328\tTime (s): 0.2206\n",
      "[10] Improvement: 30.55262447008863\tTime (s): 0.2316\n",
      "Total Training Improvement: 6347.750804042313\n",
      "Total Training Time (s): 2.5842\n",
      "[1] Improvement: 6746.036801595765\tTime (s): 0.101\n",
      "[2] Improvement: 56.36529585690005\tTime (s): 0.1085\n",
      "[3] Improvement: 48.06766113333288\tTime (s): 0.1005\n",
      "[4] Improvement: 41.49199818276975\tTime (s): 0.08484\n",
      "[5] Improvement: 36.9103623496776\tTime (s): 0.07277\n",
      "[6] Improvement: 33.993052267614985\tTime (s): 0.08344\n",
      "[7] Improvement: 32.23265647207154\tTime (s): 0.07578\n",
      "[8] Improvement: 31.228094900914584\tTime (s): 0.1066\n",
      "[9] Improvement: 30.734892359148944\tTime (s): 0.1465\n",
      "[10] Improvement: 30.630747019458795\tTime (s): 0.1294\n",
      "[11] Improvement: 30.87565189898305\tTime (s): 0.1285\n",
      "[12] Improvement: 31.484842213161755\tTime (s): 0.07785\n",
      "[13] Improvement: 32.51331420709903\tTime (s): 0.1029\n",
      "[14] Improvement: 34.04826608882286\tTime (s): 0.1268\n",
      "[15] Improvement: 36.207379120693076\tTime (s): 0.1012\n",
      "[16] Improvement: 39.142513541679364\tTime (s): 0.08577\n",
      "[17] Improvement: 43.04953655699501\tTime (s): 0.1235\n",
      "[18] Improvement: 48.185872476678924\tTime (s): 0.09721\n",
      "[19] Improvement: 54.89838092307036\tTime (s): 0.07631\n",
      "[20] Improvement: 63.66577867095475\tTime (s): 0.1432\n",
      "Total Training Improvement: 7501.763097835792\n",
      "Total Training Time (s): 2.3144\n",
      "[1] Improvement: 5703.276332879381\tTime (s): 0.2351\n",
      "[2] Improvement: 18.012776080839103\tTime (s): 0.227\n",
      "[3] Improvement: 18.046415275224717\tTime (s): 0.2253\n",
      "[4] Improvement: 18.000252051118878\tTime (s): 0.2282\n",
      "[5] Improvement: 17.925809234497137\tTime (s): 0.2606\n",
      "[6] Improvement: 17.920575849260786\tTime (s): 0.2482\n",
      "[7] Improvement: 18.079668019665405\tTime (s): 0.2489\n",
      "[8] Improvement: 18.473917621638975\tTime (s): 0.2439\n",
      "[9] Improvement: 19.143634131061845\tTime (s): 0.214\n",
      "[10] Improvement: 20.10012251866283\tTime (s): 0.199\n",
      "Total Training Improvement: 5868.9795036613505\n",
      "Total Training Time (s): 2.6285\n",
      "[1] Improvement: 4399.418434875799\tTime (s): 0.09455\n",
      "[2] Improvement: 38.61896147450898\tTime (s): 0.09762\n",
      "[3] Improvement: 25.768566641883808\tTime (s): 0.06932\n",
      "[4] Improvement: 20.647451314260252\tTime (s): 0.08213\n",
      "[5] Improvement: 17.814587876273436\tTime (s): 0.1336\n",
      "[6] Improvement: 15.99622523036669\tTime (s): 0.1063\n",
      "[7] Improvement: 14.787809733956237\tTime (s): 0.07772\n",
      "[8] Improvement: 13.996752639039187\tTime (s): 0.07183\n",
      "[9] Improvement: 13.499912691651843\tTime (s): 0.09397\n",
      "[10] Improvement: 13.208678380848141\tTime (s): 0.1163\n",
      "[11] Improvement: 13.057357349724043\tTime (s): 0.08935\n",
      "[12] Improvement: 12.997120053478284\tTime (s): 0.1013\n",
      "[13] Improvement: 12.991962789077661\tTime (s): 0.07139\n",
      "[14] Improvement: 13.015908073954051\tTime (s): 0.09816\n",
      "[15] Improvement: 13.051083841448417\tTime (s): 0.1134\n",
      "[16] Improvement: 13.086387707517133\tTime (s): 0.1426\n",
      "[17] Improvement: 13.116497660666937\tTime (s): 0.09981\n",
      "[18] Improvement: 13.141064146955614\tTime (s): 0.1114\n",
      "[19] Improvement: 13.16399581359292\tTime (s): 0.111\n",
      "[20] Improvement: 13.192819591669831\tTime (s): 0.07925\n",
      "Total Training Improvement: 4704.5715778866725\n",
      "Total Training Time (s): 2.2252\n",
      "Weights: [1. 0. 0. 0.] \n",
      "Counts: {'A': 3.1586063965958173, 'C': 0.0, 'T': 1.8413936034041831, 'G': 0.0}\n",
      "Weights: [0. 1. 0. 0.] \n",
      "Counts: {'A': 0.0, 'C': 3.3500249007810825, 'T': 0.0, 'G': 1.6499750992189177}\n",
      "Weights: [0. 0. 1. 0.] \n",
      "Counts: {'A': 1.5245716345213942, 'C': 0.0, 'T': 3.4754283654786065, 'G': 0.0}\n",
      "Weights: [0. 0. 0. 1.] \n",
      "Counts: {'A': 0.0, 'C': 1.3389862318366896, 'T': 0.0, 'G': 3.66101376816331}\n",
      "Weights: [0.25 0.25 0.25 0.25] \n",
      "Counts: {'A': 1.3274908786246902, 'C': 1.556791387424291, 'T': 0.6725754810666891, 'G': 1.4431422528843294}\n",
      "Weights: [1. 0. 0. 0.] \n",
      "Counts: {'A': 3.6484219835147615, 'C': 0.0, 'T': 1.3515780164852382, 'G': 0.0}\n",
      "Weights: [0. 1. 0. 0.] \n",
      "Counts: {'A': 0.0, 'C': 3.852816043100242, 'T': 0.0, 'G': 1.1471839568997584}\n",
      "Weights: [0. 0. 1. 0.] \n",
      "Counts: {'A': 1.388276745350232, 'C': 0.0, 'T': 3.611723254649768, 'G': 0.0}\n",
      "Weights: [0. 0. 0. 1.] \n",
      "Counts: {'A': 0.0, 'C': 1.494311908040209, 'T': 0.0, 'G': 3.505688091959791}\n",
      "Weights: [0.25 0.25 0.25 0.25] \n",
      "Counts: {'A': 1.4807778214929486, 'C': 1.001554347775447, 'T': 1.5192221785070514, 'G': 0.9984456522245533}\n"
     ]
    }
   ],
   "source": [
    "test_fit_dna_hmm(test_dna_sequences)\n",
    "test_sample_and_predict_dna_hmm(test_dna_sequences)\n",
    "test_fit_dna_hmm_from_samples(test_dna_sequences)\n",
    "test_save_and_load_hmm(test_dna_sequences)\n",
    "\n",
    "synthetic_data = np.array([[\"A\", \"A\", \"A\", \"T\"], [\"C\", \"C\", \"C\", \"G\"], [\"T\", \"T\", \"T\", \"A\"], [\"G\", \"G\", \"G\", \"C\"]])\n",
    "test_fit_dna_hmm_weights(synthetic_data)\n",
    "test_fit_dna_hmm_from_samples_weights(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished loading data in 4.25 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_dna_\") #./data/gfp_dna_\n",
    "mutated_df = load_saved_mutated_gfp_data()\n",
    "assert(X_train[0] == get_wild_type_dna_sequence())\n",
    "assert(count_substring_mismatch(X_train[999], get_wild_type_dna_sequence()) == 3)\n",
    "print(\"Finished loading data in {0:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "base_args = hmm_base_args()\n",
    "build_from_samples_args = hmm_build_from_samples_args()\n",
    "test_dna_sequences = np.array([list(seq) for seq in X_train[0:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished loading data in 1.71 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_amino_acid_\") #./data/gfp_amino_acid_\n",
    "assert(X_train[0] == get_wild_type_amino_acid_sequence())\n",
    "assert(count_substring_mismatch(X_train[999], get_wild_type_amino_acid_sequence()) == 3)\n",
    "print(\"Finished loading data in {0:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "base_amino_acid_args = hmm_amino_acid_args()\n",
    "test_amino_acid_sequences = np.array([list(seq) for seq in X_train[0:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
