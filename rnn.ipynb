{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import load_gfp_data, get_all_amino_acids, get_wild_type_amino_acid_sequence \n",
    "from utils import count_substring_mismatch, string_to_tensor, string_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch/blob/master/model.py\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"lstm\", n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input is of shape (batch_size, 1) where each input[x, 0] is the word index\n",
    "        # char RNN so we generate one character at a time. \n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "class GenerativeRNN(): \n",
    "    \n",
    "    def __init__(self, args):     \n",
    "        \"\"\"\n",
    "        Initializes the RNN to be a generative char RNN\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the neural network\n",
    "        args.name : string \n",
    "            defines the name of the neural network\n",
    "        args.description: string\n",
    "            describes the architecture of the neural network\n",
    "        args.layers : int\n",
    "            specifies the number of stacked layers we want in the LSTM\n",
    "        args.hidden_size : int\n",
    "            the size of the hidden layer\n",
    "        args.learning_rate : float\n",
    "            sets the learning rate\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        args.vocabulary : string\n",
    "            all the characters in the context of the problem\n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.description = args[\"description\"]\n",
    "        self.layers = args[\"layers\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.learning_rate = args[\"learning_rate\"]\n",
    "        self.epochs = args[\"epochs\"]\n",
    "        self.all_characters = args[\"vocabulary\"]\n",
    "        self.num_characters = len(self.all_characters)\n",
    "        self.character_to_int = dict(zip(self.all_characters, range(self.num_characters)))\n",
    "        self.int_to_character = dict(zip(range(self.num_characters), self.all_characters))\n",
    "        self.model = RNN(self.num_characters, self.hidden_size, self.num_characters, \"lstm\", self.layers)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.loss_history = []\n",
    "        \n",
    "\n",
    "    def fit(self, dataloader, verbose=True, logger=None, save_model=True):\n",
    "        # amino acid dataset specific checks\n",
    "        wild_type = get_wild_type_amino_acid_sequence()\n",
    "        three_mutation = \"\".join([self.int_to_character[np.random.randint(0, self.num_characters)] if i % 3 == 1 else wild_type[i] for i in range(10)])\n",
    "        ten_mutation = \"\".join([self.int_to_character[np.random.randint(0, self.num_characters)] for i in range(10)])\n",
    "        print(wild_type[0:10], three_mutation, ten_mutation, file=logger)\n",
    "        \n",
    "        if not os.path.isdir(\"./models/{0}\".format(self.name)):\n",
    "            os.mkdir(\"./models/{0}\".format(self.name))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.loss_history = []\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            total_loss = []\n",
    "            for i, (input, target) in enumerate(dataloader):\n",
    "                batch_size, seq_length = input.shape[0], input.shape[1]\n",
    "                hidden = self.model.init_hidden(batch_size)\n",
    "                self.model.zero_grad()\n",
    "                \n",
    "                loss = 0\n",
    "                for c in range(seq_length):\n",
    "                    output, hidden = self.model(input[:, c], hidden)\n",
    "                    loss += self.criterion(output.view(batch_size, -1), target[:, c])\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss.append(loss.item() / seq_length)\n",
    "            \n",
    "            self.loss_history.append(np.mean(total_loss))\n",
    "            generated_sequence = self.sample(predict_len = len(wild_type) - 1, prime_str = \"S\",)\n",
    "            mismatches = count_substring_mismatch(wild_type, generated_sequence)\n",
    "            wild_prob, mutation_three_prob, mutation_ten_prob = self.predict_log_prob(wild_type[1:10]), self.predict_log_prob(three_mutation[1:10]), self.predict_log_prob(ten_mutation[1:10])\n",
    "            \n",
    "            if verbose: \n",
    "                print(\"epoch {0}. loss: {1:.2f}. time: {2:.2f} seconds.\".format(epoch, self.loss_history[-1], time.time() - start_time), file = logger)\n",
    "                print(\"generated sequence: {0}\\n{1} mismatches from the wild type\".format(generated_sequence, mismatches), file = logger) \n",
    "                print(\"wild type log prob: {0}. 3 mutations log prob: {1}. 10 mutations log prob: {2}.\\n\" \\\n",
    "                      .format(wild_prob, mutation_three_prob, mutation_ten_prob), file = logger)\n",
    "            if save_model:\n",
    "                self.save_model(epoch, total_loss)        \n",
    "\n",
    "    def predict_log_prob(self, sequence, prime_str = \"S\"):\n",
    "        hidden = self.model.init_hidden(1) \n",
    "        prime_input = string_to_tensor(prime_str, self.character_to_int)\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = self.model(prime_input[p], hidden)\n",
    "        input = prime_input[-1]\n",
    "\n",
    "        log_prob = 0\n",
    "        for char in sequence:\n",
    "            output, hidden = self.model(input.view(1, -1), hidden)\n",
    "            softmax = nn.Softmax(dim = 1)\n",
    "            probs = softmax(output).view(-1)\n",
    "            i = self.character_to_int[char]\n",
    "            log_prob += np.log(probs[i].item())\n",
    "        return log_prob\n",
    "\n",
    "    def sample(self, predict_len, prime_str = 'S', temperature = 1):\n",
    "        hidden = self.model.init_hidden(1)\n",
    "        prime_input = string_to_tensor(prime_str, self.character_to_int)\n",
    "        predicted = prime_str\n",
    "\n",
    "        # Use priming string to \"build up\" hidden state\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            output, hidden = self.model(prime_input[p], hidden)\n",
    "        input = prime_input[-1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, hidden = self.model(input.view(1, -1), hidden)\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0].item()\n",
    "\n",
    "            # Add predicted character to string and use as next input\n",
    "            predicted_char = self.int_to_character[top_i]\n",
    "            predicted += predicted_char\n",
    "            input = string_to_tensor(predicted_char, self.character_to_int)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "            \n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(\"./models/{0}/{1}\".format(self.name, model_path))\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    def save_model(self, epoch=None, loss=None): \n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict()\n",
    "                }, \"./models/{0}/checkpoint_{1}.pt\".format(self.name, epoch))\n",
    "\n",
    "    \n",
    "    def show_model(self): \n",
    "        print(self.model)\n",
    "    \n",
    "    def plot_model(self, save_dir, verbose=True): \n",
    "        hidden = self.model.init_hidden(1)\n",
    "        out, _ = self.model(string_to_tensor(\"S\", self.character_to_int), hidden)\n",
    "        graph = make_dot(out)\n",
    "        if save_dir is not None:\n",
    "            graph.format = \"png\"\n",
    "            graph.render(save_dir) \n",
    "        if verbose:\n",
    "            graph.view()\n",
    "            \n",
    "    def plot_history(self, save_fig_dir): \n",
    "        plt.figure()\n",
    "        plt.title(\"Training Loss Curve\")\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xticks(range(self.epochs))\n",
    "        if save_fig_dir:\n",
    "            plt.savefig(save_fig_dir)\n",
    "        plt.show()\n",
    "    \n",
    "def get_test_args():\n",
    "    args = {\n",
    "        \"name\" : \"rnn_test_sample\",\n",
    "        \"layers\" : 2, \n",
    "        \"hidden_size\" : 200,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"epochs\" : 10,\n",
    "        \"vocabulary\" : get_all_amino_acids(),\n",
    "        \"num_data\" : 100, \n",
    "        \"batch_size\" : 10\n",
    "    }\n",
    "    args[\"description\"] = \"name: {0}, layers {1}, hidden size {2}, lr {3}, epochs {4}\".format(args[\"name\"], \n",
    "                            args[\"layers\"], args[\"hidden_size\"], args[\"learning_rate\"], args[\"epochs\"])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. print to file the epoch the time it took and the loss, print out a generated sample and the mismatches (DONE)\\n2. plot history (DONE)\\n3. save model every epoch, load model at end, plot model architecture (DONE)\\n4. tensorboard going (DONE)\\n5. code it into class (DONE)\\n6. debug (DONE)\\n7. Write tests 10 min (DONE)\\n8. code it into python file. ** try not to rely on global variables, make everything OOP. \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. print to file the epoch the time it took and the loss, print out a generated sample and the mismatches (DONE)\n",
    "2. plot history (DONE)\n",
    "3. save model every epoch, load model at end, plot model architecture (DONE)\n",
    "4. tensorboard going (DONE)\n",
    "5. code it into class (DONE)\n",
    "6. debug (DONE)\n",
    "7. Write tests 10 min (DONE)\n",
    "8. code it into python file. ** try not to rely on global variables, make everything OOP. \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(X_train, length, character_to_int, n = 100, batch_size = 1, shuffle=True, random=True):\n",
    "    if not random: \n",
    "        data = X_train[0:n]\n",
    "    else: \n",
    "        indexes = np.random.choice(len(X_train), n)\n",
    "        data = X_train[indexes]        \n",
    "    dataset = np.array([string_to_numpy(x[0:length], character_to_int) for x in data])\n",
    "    input = torch.from_numpy(dataset[:, :-1]).long()\n",
    "    output = torch.from_numpy(dataset[:, 1:]).long()\n",
    "    tensor_dataset = TensorDataset(input, output)\n",
    "    return DataLoader(tensor_dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "\n",
    "\n",
    "wild_type = get_wild_type_amino_acid_sequence()\n",
    "seq_length = len(wild_type)\n",
    "batch_size = 10\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_amino_acid_\")\n",
    "char_to_int = dict(zip(get_all_amino_acids(), range(len(get_all_amino_acids()))))\n",
    "dataloader = get_dataloader(X_train, length=seq_length, character_to_int=char_to_int, n=99, batch_size=batch_size, shuffle=True, random=True)\n",
    "args = get_test_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = GenerativeRNN(args)\n",
    "logger = open(\"./logs/{0}.txt\".format(args[\"name\"]), \"w\")\n",
    "rnn.fit(dataloader=dataloader, logger=logger)\n",
    "rnn.show_model()\n",
    "rnn.plot_history(\"./logs/{0}_training_history\".format(args[\"name\"]))\n",
    "rnn.plot_model(\"./logs/{0}_model_architecture\".format(args[\"name\"]))\n",
    "temperature_lst = [0.2, 0.8, 1.0, 1.2, 1.8]\n",
    "for temperature in temperature_lst: \n",
    "    \n",
    "    generated_sequence = rnn.sample(predict_len = len(wild_type) - 1, prime_str = \"S\", temperature = temperature)\n",
    "    mismatches = count_substring_mismatch(wild_type, generated_sequence)\n",
    "    print(\"temperature: {0}. generated sequence: {1} with {2} mismatches from the wild type.\".format(temperature, generated_sequence, mismatches), file=logger) \n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_all_sequences(model, string, base = \"S\", depth = 3): \n",
    "    if depth == 0: \n",
    "        return np.e ** model.predict_log_prob(string, base)\n",
    "    total = 0\n",
    "    for c in get_all_amino_acids(): \n",
    "        total += enumerate_all_sequences(model, string + c, base, depth - 1)\n",
    "    return total\n",
    "\n",
    "for depth in range(1, 4):\n",
    "    for base in \"QRST\":\n",
    "        np.testing.assert_almost_equal(1, enumerate_all_sequences(rnn, \"\", base, depth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:12: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.00000e-02 *\n",
      "       7.6142), tensor(1.00000e-02 *\n",
      "       9.9237), tensor(0.1153), tensor(0.1120), tensor(0.1042), tensor(1.00000e-02 *\n",
      "       7.5500), tensor(0.1046), tensor(1.00000e-02 *\n",
      "       9.2714), tensor(1.00000e-02 *\n",
      "       8.2159), tensor(0.1154)]\n",
      "[tensor(1.00000e-02 *\n",
      "       9.7746), tensor(0.1194), tensor(0.1002), tensor(1.00000e-02 *\n",
      "       9.2724), tensor(0.1102), tensor(0.1109), tensor(0.1092), tensor(0.1286), tensor(0.1177), tensor(1.00000e-02 *\n",
      "       7.7318)]\n"
     ]
    }
   ],
   "source": [
    "epoch = 8\n",
    "load_rnn = GenerativeRNN(args)\n",
    "load_rnn.load_model(\"./checkpoint_{0}.pt\".format(epoch))\n",
    "total_loss = []\n",
    "for i, (input, target) in enumerate(dataloader):\n",
    "    batch_size, seq_length = input.shape[0], input.shape[1]\n",
    "    hidden = load_rnn.model.init_hidden(batch_size)\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        output, hidden = load_rnn.model(input[:, c], hidden)\n",
    "        loss += load_rnn.criterion(output.view(batch_size, -1), target[:, c])\n",
    "    total_loss.append(loss.item() / seq_length) \n",
    "print(total_loss)\n",
    "print(torch.load(\"./models/rnn_test_sample/checkpoint_{0}.pt\".format(epoch))[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
