{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import load_gfp_data, get_all_amino_acids, get_wild_type_amino_acid_sequence, count_substring_mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/spro/char-rnn.pytorch/blob/master/model.py\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"lstm\", n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input is of shape (batch_size, 1) where each input[x, 0] is the word index\n",
    "        # char RNN so we generate one character at a time. \n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "    \n",
    "class GenerativeRNN(): \n",
    "    \n",
    "    def __init__(self, args):     \n",
    "        \"\"\"\n",
    "        Initializes the RNN to be a generative char RNN\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dictionary\n",
    "            defines the hyper-parameters of the neural network\n",
    "        args.name : string \n",
    "            defines the name of the neural network\n",
    "        args.description: string\n",
    "            describes the architecture of the neural network\n",
    "        args.input : int \n",
    "            defines the input/vocabulary size\n",
    "        args.output : int\n",
    "            the number of output layers in the final layer of the neural network\n",
    "        args.layers : int\n",
    "            specifies the number of stacked layers we want in the LSTM\n",
    "        args.hidden_size : int\n",
    "            the size of the hidden layer\n",
    "        args.learning_rate : float\n",
    "            sets the learning rate\n",
    "        args.epochs : int \n",
    "            sets the epoch size \n",
    "        \"\"\"\n",
    "        self.name = args[\"name\"]\n",
    "        self.description = args[\"description\"]\n",
    "        self.input = args[\"input\"]\n",
    "        self.output = args[\"output\"]\n",
    "        self.layers = args[\"layers\"]\n",
    "        self.hidden_size = args[\"hidden_size\"]\n",
    "        self.learning_rate = args[\"learning_rate\"]\n",
    "        self.epochs = args[\"epochs\"]\n",
    "        self.model = RNN(self.input, self.hidden_size, self.output, \"lstm\", self.layers)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.loss_history = []\n",
    "        \n",
    "\n",
    "    def fit(self, dataloader, verbose=True, logger=None, save_model=True):\n",
    "        # amino acid dataset specific checks\n",
    "        wild_type = get_wild_type_amino_acid_sequence()\n",
    "        three_mutation = \"\".join([int_to_character[np.random.randint(0, num_characters)] if i % 3 == 1 else wild_type[i] for i in range(10)])\n",
    "        ten_mutation = \"\".join([int_to_character[np.random.randint(0, num_characters)] for i in range(10)])\n",
    "        print(wild_type[0:10], three_mutation, ten_mutation)\n",
    "        \n",
    "        if not os.path.isdir(\"./models/{0}\".format(self.name)):\n",
    "            os.mkdir(\"./models/{0}\".format(self.name))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.loss_history = []\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            total_loss = []\n",
    "            for i, (input, target) in enumerate(dataloader):\n",
    "                batch_size, seq_length = input.shape[0], input.shape[1]\n",
    "                hidden = self.model.init_hidden(batch_size)\n",
    "                self.model.zero_grad()\n",
    "                loss = 0\n",
    "                for c in range(seq_length):\n",
    "                    output, hidden = self.model(input[:, c], hidden)\n",
    "                    loss += self.criterion(output.view(batch_size, -1), target[:, c])\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss.append(loss.data[0] / seq_length)\n",
    "            \n",
    "            self.loss_history.append(np.mean(total_loss))\n",
    "            generated_sequence = self.sample(prime_str = \"S\", predict_len = len(wild_type) - 1)\n",
    "            mismatches = count_substring_mismatch(wild_type, generated_sequence)\n",
    "            wild_prob, mutation_three_prob, mutation_ten_prob = self.predict_log_prob(wild_type[1:10]), self.predict_log_prob(three_mutation[1:10]), self.predict_log_prob(ten_mutation[1:10])\n",
    "            \n",
    "            if verbose: \n",
    "                print(\"epoch {0}. loss: {1:.2f}. time: {2:.2f} seconds.\".format(epoch, self.loss_history[-1], time.time() - start_time), file = logger)\n",
    "                print(\"generated sequence: {0}\\n{1} mismatches from the wild type\".format(generated_sequence, mismatches), file = logger) \n",
    "                print(\"wild type log prob: {0}. 3 mutations log prob: {1}. 10 mutations log prob: {2}.\\n\" \\\n",
    "                      .format(wild_prob, mutation_three_prob, mutation_ten_prob), file = logger)\n",
    "            if save_model:\n",
    "                self.save_model(epoch, total_loss)        \n",
    "        \n",
    "        if logger:        \n",
    "            logger.close()\n",
    "\n",
    "    def predict_log_prob(self, sequence, prime_str = \"S\"):\n",
    "        hidden = self.model.init_hidden(1) \n",
    "        prime_input = string_to_tensor(prime_str)\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = self.model(prime_input[p], hidden)\n",
    "        input = prime_input[-1]\n",
    "\n",
    "        log_prob = 0\n",
    "        for char in sequence:\n",
    "            output, hidden = self.model(input.view(1, -1), hidden)\n",
    "            softmax = nn.Softmax(dim = 1)\n",
    "            probs = softmax(output).view(-1)\n",
    "            i = character_to_int[char]\n",
    "            log_prob += np.log(probs[i].item())\n",
    "        return log_prob\n",
    "\n",
    "    def sample(self, predict_len, prime_str = 'S', temperature = 1):\n",
    "        hidden = self.model.init_hidden(1)\n",
    "        prime_input = string_to_tensor(prime_str)\n",
    "        predicted = prime_str\n",
    "\n",
    "        # Use priming string to \"build up\" hidden state\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            output, hidden = self.model(prime_input[p], hidden)\n",
    "        input = prime_input[-1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, hidden = self.model(input.view(1, -1), hidden)\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0].item()\n",
    "\n",
    "            # Add predicted character to string and use as next input\n",
    "            predicted_char = int_to_character[top_i]\n",
    "            predicted += predicted_char\n",
    "            input = string_to_tensor(predicted_char)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "            \n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(\"./models/{0}/{1}\".format(self.name, model_path))\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    def save_model(self, epoch=None, loss=None): \n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'loss': loss,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict()\n",
    "                }, \"./models/{0}/checkpoint_{1}.pt\".format(self.name, epoch))\n",
    "\n",
    "    \n",
    "    def show_model(self): \n",
    "        print(self.model)\n",
    "    \n",
    "    def plot_model(self, save_dir, verbose=True): \n",
    "        hidden = self.model.init_hidden(1)\n",
    "        out, _ = self.model(string_to_tensor(\"S\"), hidden)\n",
    "        graph = make_dot(out)\n",
    "        if save_dir is not None:\n",
    "            graph.format = \"png\"\n",
    "            graph.render(save_dir) \n",
    "        if verbose:\n",
    "            graph.view()\n",
    "            \n",
    "    def plot_history(self, save_fig_dir): \n",
    "        plt.figure()\n",
    "        plt.title(\"Training Loss Curve\")\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xticks(range(self.epochs))\n",
    "        if save_fig_dir:\n",
    "            plt.savefig(save_fig_dir)\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. print to file the epoch the time it took and the loss, print out a generated sample and the mismatches (DONE)\\n2. plot history (DONE)\\n3. save model every epoch, load model at end, plot model architecture (DONE)\\n4. tensorboard going (DONE)\\n5. code it into class (DONE)\\n6. Write tests 10 min (DONE)\\n7. debug (DONE)\\n'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. print to file the epoch the time it took and the loss, print out a generated sample and the mismatches (DONE)\n",
    "2. plot history (DONE)\n",
    "3. save model every epoch, load model at end, plot model architecture (DONE)\n",
    "4. tensorboard going (DONE)\n",
    "5. code it into class (DONE)\n",
    "6. debug (DONE)\n",
    "7. Write tests 10 min (DONE)\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = get_all_amino_acids()\n",
    "num_characters = len(all_characters)\n",
    "character_to_int = dict(zip(all_characters, range(num_characters)))\n",
    "int_to_character = dict(zip(range(num_characters), all_characters))\n",
    "\n",
    "def string_to_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for i, c in enumerate(string): \n",
    "        tensor[i] = character_to_int[c]\n",
    "    return tensor\n",
    "\n",
    "def string_to_numpy(string):\n",
    "    arr = np.zeros(len(string))\n",
    "    for i, c in enumerate(string): \n",
    "        arr[i] = character_to_int[c]\n",
    "    return arr\n",
    "    \n",
    "def get_dataloader(X_train, length, n = 100, batch_size = 1, shuffle=True, random=True):\n",
    "    if not random: \n",
    "        data = X_train[0:n]\n",
    "    else: \n",
    "        indexes = np.random.choice(len(X_train), n)\n",
    "        data = X_train[indexes]        \n",
    "    dataset = np.array([string_to_numpy(x[0:length]) for x in data])\n",
    "    input = torch.from_numpy(dataset[:, :-1]).long()\n",
    "    output = torch.from_numpy(dataset[:, 1:]).long()\n",
    "    tensor_dataset = TensorDataset(input, output)\n",
    "    return DataLoader(tensor_dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "\n",
    "def get_medium_args():\n",
    "    return {\n",
    "        \"name\" : \"rnn_test_sample\",\n",
    "        \"description\" : \"layers 2, hidden size 200, lr 0.005, epochs 10\",\n",
    "        \"input\" : num_characters,\n",
    "        \"output\" : num_characters,\n",
    "        \"layers\" : 2, \n",
    "        \"hidden_size\" : 200,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"epochs\" : 10 \n",
    "    }\n",
    "\n",
    "\n",
    "wild_type = get_wild_type_amino_acid_sequence()\n",
    "seq_length = len(wild_type)\n",
    "batch_size = 10\n",
    "X_train, X_test, y_train, y_test = load_gfp_data(\"./data/gfp_amino_acid_\")\n",
    "dataloader = get_dataloader(X_train, length=seq_length, n=99, batch_size=batch_size, shuffle=True, random=True)\n",
    "args = get_medium_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKGEELFTGV SIGE*LFGGV VRRHYEQTQI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:98: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (encoder): Embedding(21, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=21, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucXHV9//HXe/aaZDfXnSQkgQSS7EJiucYo12xBEJBirahgvbVatNIqau1P66W/n9XWVn/aWn1YUayoiPhTaa1EiKJc5RaQWxJCQkhIYpLdXHdz2+zl8/tjzi6TZZNsLrNnZuf9fDzOIzPnfOecz+wD5j3nfL/zPYoIzMzMADJpF2BmZsXDoWBmZn0cCmZm1sehYGZmfRwKZmbWx6FgZmZ9HApWUiRVSNop6YRj2dbMchwKVlDJh3Lv0iNpT97zPz3c/UVEd0TURcSLx7Lt4ZL0WUnfOdb7HeSxJelDkpZI2iVpnaQfSXpFGvXY8FKZdgE2vEVEXe9jSauB90TErw7UXlJlRHQNRW0l7GvAxcBfAL8l9//xG4HLgWcOZ0f+e1t/PlOwVCXfuG+VdIukduBtks6W9JCk7ZI2SPqKpKqkfaWkkDQjef79ZPsvJLVLelDSiYfbNtl+maTnJO2Q9O+SHpD0riN4T3Ml3ZPU/7Sk1+Vtu0LSsuT46yR9KFk/UdLC5DVbJd17gH2fArwXeEtE3B0R+yJid0R8LyL+JWlzf37dkt4j6e5+f5P3S1oJPCvpm5I+3+84t0v6QPJ4mqTbJLVKekHSdYf7N7HS4VCwYvAG4AfAGOBWoAv4INAAnAtcSu6D8EDeCnwKGA+8CPzD4baVNBH4EfDR5LgvAPMP941IqgZ+DtwOZIEPAbdKmpU0+U/g3RFRD5wK3JOs/yiwKnnNZOCTBzjERcDqiHj8cGvr50rglcAfALcAV0tS8h4mABcmdWeS9/MoMJXcGcpHJV10lMe3IuVQsGJwf0T8T0T0RMSeiHg0Ih6OiK6IWAXcACw4yOt/HBGLI6ITuBk4/QjaXgE8ERH/nWz7MrD5CN7LuUA18IWI6Ewulf0CuDrZ3gnMkVQfEVvzPtw7gSnACcm3/wHPFIAJwIYjqKu/f4yIbRGxB7gbqALOTra9GbgvIjYl60ZHxD8mda0Ebsx7PzbMOBSsGKzNfyLp5OTyxUZJbcBnyH17P5CNeY93A3UHaniQtlPy64jcTJHrBlF7f1OAF2P/mSbXkPuWDbmzoiuBFyXdLelVyfrPJ+3ukvS8pI8eYP9bgOOOoK7+8t9rD7kztGuSVW8lF5gA04ETksta2yVtB/6W3NmMDUMOBSsG/afq/Qa5DtNZETEa+DSgAtewAZjW+yS5lDL1wM0P6PfA8b2XYhInAOsBkjOgK4GJ5C7L/DBZ3xYRH4qIGcAfA/9L0kBnR3cBMySdcZAadgEj854P9AHe/29+C/CmpI/lTOCnyfq1wIqIGJu31EfEHx3k+FbCHApWjOqBHcCuvI7VQvs5cKakP5JUSa5PI3uI11RIqs1basiNBuoCPiKpStKF5EYF3SpphKS3ShqdXKJqB3oAkuPOTMJkB9Dduy1fRCwjdzntVkkLJFXn7bf37OIJ4I3J+kbgzw/15iPiUaAt2ffCiGhPNj0I7JP0keQ9Vkj6A0lnHWqfVpocClaMPgK8k9yH5jfIXdooqOT6+VuAL5G7RDMT+B3QcZCXvQ3Yk7csj4gO4I+A15Prk/gK8NaIWJG85p3AmuSy2LuTfQA0Ab8GdgIPAP8WEfcd4LjXAV9Plm3ACnKXpG5Ptn+R3JlAC/Bt4PuD+iPkzhZeQ67TH4BkuOrl5DrdVyfv6RvA6EHu00qMfJMds5eTVEHuUtBVB/lwNht2fKZglpB0qaSxyWWgT5EbEfRIymWZDSmHgtlLziP3W4FW4LXAG5LLQWZlw5ePzMysj88UzMysT8lNiNfQ0BAzZsxIuwwzs5Ly2GOPbY6IQw2zLr1QmDFjBosXL067DDOzkiJpzWDa+fKRmZn1cSiYmVkfh4KZmfVxKJiZWR+HgpmZ9XEomJlZH4eCmZn1KZtQeG5TO//w86Xs7exOuxQzs6JVNqGwbttubrz/BR5dvTXtUszMilbZhMLZJzVQXZnh7uWtaZdiZla0ChYKko6X9BtJSyUtkfTBAdo0S9oh6Ylk+XSh6hlRXcGrThzPPc85FMzMDqSQZwpdwEciYg7wauA6SXMGaHdfRJyeLJ8pYD00N01kZctO1m3bXcjDmJmVrIKFQkRsiIjHk8ftwDJgaqGONxgLGnMTBPoSkpnZwIakT0HSDOAM4OEBNp8t6UlJv5A09wCvv1bSYkmLW1uP/AN9ZnYU08aN8CUkM7MDKHgoSKoDfgJcHxFt/TY/DkyPiNOAfwf+a6B9RMQNETEvIuZls4ecDvxgtdDclOW3Kzezr6vniPdjZjZcFTQUJFWRC4SbI+Kn/bdHRFtE7EweLwSqJDUUsqYFjRPZta+bxR6aamb2MoUcfSTgRmBZRHzpAG0mJ+2QND+pZ0uhagI4Z+YEqisy3O1LSGZmL1PIM4VzgbcDF+YNOb1c0vskvS9pcxXwjKQnga8AV0dEFLAmRtVU8soTx3GPO5vNzF6mYLfjjIj7AR2izVeBrxaqhgNZ0JjlHxc+y++372HK2BFDfXgzs6JVNr9oztfcNBHAo5DMzPopy1CYPbGOKWNqfQnJzKyfsgwFSSxomsgDKzfT2e2hqWZmvcoyFCDXr9De0cVja7alXYqZWdEo21A4d9YEKjPylBdmZnnKNhTqa6uYN2OcO5vNzPKUbShA7tfNyza0saltb9qlmJkVhbIOheam3DxKHoVkZpZT1qFw8uR6Jo+u9SUkM7NEWYeCJBY0ZrlvRStdHppqZlbeoQCwoClL294ufrd2e9qlmJmlruxD4dxZDVRkxN3LW9IuxcwsdWUfCmNGVHHWCR6aamYGDgUgdwnpmfVttLR7aKqZlTeHArkpLwDufW5zypWYmaXLoQDMnTKabH2NLyGZWdlzKLD/0NTunoLe+M3MrKg5FBILGrNs393JEx6aamZlzKGQOH92AxnBPR6aamZlzKGQGDuymjM8NNXMypxDIc+CxixPrd/Blp0daZdiZpYKh0Ke5qYsEXDvCp8tmFl5cijkecWUMTTUVXsqbTMrWw6FPJmMuGB2lntXbKbHQ1PNrAw5FPpZ0JRl6659PLV+R9qlmJkNOYdCP+fPziLhWVPNrCw5FPoZP6qa06aN9dBUMytLDoUBLGjM8sTa7WzbtS/tUszMhpRDYQAemmpm5cqhMIBTp41l3MgqX0Iys7LjUBhARUZc0Jjl3udaPTTVzMpKwUJB0vGSfiNpqaQlkj44QBtJ+oqklZKeknRmoeo5XAsas2zeuY8lv29LuxQzsyFTyDOFLuAjETEHeDVwnaQ5/dpcBsxOlmuBrxewnsNyQXI3Ng9NNbNyUrBQiIgNEfF48rgdWAZM7dfs9cB3I+chYKyk4wpV0+FoqKvh1Glj3K9gZmVlSPoUJM0AzgAe7rdpKrA27/k6Xh4cSLpW0mJJi1tbh+5DekFjlsdf3MaO3Z1DdkwzszQVPBQk1QE/Aa6PiCO6QB8RN0TEvIiYl81mj22BB9HclKUn4L6VPlsws/JQ0FCQVEUuEG6OiJ8O0GQ9cHze82nJuqJw+vHjGDOiyrOmmlnZKOToIwE3Assi4ksHaPYz4B3JKKRXAzsiYkOhajpcFRlx/uwG7nmulQgPTTWz4a+QZwrnAm8HLpT0RLJcLul9kt6XtFkIrAJWAt8E3l/Aeo7IgsYsLe0dLN3goalmNvxVFmrHEXE/oEO0CeC6QtVwLCxo6h2a2srcKWNSrsbMrLD8i+ZDmFhfy9wpoz001czKgkNhEBY0ZnlszTba9npoqpkNbw6FQWhumkh3T/DAis1pl2JmVlAOhUE484Sx1NdW+hKSmQ17DoVBqKzIcN6sBu5e7qGpZja8ORQGqbkpy8a2vSzf1J52KWZmBeNQGKQFjROB3NBUM7PhyqEwSJPH1HLy5HpPeWFmw5pD4TAsaMqyeM1WdnZ0pV2KmVlBOBQOQ3PjRDq7gwdWemiqmQ1PDoXDcNb0cdTVeGiqmQ1fDoXDUF2Z4ZyZE7jHQ1PNbJhyKBym5qaJrN++h5UtO9MuxczsmHMoHKbmvFlTzcyGG4fCYZoydgSNk+rcr2Bmw5JD4QgsaMzyyAtb2eWhqWY2zDgUjkBz00T2dffw4PNb0i7FzOyYcigcgXkzxjGyusKXkMxs2HEoHIGaygrOmTmBu59r8dBUMxtWHApHaEHTRNZu3cOqzbvSLsXM7JhxKByh5kYPTTWz4cehcISOHz+SmdlR7lcws2HFoXAUFjRO5KFVW9izrzvtUszMjgmHwlFobsqyr6uHh1Z5aKqZDQ8OhaMw/8Tx1FZlfAnJzIYNh8JRqK2q4OyTJnD38pa0SzEzOyYcCkepuWkiq7fsZrWHpprZMOBQOEovzZrqswUzK30OhaM0fcIoTmzw0FQzGx4cCsfAgsYsD67awt5OD001s9LmUDgGFjRl2dvZw8MvbE27FDOzo1KwUJD0bUktkp45wPZmSTskPZEsny5ULYV29kkTqKnMcI+nvDCzElfIM4XvAJceos19EXF6snymgLUUVG1VBa86KTdrqplZKStYKETEvUDZXE9pbsyyqnUXa7fuTrsUM7MjlnafwtmSnpT0C0lzD9RI0rWSFkta3NpanJdoPDTVzIaDNEPhcWB6RJwG/DvwXwdqGBE3RMS8iJiXzWaHrMDDcWLDKE4YP9JDU82spKUWChHRFhE7k8cLgSpJDWnVc7QksaAxy2+f30JHl4emmllpSi0UJE2WpOTx/KSWkp5utLkpy+593Tz6wra0SzEzOyKVhdqxpFuAZqBB0jrg74EqgIj4D+Aq4C8ldQF7gKujxG94fPbMCVRXZLjnuRbOm12yJz1mVsYKFgoRcc0htn8V+Gqhjp+GkdWVzD9xPHcvb+UTr0u7GjOzw5f26KNhp7kpy4qWnazfviftUszMDptD4Rjz0FQzK2WDCgVJH5Q0Wjk3Snpc0iWFLq4UzczWMXXsCE95YWYlabBnCn8eEW3AJcA44O3A5wtWVQmTxIKmLA+s3My+rp60yzEzOyyDDQUl/14OfC8iluSts36aG7Ps2tfN4jVlM8uHmQ0Tgw2FxyQtIhcKd0qqB/w1+ADOmdVAVYX862YzKzmDDYV3Ax8DXhkRu8n93uDPClZViaurqWTe9PHuVzCzkjPYUDgbWB4R2yW9DfgksKNwZZW+5qYsz25sZ8MOD001s9Ix2FD4OrBb0mnAR4Dnge8WrKphoLlpIoDPFsyspAw2FLqSKSheD3w1Ir4G1BeurNLXOKmOyaNr3a9gZiVlsKHQLunj5Iai3i4pQzKPkQ1MEs1NWe5fsZnObvfJm1lpGGwovAXoIPd7hY3ANOALBatqmGhuytLe0cXjazxrqpmVhkGFQhIENwNjJF0B7I0I9ykcwjmzGqjMeGiqmZWOwU5z8WbgEeBNwJuBhyVdVcjChoPRtVWcOX0cd7uz2cxKxGAvH32C3G8U3hkR7wDmA58qXFnDR3NTlqUb2mhp25t2KWZmhzTYUMhERP60n1sO47VlrbkxNzT1bl9CMrMSMNgP9jsk3SnpXZLeBdwOLCxcWcPHKcfVM7G+xv0KZlYSBnXntYj4qKQ3Aucmq26IiNsKV9bwIYkFjVnuXLKRru4eKit8gmVmxWvQn1AR8ZOI+HCyOBAOQ3PTRNr2dvHE2u1pl2JmdlAHDQVJ7ZLaBljaJbUNVZGl7rzZDVR4aKqZlYCDhkJE1EfE6AGW+ogYPVRFlroxI6o44/ixHppqZkXPF7iHSHNTlqfX76C1vSPtUszMDsihMER6Z02915eQzKyIORSGyJzjRtNQV+1+BTMrag6FIZLJiAsas9y7opXunki7HDOzATkUhlBz00S27+7kyXUemmpmxcmhMITOn9VARr4bm5kVL4fCEBo3qprTjh/reZDMrGg5FIbYxXMm8eTa7f51s5kVJYfCEHvH2TNoqKvmc7cvJXfbazOz4uFQGGJ1NZV8+OImHl29jTue2Zh2OWZm+ylYKEj6tqQWSc8cYLskfUXSSklPSTqzULUUmzfPm0bjpDo+f8ezdHR1p12OmVmfQp4pfAe49CDbLwNmJ8u1wNcLWEtRqazI8InXzWHNlt1878E1aZdjZtanYKEQEfcCWw/S5PXAdyPnIWCspOMKVU+xWdCY5YLGLF+5awXbdu1LuxwzMyDdPoWpwNq85+uSdS8j6VpJiyUtbm0dPsM5P3H5Kezs6OLf7lqRdilmZkCJdDRHxA0RMS8i5mWz2bTLOWaaJtfzlleewPcfWsOq1p1pl2NmlmoorAeOz3s+LVlXVj58cSM1lRn+6RfPpl2KmVmqofAz4B3JKKRXAzsiYkOK9aQiW1/D+/9wFr9cuokHn9+SdjlmVuYKOST1FuBBoEnSOknvlvQ+Se9LmiwEVgErgW8C7y9ULcXu3eedyJQxtXxu4VJ6PIOqmaWoslA7johrDrE9gOsKdfxSUltVwd9eejLX3/oEt/1uPW88a1raJZlZmSqJjuZycOVpUzht2hi+cOdy9uzzD9rMLB0OhSKRyYhPXjGHjW17+eZ9q9Iux8zKlEOhiLxyxngue8Vk/uOe52lp25t2OWZWhhwKReZjl51MZ3cP/3fRc2mXYmZlyKFQZKZPGMU7z57Bjx5by9Lft6VdjpmVGYdCEfrrC2czZkQVn1voey6Y2dByKBShMSOr+OBFs3lg5RZ+s7wl7XLMrIw4FIrUn75qOic2jOJzty+js7sn7XLMrEw4FIpUdWWGj192Ms+37uKHj7yYdjlmViYcCkXs4jmTeNWJ4/nyr1bQtrcz7XLMrAw4FIqYJD51xRy27d7H136zMu1yzKwMOBSK3CumjuENZ0zlP+9fzdqtu9Mux8yGOYdCCfjoa5vIZOCf7/A9F8yssBwKJeC4MSO49vyT+PlTG3hszba0yzGzYcyhUCLeu2Am2foaPnu7f9BmZoXjUCgRo2oq+ZtLGvndi9u5/emyu0GdmQ0Rh0IJueqs4zl5cj2f/8Wz7O30PRfM7NhzKJSQioz45OvmsG7bHm767eq0yzGzYcihUGLOm93AhSdP5Ku/XsmWnR1pl2Nmw4xDoQT93eUns7uzm3+7a0XapZjZMONQKEGzJtbz1vkncPPDL7KypT3tcsxsGHEolKjrXzObkVUV/NNC/6DNzI4dh0KJmlBXw3UXzuKuZ1t4YOXmtMsxs2HCoVDC3nXODKaOHcFnb19Gd49/0GZmR8+hUMJqqyr42GUns2xDGz95bF3a5ZjZMOBQKHFXnHocZ5wwli8uWs6ujq60yzGzEudQKHFS7gdtLe0dfOPeVWmXY2YlzqEwDJw1fRyvO/U4brj3eTbu2Jt2OWZWwhwKw8THLj2Znh74wp3L0y7FzEqYQ2GYOH78SP7s3Bn89HfreGb9jrTLMbMS5VAYRt7/h7MYN7La91wwsyNW0FCQdKmk5ZJWSvrYANvfJalV0hPJ8p5C1jPcjRlRxfWvmc1Dq7byq2UtaZdjZiWoYKEgqQL4GnAZMAe4RtKcAZreGhGnJ8u3ClVPubhm/gnMzI7inxYuo7O7J+1yzKzEFPJMYT6wMiJWRcQ+4IfA6wt4PAOqKjL83eWnsGrzLm5+aE3a5ZhZiSlkKEwF1uY9X5es6++Nkp6S9GNJxxewnrJx4ckTOXfWBP71rhXs2N2ZdjlmVkLS7mj+H2BGRJwK/BK4aaBGkq6VtFjS4tbW1iEtsBRJ4hOXz2HHnk6++hvfc8HMBq+QobAeyP/mPy1Z1ycitkRE7+3DvgWcNdCOIuKGiJgXEfOy2WxBih1u5kwZzZvOmsZ3fruaNVt2pV2OmZWIQobCo8BsSSdKqgauBn6W30DScXlPrwSWFbCesvORS5qozGT45zt8zwUzG5yChUJEdAF/BdxJ7sP+RxGxRNJnJF2ZNPuApCWSngQ+ALyrUPWUo0mja3nvgpNY+PRGHl29Ne1yzKwEqNR+5DRv3rxYvHhx2mWUjN37uvjDL97N5DEjuO0vzyGTUdolmVkKJD0WEfMO1S7tjmYrsJHVlfzNJU08uXY7//PU79Mux8yKnEOhDLzxzGnMnTKaf7ljOXs7u9Mux8yKmEOhDGQy4hOvO4X12/dw4/0vpF2OmRUxh0KZOGdmA685ZRJfv/t5Wts7Dv0CMytLDoUy8vHLT2ZvZzdf/tVzaZdiZkXKoVBGZmbreNurp/PDR17kuU3taZdjZkXIoVBmPnDRbEbVVPK52/07QTN7OYdCmRk/qpoPXDibe55r5Z7nPI+Ume3PoVCG3nHOdE4YP5K/+O5i3v2dR7n10RfZvNOdz2YGlWkXYEOvprKCm/58Pt99cDWLlmzirmdbkJ5m3vRxXDJnMpfMncT0CaPSLtPMUuBpLspcRLB0QxuLlmxi0dJNLNvQBkDTpHoumTuJS+ZM5hVTRyN5egyzUjbYaS4cCraftVt3s2jpJhYtyU2i1xMwZUwtF8+ZxCVzJzP/xPFUVfiqo1mpcSjYUdu6ax93LcudQdy3opW9nT2Mrq3kolMmccmcSVzQmGVUja9AmpUCh4IdU3v2dXPvitakD2IT23d3Ul2Z4fxZDVw8ZxIXnTKJbH1N2mWa2QEMNhT8Nc8GZUR1Ba+dO5nXzp1MV3cPj67exi+XbuLOJRv7OqrPOmFcXz/EjAZ3VJuVIp8p2FGJCJZtaGfR0o0sWrKJpUlHdeOkur6RTH8wdYw7qs1S5stHloq1W3fzy6WbWLR0I4+8kOuoPq63o3rOZF51kjuqzdLgULDUbd21j18/28KiJRu5N6+j+sKTJ3LJ3Mlc0Jilzh3VZkPCoWBFZc++bu5b0cqipZu4a9kmtiUd1efNauDUaWPI1tcwsb6WbH0N2foaGuqqqamsSLtss2HDHc1WVEZUV3DJ3MlcknRUL16zjUVLNvGrZZv49bMtA75m7MgqsnU1SWDU9AXGfgFSV8PYkVXuszA7RnymYKnb19XDll0dtLbnlpb2lx7nnu+ldWcHLW0ddHT1vOz1VRXqC4/cUpsXHjV94ZGtr6G2ymcfVp58pmAlo7oyw3FjRnDcmBEHbRcRtHd09QuMvADZ2cG6bXt4Yu12tuzax0Dfd0bXVr78bCMJjdEjqqirqcwttZWMqqmgvqaK2qqMz0SsbDgUrGRIYnRtFaNrq5iZrTto267uHrbu2rdfaLS07+0Lj9b2Dp5ct52Wtg72dHYfdF8Z0S8sKvue5z/u3VY/4HoHjJUGh4INS5UVGSaOrmXi6NpDtt2VnH207+2ivaOTXR3d7OzoZGdHNzv3drGro4udyZL/eOOOvezq6KI9Wd8ziCuxGbF/cNS+PGBqqjLUVFZQU5nJWyqS9XnbknbVlQOvr8g4fOzwORSs7I1KPpCPRkSwp7M7CY5cmOQHSW9w9K7PD5edHV1satvbt62jq2fAvpPDVZnR/oExQKhUVwy8rboyQ2VFhuoKUVmRoaoiQ1WFqMzk/q2qyFCZ/Nv3vN+26orcPnrrqMwo2Wdue2VGPmsqQg4Fs2NAEiOrKxlZXQn1R7+/iKCzO+jo6u4LiY7Olz/e1/u8t11+m65uOjp72NfdQ0dnz/77Sra17emio6s7bz8v7aNrMKc+R+nAQfNSCFVWiIpMLkQqMrnt+c8rMy89z7XN5G0TFUkAVfaur+h97f7tegNsv33n7S+j3Lq+RSKTITk+ZJQ7RiZD3/betpl+z3OvLc5AdCiYFSFJVFfmvmEfg4w5Ir3B1NXTQ2d30NndQ1fyb2d3LjT2JeHR1Z0Ln5e2974ueZz3utzzHjp7evc5wP6TfXZ299DdE8kxgu6eYFdXV9+6/H/z23Ynr+97bbKumEj0hUOFckGU6Q0RvRRMmQx9gXLN/BN4z/knFbQuh4KZDagvmIbJXXsjYsAw6UoC7qV1PX0h1JUXLt0R9PRAd+TadPeQW9+3LfZ73t0T9ERuPz2Rt777pfZd+72W3H4j97h3e99re4KGusLPROxQMLOyIOUuB/mH8gc3PL4CmJnZMeFQMDOzPg4FMzPrU9BQkHSppOWSVkr62ADbayTdmmx/WNKMQtZjZmYHV7BQkFQBfA24DJgDXCNpTr9m7wa2RcQs4MvAPxeqHjMzO7RCninMB1ZGxKqI2Af8EHh9vzavB25KHv8YuEj+iaOZWWoKGQpTgbV5z9cl6wZsExFdwA5gQv8dSbpW0mJJi1tbWwtUrpmZlURHc0TcEBHzImJeNptNuxwzs2GrkD9eWw8cn/d8WrJuoDbrJFUCY4AtB9vpY489tlnSmiOsqQHYfISvPZZcx/5cx/6KoY5iqAFcR39HU8f0wTQqZCg8CsyWdCK5D/+rgbf2a/Mz4J3Ag8BVwK/jELeCi4gjPlWQtHgwdx4qNNfhOoq9jmKowXWkU0fBQiEiuiT9FXAnUAF8OyKWSPoMsDgifgbcCHxP0kpgK7ngMDOzlBR07qOIWAgs7Lfu03mP9wJvKmQNZmY2eCXR0XwM3ZB2AQnXsT/Xsb9iqKMYagDX0V/B69AhLuGbmVkZKbczBTMzOwiHgpmZ9SmbUDjU5HxDVMO3JbVIeiaN4+fVcbyk30haKmmJpA+mUEOtpEckPZnU8H+GuoZ+9VRI+p2kn6dYw2pJT0t6QtLiFOsYK+nHkp6VtEzS2SnU0JT8HXqXNknXp1DHh5L/Pp+RdIuk2qGuIanjg0kNSwr+d4iIYb+QGxL7PHASUA08CcxJoY4LgDOBZ1L+exwHnJk8rgeeG+q/ByCgLnlcBTwMvDrFv8mHgR8AP0+xhtVAQ5r/bSR13AS8J3lcDYxNuZ4KYCMwfYiPOxV4ARiRPP8R8K4U3v8rgGeAkeRGjP4KmFWo45XLmcJgJucruIi4l9zvMVIVERsi4vHkcTuwjJfPS1XoGiIidiZPq5IllVEPkqYBrwO+lcbxi4mkMeS+vNwIEBH7ImJ7ulVxEfB8RBzpTAZHoxIYkcy4MBL4fQo1nAI8HBG7IzdH3D3AnxTqYOUSCoOZnK8sJfewOIOZ9aylAAAELUlEQVTcN/WhPnaFpCeAFuCXETHkNST+FfhboCel4/cKYJGkxyRdm1INJwKtwH8ml9O+JWlUSrX0uhq4ZagPGhHrgS8CLwIbgB0RsWio6yB3lnC+pAmSRgKXs/8UQsdUuYSCDUBSHfAT4PqIaBvq40dEd0ScTm5erPmSXjHUNUi6AmiJiMeG+tgDOC8iziR3D5LrJF2QQg2V5C5xfj0izgB2Aan0wQFIqgauBP5fCsceR+6KwonAFGCUpLcNdR0RsYzcvWYWAXcATwDdhTpeuYTCYCbnKyuSqsgFws0R8dM0a0kuT/wGuDSFw58LXClpNbnLihdK+n4KdfR+MyUiWoDbyF32HGrrgHV5Z20/JhcSabkMeDwiNqVw7NcAL0REa0R0Aj8FzkmhDiLixog4KyIuALaR6wcsiHIJhb7J+ZJvHleTm4yvLCU3MroRWBYRX0qphqykscnjEcDFwLNDXUdEfDwipkXEDHL/Xfw6Iob826CkUZLqex8Dl5C7bDCkImIjsFZSU7LqImDpUNeR5xpSuHSUeBF4taSRyf8zF5HrfxtykiYm/55Arj/hB4U6VkHnPioWcYDJ+Ya6Dkm3AM1Ag6R1wN9HxI1DXQe5b8dvB55OrukD/F3k5qoaKscBNyW3bc0AP4qI1IaDFoFJwG3JjQcrgR9ExB0p1fLXwM3JF6hVwJ+lUUQSjhcD703j+BHxsKQfA48DXcDvSG+6i59ImgB0AtcVsvPf01yYmVmfcrl8ZGZmg+BQMDOzPg4FMzPr41AwM7M+DgUzM+vjUDArMEnNac6+anY4HApmZtbHoWCWkPS25B4PT0j6RjJh305JX07msb9LUjZpe7qkhyQ9Jem2ZJ4cJM2S9KvkPhGPS5qZ7L4u7x4FNye/kEXS55P7Wjwl6YspvXWzPg4FM0DSKcBbgHOTSfq6gT8FRgGLI2IuuSmL/z55yXeB/xURpwJP562/GfhaRJxGbp6cDcn6M4DrgTnk7utxbvIL1TcAc5P9fLaw79Ls0BwKZjkXAWcBjyZTf1xE7sO7B7g1afN94LzkngNjI+KeZP1NwAXJ3EVTI+I2gIjYGxG7kzaPRMS6iOghN8vlDGAHsBe4UdKfAL1tzVLjUDDLEXBTRJyeLE0R8b8HaHek88J05D3uBiqTG6bMJzcT6RXkpkU2S5VDwSznLuCqvNkox0uaTu7/kauSNm8F7o+IHcA2Secn698O3JPcxW6dpD9O9lGT3BRlQMn9LMYkExF+CDitEG/M7HCUxSypZocSEUslfZLcnc8yJLNRkrvJzPxkWwu5fgeAdwL/kXzo588k+nbgG5I+k+zjTQc5bD3w38nN4EXuPtFmqfIsqWYHIWlnRNSlXYfZUPHlIzMz6+MzBTMz6+MzBTMz6+NQMDOzPg4FMzPr41AwM7M+DgUzM+vz/wGvjHyg4KeiDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: 0.2. generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK with 1 mismatches from the wild type.\n",
      "temperature: 0.8. generated sequence: SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHFDKNFKSRERVEDFKFVPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDGELYKTAAKIGDVQLEDFKNGILVNKGIELKGIDFKEDGNILGHKLVEFVTAGITHGMVELEFTGTGVVPILVELDGDVNRILGHKLEYNYN with 145 mismatches from the wild type.\n",
      "temperature: 1.0. generated sequence: SMKEGETLFVTGIGDGPVLLPDNHYLSTQSALSKTEGDATVLGIDDGHKLNYNSHMVQLADHMVEKFSGVIELGDDGVNRIELHKFEGDTLYKNRHIDFKSGDPNETRHKEFKDNYNSHNYVLADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSAILKDDNKIFKEGDTLVNRIELKGIDFKEDGNIHLYKNIGHKLYNSHNVYIMADKQKNGIKV with 217 mismatches from the wild type.\n",
      "temperature: 1.2. generated sequence: SKGEELFGVTLSYGVQETTRLMVDEQFSVGEGDRLEYTQSRAIDKNFKIRHNEFSKGELRIDLGHYQNTTIGDAKLPVNGIEGKDTFHLVSAEGYVQERTQFRDHMKLEFVCAATYGKPLVTTLSYGKLTLKFICTTGKLQPPPDTLVTLSKGNIRDHKQFEFETGYGVQCFSRYNHMKELYVQNTPIGDGPVLLPDLNHYSLSTSALKEGDTNLYKSGLELVTRRIEFKCDGYVQL with 214 mismatches from the wild type.\n",
      "temperature: 1.8. generated sequence: SGELGTTLKREDHMYVLLEFKTCALTGPVVPCPTLTVLGHCSYFYGNSHQVRIEDDGKQFSVGPEGDYTPRTAKLVDRIFLESGDHQANHYSLYSTPRIRDHYMQLTDAHKSFQFSPMPEYGVQRRETRHEDFVSGGEGFDGVVCPLVQTHAFNGVIEGFVEGDALDGSVQLATYTQAEHLKDGYNHNSKVIFEGDVQECFKNMRIEVDGYTKQAMDKQEGIFVKDLGVQFIGDDGP with 219 mismatches from the wild type.\n"
     ]
    }
   ],
   "source": [
    "rnn = GenerativeRNN(args)\n",
    "logger = open(\"./logs/{0}.txt\".format(args[\"name\"]), \"w\")\n",
    "rnn.fit(dataloader=dataloader, logger=logger)\n",
    "rnn.show_model()\n",
    "rnn.plot_history(\"./logs/{0}_training_history\".format(args[\"name\"]))\n",
    "rnn.plot_model(\"./logs/{0}_model_architecture\".format(args[\"name\"]))\n",
    "temperature_lst = [0.2, 0.8, 1.0, 1.2, 1.8]\n",
    "for temperature in temperature_lst: \n",
    "    generated_sequence = sample(\"S\", seq_length - 1, temperature=temperature)\n",
    "    mismatches = count_substring_mismatch(wild_type, generated_sequence)\n",
    "    print(\"temperature: {0}. generated sequence: {1} with {2} mismatches from the wild type.\".format(temperature, generated_sequence, mismatches)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_all_sequences(model, string, base = \"S\", depth = 3): \n",
    "    if depth == 0: \n",
    "        return np.e ** model.predict_log_prob(string, base)\n",
    "    total = 0\n",
    "    for c in all_characters: \n",
    "        total += enumerate_all_sequences(model, string + c, base, depth - 1)\n",
    "    return total\n",
    "\n",
    "for depth in range(1, 4):\n",
    "    for base in \"QRST\":\n",
    "        np.testing.assert_almost_equal(1, enumerate_all_sequences(rnn, \"\", base, depth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:12: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.2853), tensor(0.1678), tensor(0.1911), tensor(0.1967), tensor(0.1559), tensor(0.2365), tensor(0.2600), tensor(0.2189), tensor(0.1826), tensor(0.1993)]\n",
      "[tensor(0.5044), tensor(0.4353), tensor(0.4265), tensor(0.3414), tensor(0.2914), tensor(0.2785), tensor(0.2048), tensor(0.2876), tensor(0.2327), tensor(0.2484)]\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "load_rnn = GenerativeRNN(args)\n",
    "load_rnn.load_model(\"./checkpoint_{0}.pt\".format(epoch))\n",
    "total_loss = []\n",
    "for i, (input, target) in enumerate(dataloader):\n",
    "    batch_size, seq_length = input.shape[0], input.shape[1]\n",
    "    hidden = load_rnn.model.init_hidden(batch_size)\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        output, hidden = load_rnn.model(input[:, c], hidden)\n",
    "        loss += load_rnn.criterion(output.view(batch_size, -1), target[:, c])\n",
    "    total_loss.append(loss.data[0] / seq_length) \n",
    "print(total_loss)\n",
    "print(torch.load(\"./models/rnn_test_sample/checkpoint_{0}.pt\".format(epoch))[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
